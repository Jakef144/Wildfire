{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ad88f5-7966-4d85-ac8a-7dbbd2228d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found. Starting PDF to JSON conversion...\n",
      "Number of pages: 76\n",
      "Finished extracting text from all pages.\n",
      "JSON file saved as: annual_report_2008_508.json\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries  \n",
    "import fitz  # PyMuPDF  \n",
    "import os  \n",
    "import json  \n",
    "\n",
    "# Define the PDF file path  \n",
    "pdf_path = 'annual_report_2008_508.pdf'  \n",
    "  \n",
    "# Check if the file exists  \n",
    "if not os.path.exists(pdf_path):  \n",
    "    print(\"File not found. Please check the file path.\")  \n",
    "else:  \n",
    "    print(\"File found. Starting PDF to JSON conversion...\")  \n",
    "  \n",
    "    # Open the PDF file  \n",
    "    pdf_document = fitz.open(pdf_path)  \n",
    "    num_pages = len(pdf_document)  \n",
    "    print(\"Number of pages:\", num_pages)  \n",
    "  \n",
    "    # Create a dictionary where each page's text is stored  \n",
    "    pdf_data = {}  \n",
    "  \n",
    "    for page_num in range(num_pages):  \n",
    "        page = pdf_document[page_num]  \n",
    "        text = page.get_text()  \n",
    "        # Store the text in the dictionary with page number as key (starting at 1)  \n",
    "        pdf_data[page_num + 1] = text  \n",
    "  \n",
    "    pdf_document.close()  \n",
    "    print(\"Finished extracting text from all pages.\")  \n",
    "  \n",
    "    # Specify the output JSON file path  \n",
    "    json_output_path = 'annual_report_2008_508.json'  \n",
    "      \n",
    "    # Make sure the directory exists  \n",
    "    output_dir = os.path.dirname(json_output_path)  \n",
    "    if output_dir and not os.path.exists(output_dir):  \n",
    "        os.makedirs(output_dir)  \n",
    "      \n",
    "    # Write the dictionary to a JSON file  \n",
    "    with open(json_output_path, 'w', encoding='utf-8') as json_file:  \n",
    "        json.dump(pdf_data, json_file, ensure_ascii=False, indent=4)  \n",
    "      \n",
    "    print(\"JSON file saved as:\", json_output_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb86f68-6791-4979-a918-ca3ef3579b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys (pages): 76\n",
      "Keys (first 5): ['1', '2', '3', '4', '5']\n",
      "Sample content from first page (first 300 characters):\n",
      "Page 1: National Interagency \n",
      "Coordination Center \n",
      "Wildland Fire \n",
      "Summary and Statistics \n",
      "Annual Report \n",
      "2008 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "Sample content from middle page 39 (first 300 characters):\n",
      " \n",
      " 38\n",
      "Types 1 and 2 IMT Summary 2008 \n",
      "Incident Management Team summary: The tables below depict total Type 1 and Type 2 Incident \n",
      "Management Teams requested through NICC. \n",
      " \n",
      "By Requesting Agency \n",
      " \n",
      "                    \n",
      "By Requesting Geographic Area \n",
      " \n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:31: SyntaxWarning: invalid escape sequence '\\\t'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\\t'\n",
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_20128\\4058892382.py:31: SyntaxWarning: invalid escape sequence '\\\t'\n",
      "  return bool(re.search(r'\\d+\\s+\\d+', line)) or '|' in line or '\\\t' in line\n",
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_20128\\4058892382.py:31: SyntaxWarning: invalid escape sequence '\\\t'\n",
      "  return bool(re.search(r'\\d+\\s+\\d+', line)) or '|' in line or '\\\t' in line\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty separator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m20\u001b[39m]:  \u001b[38;5;66;03m# Check first 20 pages\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     page_text \u001b[38;5;241m=\u001b[39m data[page_key]\n\u001b[1;32m---> 37\u001b[0m     lines \u001b[38;5;241m=\u001b[39m page_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m might_be_table_row(line):\n",
      "\u001b[1;31mValueError\u001b[0m: empty separator"
     ]
    }
   ],
   "source": [
    "# Let's first load the JSON file and examine its structure\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('annual_report_2008_508.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the structure - how many pages and what the keys look like\n",
    "print(f\"Number of keys (pages): {len(data)}\")\n",
    "print(f\"Keys (first 5): {list(data.keys())[:5]}\")\n",
    "\n",
    "# Let's look at the first page to understand the content\n",
    "print(\"\\\n",
    "Sample content from first page (first 300 characters):\")\n",
    "first_page_key = list(data.keys())[0]\n",
    "print(f\"Page {first_page_key}: {data[first_page_key][:300]}...\")\n",
    "\n",
    "# Let's also look at a page from the middle of the document\n",
    "middle_page_key = list(data.keys())[len(data)//2]\n",
    "print(f\"\\\n",
    "Sample content from middle page {middle_page_key} (first 300 characters):\")\n",
    "print(f\"{data[middle_page_key][:300]}...\")\n",
    "\n",
    "# Let's check if there are any tables or structured data patterns\n",
    "# by looking for common table indicators like rows of numbers\n",
    "import re\n",
    "\n",
    "# Function to check if a line might be part of a table\n",
    "def might_be_table_row(line):\n",
    "    # Check if line has multiple numbers or tabular patterns\n",
    "    return bool(re.search(r'\\d+\\s+\\d+', line)) or '|' in line or '\\\t' in line\n",
    "\n",
    "# Check a few pages for potential table content\n",
    "table_pages = []\n",
    "for page_key in list(data.keys())[:20]:  # Check first 20 pages\n",
    "    page_text = data[page_key]\n",
    "    lines = page_text.split('\\\n",
    "')\n",
    "    for line in lines:\n",
    "        if might_be_table_row(line):\n",
    "            table_pages.append(page_key)\n",
    "            break\n",
    "\n",
    "print(f\"\\\n",
    "Pages that might contain tables: {table_pages[:5]}\")\n",
    "\n",
    "# Let's also look for key sections or headers that might indicate important data\n",
    "def find_headers(text):\n",
    "    # Look for capitalized lines that might be headers\n",
    "    headers = []\n",
    "    lines = text.split('\\\n",
    "')\n",
    "    for line in lines:\n",
    "        # If line is all caps and not too long, it might be a header\n",
    "        if line.strip() and line.strip().isupper() and len(line.strip()) < 50:\n",
    "            headers.append(line.strip())\n",
    "    return headers\n",
    "\n",
    "# Check for potential headers in the first few pages\n",
    "all_headers = []\n",
    "for page_key in list(data.keys())[:10]:  # Check first 10 pages\n",
    "    headers = find_headers(data[page_key])\n",
    "    if headers:\n",
    "        all_headers.extend(headers)\n",
    "\n",
    "print(f\"\\\n",
    "Potential section headers found: {all_headers[:10]}\")\n",
    "\n",
    "# Let's also look for dates, which could be important for time dimensions\n",
    "def find_dates(text):\n",
    "    # Simple regex for dates in format like 2008, MM/DD/YYYY, or Month DD, YYYY\n",
    "    date_patterns = [\n",
    "        r'\\\b\\d{4}\\\b',  # Year like 2008\n",
    "        r'\\\b\\d{1,2}/\\d{1,2}/\\d{4}\\\b',  # MM/DD/YYYY\n",
    "        r'\\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\\b'  # Month DD, YYYY\n",
    "    ]\n",
    "    \n",
    "    dates = []\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        dates.extend(matches)\n",
    "    \n",
    "    return dates\n",
    "\n",
    "# Check for dates in the document\n",
    "all_dates = []\n",
    "for page_key in list(data.keys())[:10]:  # Check first 10 pages\n",
    "    dates = find_dates(data[page_key])\n",
    "    if dates:\n",
    "        all_dates.extend(dates)\n",
    "\n",
    "print(f\"\\\n",
    "Dates found in the document: {all_dates[:10]}\")\n",
    "\n",
    "print(\"\\\n",
    "This initial analysis helps us understand what kind of data we're working with.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ccbe84-13e7-4b48-9882-043f9a7c1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of page 15:\n",
      " \n",
      " 14\n",
      "Significant Incidents Over 40,000 Acres \n",
      " \n",
      "Name \n",
      "Inc. \n",
      "Type \n",
      "GACC \n",
      "State \n",
      "Start Date \n",
      "Contain or \n",
      "Control \n",
      "Date \n",
      "Size \n",
      "(Acres) \n",
      "Cause \n",
      "Cost \n",
      "Glass Fire \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "2/25/2008 \n",
      "3/2/2008 \n",
      "219,556 \n",
      "H \n",
      "NR \n",
      "Klamath Theater \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "9/26/2008 \n",
      "192,038 \n",
      "L \n",
      "$126,086,065 \n",
      "Basin Complex \n",
      "WF \n",
      "SO \n",
      "CA \n",
      "6/21/2008 \n",
      "7/29/2008 \n",
      "162,818 \n",
      "L \n",
      "$78,096,079 \n",
      "Iron & Alps \n",
      "Complexes \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "9/4/2008 \n",
      "105,805 \n",
      "L \n",
      "$73,974,917 \n",
      "Dunn Mtn. \n",
      "Assist \n",
      "WF \n",
      "NR \n",
      "MT \n",
      "8/21/2008 \n",
      "9/2/2008 \n",
      "102,383 \n",
      "L \n",
      "$2,900,000 \n",
      "Lime Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/20/2008 \n",
      "8/30/2008 \n",
      "99,585 \n",
      "L \n",
      "$59,329,698 \n",
      "Huckabee \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "4/30/2008 \n",
      "5/8/2008 \n",
      "98,200 \n",
      "U \n",
      "NR \n",
      "SHU Lightning \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "7/30/2008 \n",
      "86,500 \n",
      "L \n",
      "$56,438,391 \n",
      "Siskiyou / Blue 2 \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "9/13/2008 \n",
      "82,186 \n",
      "L \n",
      "$65,692,836 \n",
      "Indians \n",
      "WF \n",
      "SO \n",
      "CA \n",
      "6/8/2008 \n",
      "7/12/2008 \n",
      "76,554 \n",
      "H \n",
      "$42,500,000 \n",
      "Panther \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "7/22/2008 \n",
      "10/8/2008 \n",
      "72,344 \n",
      "L \n",
      "NR \n",
      "Gunbarrel \n",
      "WF \n",
      "RM \n",
      "WY \n",
      "7/26/2008 \n",
      "9/8/2008 \n",
      "68,148 \n",
      "L \n",
      "$11,200,000 \n",
      "Highway 322 \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "3/14/2008 \n",
      "3/19/2008 \n",
      "67,500 \n",
      "H \n",
      "NR \n",
      "Stiles Complex \n",
      "WF \n",
      "SW \n",
      "NM \n",
      "3/14/2008 \n",
      "3/16/2008 \n",
      "67,008 \n",
      "H \n",
      "$71,644 \n",
      "BTU Lightning \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 10/22/2008 \n",
      "64,995 \n",
      "L \n",
      "$94,825,683 \n",
      "Ukonom-South \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/20/2008 \n",
      "11/5/2008 \n",
      "58,871 \n",
      "L \n",
      "NR \n",
      "MEU Lightning \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/20/2008 \n",
      "12/1/2008 \n",
      "54,819 \n",
      "L \n",
      "$66,000,000 \n",
      "East Slide Rock \n",
      "Ridge \n",
      "WF \n",
      "WB \n",
      "NV \n",
      "8/10/2008 \n",
      "9/14/2008 \n",
      "54,549 \n",
      "L \n",
      "$8,873,000 \n",
      "Porter \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "3/14/2008 \n",
      "3/21/2008 \n",
      "51,400 \n",
      "NR \n",
      "NR \n",
      "Rocky \n",
      "WF \n",
      "SW \n",
      "NM \n",
      "6/18/2008 \n",
      "6/25/2008 \n",
      "49,132 \n",
      "L \n",
      "$2,158,700 \n",
      "Canyon \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "11/3/2008 \n",
      "47,680 \n",
      "L \n",
      "NR \n",
      "Bridger Fire \n",
      "WF \n",
      "RM \n",
      "CO \n",
      "6/8/2008 \n",
      "7/9/2008 \n",
      "45,800 \n",
      "L \n",
      "NR \n",
      "Hughes Ranch \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "6/4/2008 \n",
      "6/9/2008 \n",
      "45,241 \n",
      "U \n",
      "NR \n",
      "Evans Road \n",
      "WF \n",
      "SA \n",
      "NC \n",
      "6/1/2008 \n",
      "9/29/2008 \n",
      "40,704 \n",
      "L \n",
      "$18,249,415 \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Page 15 contains keywords: ['acres', 'cost']\n",
      "Context around 'acres' on page 15:\n",
      "... \n",
      " 14\n",
      "significant incidents over 40,000 acres \n",
      " \n",
      "name \n",
      "inc. \n",
      "type \n",
      "gacc \n",
      "state \n",
      "start date \n",
      "contain or \n",
      "control \n",
      "date \n",
      "size \n",
      "(acres) \n",
      "cause \n",
      "cost \n",
      "glass fire \n",
      "wf \n",
      "sa \n",
      "tx \n",
      "2/25/2008 \n",
      "3/2/2008...\n",
      "Context around 'cost' on page 15:\n",
      "...\n",
      "contain or \n",
      "control \n",
      "date \n",
      "size \n",
      "(acres) \n",
      "cause \n",
      "cost \n",
      "glass fire \n",
      "wf \n",
      "sa \n",
      "tx \n",
      "2/25/2008 \n",
      "3/2/2008 \n",
      "219,556 \n",
      "h \n",
      "nr \n",
      "klamath theater \n",
      "wf \n",
      "no \n",
      "ca \n",
      "6/21/2008 \n",
      "9/26/2008 \n",
      "192,038 \n",
      "l \n",
      "$126,086,065 \n",
      "basin ...\n",
      "Page 20 contains keywords: ['acres']\n",
      "Context around 'acres' on page 20:\n",
      "... \n",
      " 19\n",
      "wildfire acres by agency \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "st / ot – states and other non-federal \n",
      "...\n",
      "Page 25 contains keywords: ['acres', 'fires']\n",
      "Context around 'acres' on page 25:\n",
      "... \n",
      " 24\n",
      "prescribed fire projects and acres \n",
      "national reporting of prescribed fires began in 1998. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "Context around 'fires' on page 25:\n",
      "...jects and acres \n",
      "national reporting of prescribed fires began in 1998. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "Page 30 contains keywords: ['acres', 'fires']\n",
      "Context around 'acres' on page 30:\n",
      "...nts began in 1998. \n",
      " \n",
      "wildland fire use fires and acres burned by agency  \n",
      " \n",
      "...\n",
      "Context around 'fires' on page 30:\n",
      "... \n",
      " 29\n",
      "wildland fire use fires by agency and geographic area \n",
      "national reporting of wildland fire use incidents began in 1998. \n",
      " \n",
      "wildland fire use fires and acres burned by a...\n",
      "Page 35 contains keywords: ['total']\n",
      "Context around 'total' on page 35:\n",
      "...rough nicc. type 1 teams were assigned a combined total of 609 \n",
      "days in 2008, down from 62 assignments and 805 assignment days in 2007. the record was set \n",
      "in 2002 when teams were assigned 85 times fo...\n"
     ]
    }
   ],
   "source": [
    "# Let's examine page 15 which might contain a table\n",
    "import json\n",
    "\n",
    "with open('annual_report_2008_508.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the content of page 15\n",
    "print(\"Content of page 15:\")\n",
    "print(data['15'])\n",
    "\n",
    "# Let's also look at a few more pages to find structured data\n",
    "# Pages that might contain statistics or summary data\n",
    "potential_data_pages = ['15', '20', '25', '30', '35', '40']\n",
    "\n",
    "# Check for keywords that might indicate important data sections\n",
    "keywords = ['summary', 'statistics', 'total', 'acres', 'fires', 'cost', 'budget', 'resources']\n",
    "\n",
    "for page_key in potential_data_pages:\n",
    "    if page_key in data:\n",
    "        page_text = data[page_key].lower()\n",
    "        found_keywords = [keyword for keyword in keywords if keyword in page_text]\n",
    "        if found_keywords:\n",
    "            print(f\"\\\n",
    "Page {page_key} contains keywords: {found_keywords}\")\n",
    "            # Print a snippet around the first keyword found\n",
    "            for keyword in found_keywords[:2]:  # Just check first two keywords\n",
    "                index = page_text.find(keyword)\n",
    "                if index >= 0:\n",
    "                    start = max(0, index - 50)\n",
    "                    end = min(len(page_text), index + 150)\n",
    "                    print(f\"\\\n",
    "Context around '{keyword}' on page {page_key}:\")\n",
    "                    print(f\"...{page_text[start:end]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4788cf34-6e62-4ff4-b86d-4ff55598ac7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with potential tables (first 10): []\n",
      "Pages with time dimension (first 10): []\n",
      "Pages with geographic dimension (first 10): []\n",
      "Pages with incident type dimension (first 10): []\n",
      "Best candidates for fact tables (pages with tables and multiple dimensions): []\n"
     ]
    }
   ],
   "source": [
    "# Let's look at more pages with potential tables and structured data\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open('annual_report_2008_508.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Let's identify key sections in the report that could form dimensions in our star schema\n",
    "# We'll look for patterns that indicate tables, summaries, or categorized data\n",
    "\n",
    "# First, let's identify potential fact tables (numerical data)\n",
    "# We'll look for pages with tables containing numbers, which could be our measures\n",
    "\n",
    "def has_table_structure(text):\n",
    "    # Check if text has patterns that look like a table\n",
    "    lines = text.split(' ')\n",
    "    num_lines_with_numbers = 0\n",
    "    for line in lines:\n",
    "        # If line has multiple numbers, it might be a table row\n",
    "        if re.search(r'\\\b\\d+\\\b.*\\\b\\d+\\\b', line):\n",
    "            num_lines_with_numbers += 1\n",
    "    \n",
    "    # If we have several lines with numbers, it might be a table\n",
    "    return num_lines_with_numbers >= 3\n",
    "\n",
    "# Let's identify potential dimension tables\n",
    "# 1. Time dimension - pages with year/date information\n",
    "# 2. Geographic dimension - pages with state/region information\n",
    "# 3. Incident type dimension - pages with fire type classifications\n",
    "\n",
    "def extract_potential_dimensions(text):\n",
    "    dimensions = {\n",
    "        'time': False,\n",
    "        'geographic': False,\n",
    "        'incident_type': False\n",
    "    }\n",
    "    \n",
    "    # Check for time dimension indicators\n",
    "    if re.search(r'\\\b(year|month|quarter|annual|2008|2007)\\\b', text, re.IGNORECASE):\n",
    "        dimensions['time'] = True\n",
    "    \n",
    "    # Check for geographic dimension indicators\n",
    "    if re.search(r'\\\b(state|region|area|county|GACC)\\\b', text, re.IGNORECASE):\n",
    "        dimensions['geographic'] = True\n",
    "    \n",
    "    # Check for incident type dimension indicators\n",
    "    if re.search(r'\\\b(wildfire|prescribed fire|fire use|incident type)\\\b', text, re.IGNORECASE):\n",
    "        dimensions['incident_type'] = True\n",
    "    \n",
    "    return dimensions\n",
    "\n",
    "# Analyze each page for potential star schema components\n",
    "schema_components = {}\n",
    "\n",
    "for page_key in data.keys():\n",
    "    page_text = data[page_key]\n",
    "    \n",
    "    # Check if page has table structure (potential fact table)\n",
    "    has_table = has_table_structure(page_text)\n",
    "    \n",
    "    # Check for potential dimensions\n",
    "    dimensions = extract_potential_dimensions(page_text)\n",
    "    \n",
    "    # Store the analysis\n",
    "    schema_components[page_key] = {\n",
    "        'has_table': has_table,\n",
    "        'dimensions': dimensions\n",
    "    }\n",
    "\n",
    "# Let's summarize our findings\n",
    "pages_with_tables = [page for page, info in schema_components.items() if info['has_table']]\n",
    "pages_with_time_dim = [page for page, info in schema_components.items() \n",
    "                      if info['dimensions']['time']]\n",
    "pages_with_geo_dim = [page for page, info in schema_components.items() \n",
    "                     if info['dimensions']['geographic']]\n",
    "pages_with_incident_dim = [page for page, info in schema_components.items() \n",
    "                          if info['dimensions']['incident_type']]\n",
    "\n",
    "print(f\"Pages with potential tables (first 10): {pages_with_tables[:10]}\")\n",
    "print(f\"Pages with time dimension (first 10): {pages_with_time_dim[:10]}\")\n",
    "print(f\"Pages with geographic dimension (first 10): {pages_with_geo_dim[:10]}\")\n",
    "print(f\"Pages with incident type dimension (first 10): {pages_with_incident_dim[:10]}\")\n",
    "\n",
    "# Let's examine a few pages that have both tables and multiple dimensions\n",
    "# These are good candidates for our fact tables\n",
    "good_candidates = [page for page, info in schema_components.items() \n",
    "                  if info['has_table'] and sum(info['dimensions'].values()) >= 2]\n",
    "\n",
    "print(f\"\\\n",
    "Best candidates for fact tables (pages with tables and multiple dimensions): {good_candidates[:10]}\")\n",
    "\n",
    "# Let's look at one of these good candidates in detail\n",
    "if good_candidates:\n",
    "    sample_page = good_candidates[0]\n",
    "    print(f\"\\\n",
    "Sample content from page {sample_page} (potential fact table):\")\n",
    "    print(data[sample_page])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3248176-a795-4c47-aa16-5dd1ee940120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text from page 15:\n",
      " \n",
      " 14\n",
      "Significant Incidents Over 40,000 Acres \n",
      " \n",
      "Name \n",
      "Inc. \n",
      "Type \n",
      "GACC \n",
      "State \n",
      "Start Date \n",
      "Contain or \n",
      "Control \n",
      "Date \n",
      "Size \n",
      "(Acres) \n",
      "Cause \n",
      "Cost \n",
      "Glass Fire \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "2/25/2008 \n",
      "3/2/2008 \n",
      "219,556 \n",
      "H \n",
      "NR \n",
      "Klamath Theater \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "9/26/2008 \n",
      "192,038 \n",
      "L \n",
      "$126,086,065 \n",
      "Basin Complex \n",
      "WF \n",
      "SO \n",
      "CA \n",
      "6/21/2008 \n",
      "7/29/2008 \n",
      "162,818 \n",
      "L \n",
      "$78,096,079 \n",
      "Iron & Alps \n",
      "Complexes \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "9/4/2008 \n",
      "105,805 \n",
      "L \n",
      "$73,974,917 \n",
      "Dunn Mtn. \n",
      "Assist \n",
      "WF \n",
      "NR \n",
      "MT \n",
      "8/21/2008 \n",
      "9/2/2008 \n",
      "102,383 \n",
      "L \n",
      "$2,900,000 \n",
      "Lime Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/20/2008 \n",
      "8/30/2008 \n",
      "99,585 \n",
      "L \n",
      "$59,329,698 \n",
      "Huckabee \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "4/30/2008 \n",
      "5/8/2008 \n",
      "98,200 \n",
      "U \n",
      "NR \n",
      "SHU Lightning \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "7/30/2008 \n",
      "86,500 \n",
      "L \n",
      "$56,438,391 \n",
      "Siskiyou / Blue 2 \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "9/13/2008 \n",
      "82,186 \n",
      "L \n",
      "$65,692,836 \n",
      "Indians \n",
      "WF \n",
      "SO \n",
      "CA \n",
      "6/8/2008 \n",
      "7/12/2008 \n",
      "76,554 \n",
      "H \n",
      "$42,500,000 \n",
      "Panther \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "7/22/2008 \n",
      "10/8/2008 \n",
      "72,344 \n",
      "L \n",
      "NR \n",
      "Gunbarrel \n",
      "WF \n",
      "RM \n",
      "WY \n",
      "7/26/2008 \n",
      "9/8/2008 \n",
      "68,148 \n",
      "L \n",
      "$11,200,000 \n",
      "Highway 322 \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "3/14/2008 \n",
      "3/19/2008 \n",
      "67,500 \n",
      "H \n",
      "NR \n",
      "Stiles Complex \n",
      "WF \n",
      "SW \n",
      "NM \n",
      "3/14/2008 \n",
      "3/16/2008 \n",
      "67,008 \n",
      "H \n",
      "$71,644 \n",
      "BTU Lightning \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 10/22/2008 \n",
      "64,995 \n",
      "L \n",
      "$94,825,683 \n",
      "Ukonom-South \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/20/2008 \n",
      "11/5/2008 \n",
      "58,871 \n",
      "L \n",
      "NR \n",
      "MEU Lightning \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/20/2008 \n",
      "12/1/2008 \n",
      "54,819 \n",
      "L \n",
      "$66,000,000 \n",
      "East Slide Rock \n",
      "Ridge \n",
      "WF \n",
      "WB \n",
      "NV \n",
      "8/10/2008 \n",
      "9/14/2008 \n",
      "54,549 \n",
      "L \n",
      "$8,873,000 \n",
      "Porter \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "3/14/2008 \n",
      "3/21/2008 \n",
      "51,400 \n",
      "NR \n",
      "NR \n",
      "Rocky \n",
      "WF \n",
      "SW \n",
      "NM \n",
      "6/18/2008 \n",
      "6/25/2008 \n",
      "49,132 \n",
      "L \n",
      "$2,158,700 \n",
      "Canyon \n",
      "Complex \n",
      "WF \n",
      "NO \n",
      "CA \n",
      "6/21/2008 \n",
      "11/3/2008 \n",
      "47,680 \n",
      "L \n",
      "NR \n",
      "Bridger Fire \n",
      "WF \n",
      "RM \n",
      "CO \n",
      "6/8/2008 \n",
      "7/9/2008 \n",
      "45,800 \n",
      "L \n",
      "NR \n",
      "Hughes Ranch \n",
      "WF \n",
      "SA \n",
      "TX \n",
      "6/4/2008 \n",
      "6/9/2008 \n",
      "45,241 \n",
      "U \n",
      "NR \n",
      "Evans Road \n",
      "WF \n",
      "SA \n",
      "NC \n",
      "6/1/2008 \n",
      "9/29/2008 \n",
      "40,704 \n",
      "L \n",
      "$18,249,415 \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Found 23 fire incidents using regex pattern\n",
      "Extracted table data (first 5 rows):\n",
      "                            Name Inc_Type GACC State Start_Date  \\\n",
      "0   \\nCause \\nCost \\nGlass Fire        WF   SA    TX  2/25/2008   \n",
      "1             \\nKlamath Theater        WF   NO    CA  6/21/2008   \n",
      "2               \\nBasin Complex        WF   SO    CA  6/21/2008   \n",
      "3     \\nIron & Alps \\nComplexes        WF   NO    CA  6/21/2008   \n",
      "4          \\nDunn Mtn. \\nAssist        WF   NR    MT  8/21/2008   \n",
      "\n",
      "  Contain_Control_Date Size_Acres Cause           Cost  \n",
      "0             3/2/2008    219,556     H            NR   \n",
      "1            9/26/2008    192,038     L  $126,086,065   \n",
      "2            7/29/2008    162,818     L   $78,096,079   \n",
      "3             9/4/2008    105,805     L   $73,974,917   \n",
      "4             9/2/2008    102,383     L    $2,900,000   \n",
      "Structured data saved to 'structured_incidents_data.json'\n"
     ]
    }
   ],
   "source": [
    "# Let's take a more careful approach to extract the table from page 15\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "with open('annual_report_2008_508.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Get the content of page 15 which has the table of significant incidents\n",
    "page_15_text = data['15']\n",
    "\n",
    "# Print the raw text to better understand the structure\n",
    "print(\"Raw text from page 15:\")\n",
    "print(page_15_text)\n",
    "\n",
    "# The table structure is complex due to PDF formatting\n",
    "# Let's try a different approach by identifying fire incident names and their associated data\n",
    "\n",
    "# Based on visual inspection, we can see that each row starts with a fire name\n",
    "# followed by data like WF, state code, dates, acres, etc.\n",
    "\n",
    "# Let's define a pattern to identify fire names and extract rows\n",
    "# Fire names are typically at the beginning of lines and are followed by WF (Wildfire)\n",
    "fire_pattern = r'([A-Za-z\\s&\\.]+)\\s+(WF)\\s+([A-Z]{2})\\s+([A-Z]{2})\\s+(\\d+/\\d+/\\d{4})\\s+(\\d+/\\d+/\\d{4})\\s+(\\d+,?\\d*)\\s+([A-Z])\\s+(.*)'\n",
    "\n",
    "# Extract all matches\n",
    "matches = re.findall(fire_pattern, page_15_text)\n",
    "\n",
    "# If we don't find matches with the complex pattern, try a simpler approach\n",
    "if not matches:\n",
    "    print(\"\\\n",
    "Complex pattern didn't match. Trying a simpler approach...\")\n",
    "    \n",
    "    # Let's manually identify the rows by looking for patterns like fire names followed by WF\n",
    "    lines = page_15_text.split('\\\n",
    "')\n",
    "    \n",
    "    # Define the expected columns based on visual inspection\n",
    "    columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date', \n",
    "               'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']\n",
    "    \n",
    "    # Initialize a list to store our structured data\n",
    "    structured_data = []\n",
    "    \n",
    "    # Process each line\n",
    "    current_row = {}\n",
    "    column_index = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Skip header lines\n",
    "        if \"Significant Incidents\" in line or \"Name\" in line or \"Inc.\" in line:\n",
    "            continue\n",
    "            \n",
    "        # If we find a line that looks like it starts a new fire entry\n",
    "        # (typically starts with a name followed by WF)\n",
    "        if \"WF\" in line and column_index == 0:\n",
    "            # If we have a current row with data, add it to our structured data\n",
    "            if current_row and len(current_row) > 0:\n",
    "                structured_data.append(current_row)\n",
    "                \n",
    "            # Start a new row\n",
    "            current_row = {}\n",
    "            column_index = 0\n",
    "            \n",
    "            # Split the line into tokens\n",
    "            tokens = line.split()\n",
    "            \n",
    "            # Process tokens to fill in columns\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                # Fire name might be multiple words\n",
    "                if column_index == 0:\n",
    "                    name_parts = []\n",
    "                    while i < len(tokens) and tokens[i] != \"WF\":\n",
    "                        name_parts.append(tokens[i])\n",
    "                        i += 1\n",
    "                    current_row['Name'] = \" \".join(name_parts)\n",
    "                \n",
    "                # Incident type (WF)\n",
    "                if i < len(tokens) and tokens[i] == \"WF\":\n",
    "                    current_row['Inc_Type'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # GACC (Geographic Area Coordination Center)\n",
    "                if column_index == 1 and i < len(tokens):\n",
    "                    current_row['GACC'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # State\n",
    "                if column_index == 2 and i < len(tokens):\n",
    "                    current_row['State'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # Start Date\n",
    "                if column_index == 3 and i < len(tokens):\n",
    "                    current_row['Start_Date'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # Contain/Control Date\n",
    "                if column_index == 4 and i < len(tokens):\n",
    "                    current_row['Contain_Control_Date'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # Size (Acres)\n",
    "                if column_index == 5 and i < len(tokens):\n",
    "                    current_row['Size_Acres'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # Cause\n",
    "                if column_index == 6 and i < len(tokens):\n",
    "                    current_row['Cause'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # Cost\n",
    "                if column_index == 7 and i < len(tokens):\n",
    "                    current_row['Cost'] = tokens[i]\n",
    "                    i += 1\n",
    "                    column_index += 1\n",
    "                \n",
    "                # If we've processed all columns, break\n",
    "                if column_index > 7:\n",
    "                    break\n",
    "        \n",
    "        # If we're in the middle of processing a row and find additional data\n",
    "        elif current_row and len(current_row) > 0:\n",
    "            # This might be continuation data for the current row\n",
    "            # For simplicity, we'll just append it to the notes\n",
    "            if 'Notes' not in current_row:\n",
    "                current_row['Notes'] = line\n",
    "            else:\n",
    "                current_row['Notes'] += \" \" + line\n",
    "    \n",
    "    # Add the last row if it has data\n",
    "    if current_row and len(current_row) > 0:\n",
    "        structured_data.append(current_row)\n",
    "    \n",
    "    # Create a DataFrame from the structured data\n",
    "    df = pd.DataFrame(structured_data)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"\\\n",
    "Extracted table data (first 5 rows):\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open('structured_incidents_data.json', 'w') as f:\n",
    "        json.dump({\"significant_incidents\": structured_data}, f, indent=4)\n",
    "    \n",
    "    print(\"\\\n",
    "Structured data saved to 'structured_incidents_data.json'\")\n",
    "else:\n",
    "    # If we found matches with the complex pattern\n",
    "    print(f\"\\\n",
    "Found {len(matches)} fire incidents using regex pattern\")\n",
    "    \n",
    "    # Convert matches to a DataFrame\n",
    "    columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date', \n",
    "               'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']\n",
    "    df = pd.DataFrame(matches, columns=columns)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"\\\n",
    "Extracted table data (first 5 rows):\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save to JSON\n",
    "    structured_data = df.to_dict(orient='records')\n",
    "    with open('structured_incidents_data.json', 'w') as f:\n",
    "        json.dump({\"significant_incidents\": structured_data}, f, indent=4)\n",
    "    \n",
    "    print(\"\\\n",
    "Structured data saved to 'structured_incidents_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecd4aaf4-646b-4335-b1a8-299aa63a9911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension tables saved to 'dimension_tables.json' with cleaned fire names and formatted dates\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'Date' column in time_dim to string to avoid JSON serialization issues\n",
    "\n",
    "time_dim['Date'] = time_dim['Date'].astype(str)\n",
    "\n",
    "# Now update our dimensions object with converted time_dim\n",
    "\n",
    "dimensions = {\n",
    "    'state_dimension': state_dim.to_dict(orient='records'),\n",
    "    'gacc_dimension': gacc_dim.to_dict(orient='records'),\n",
    "    'time_dimension': time_dim.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "with open('dimension_tables.json', 'w') as f:\n",
    "    json.dump(dimensions, f, indent=4)\n",
    "\n",
    "print(\"\\\n",
    "Dimension tables saved to 'dimension_tables.json' with cleaned fire names and formatted dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f44ad63-5eb6-4b48-9775-b55ebe079819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned fire names:\n",
      "1. Glass Fire\n",
      "2. Klamath Theater\n",
      "3. Basin Complex\n",
      "4. Iron & Alps \n",
      "Complexes\n",
      "5. Dunn Mtn. \n",
      "Assist\n",
      "6. Lime Complex\n",
      "7. Huckabee\n",
      "8. SHU Lightning \n",
      "Complex\n",
      "9. Complex\n",
      "10. Indians\n",
      "11. Panther\n",
      "12. Gunbarrel\n",
      "13. \n",
      "14. Stiles Complex\n",
      "15. BTU Lightning \n",
      "Complex\n",
      "16. South \n",
      "Complex\n",
      "17. MEU Lightning \n",
      "Complex\n",
      "18. East Slide Rock \n",
      "Ridge\n",
      "19. NR \n",
      "NR \n",
      "Rocky\n",
      "20. Canyon \n",
      "Complex\n",
      "21. Bridger Fire\n",
      "22. Hughes Ranch\n",
      "23. Evans Road\n",
      "Final structured data saved to 'structured_incidents_data.json'\n",
      "Final structured data (first 5 rows):\n",
      "                      Name Inc_Type GACC State Start_Date  \\\n",
      "0               Glass Fire       WF   SA    TX  2/25/2008   \n",
      "1          Klamath Theater       WF   NO    CA  6/21/2008   \n",
      "2            Basin Complex       WF   SO    CA  6/21/2008   \n",
      "3  Iron & Alps \\nComplexes       WF   NO    CA  6/21/2008   \n",
      "4       Dunn Mtn. \\nAssist       WF   NR    MT  8/21/2008   \n",
      "\n",
      "  Contain_Control_Date Size_Acres Cause  \n",
      "0             3/2/2008    219,556     H  \n",
      "1            9/26/2008    192,038     L  \n",
      "2            7/29/2008    162,818     L  \n",
      "3             9/4/2008    105,805     L  \n",
      "4             9/2/2008    102,383     L  \n",
      "State Dimension Table:\n",
      "  State_Code   Region\n",
      "0         TX  Unknown\n",
      "1         CA  Unknown\n",
      "2         MT  Unknown\n",
      "3         WY  Unknown\n",
      "4         NM  Unknown\n",
      "5         NV  Unknown\n",
      "6         CO  Unknown\n",
      "7         NC  Unknown\n",
      "GACC Dimension Table:\n",
      "  GACC_Code GACC_Name\n",
      "0        SA   Unknown\n",
      "1        NO   Unknown\n",
      "2        SO   Unknown\n",
      "3        NR   Unknown\n",
      "4        RM   Unknown\n",
      "5        SW   Unknown\n",
      "6        WB   Unknown\n",
      "Time Dimension Table (first 5 rows):\n",
      "   Date_ID        Date  Month  Year\n",
      "0        1  2008-02-25      2  2008\n",
      "1        2  2008-06-21      6  2008\n",
      "2        3  2008-06-21      6  2008\n",
      "3        4  2008-06-21      6  2008\n",
      "4        5  2008-08-21      8  2008\n",
      "Dimension tables saved to 'dimension_tables.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_20128\\3462682776.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df['Name'] = df['Name'].str.replace('^[_\\s]+', '', regex=True)  # Remove leading underscores and spaces\n",
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_20128\\3462682776.py:15: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df['Name'] = df['Name'].str.replace('[_\\s]+$', '', regex=True)  # Remove trailing underscores and spaces\n"
     ]
    }
   ],
   "source": [
    "# Let's fix the JSON serialization issues and clean up the fire names properly\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load our structured data\n",
    "with open('structured_incidents_data.json', 'r') as f:\n",
    "    structured_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(structured_data['significant_incidents'])\n",
    "\n",
    "# Clean up the fire names by removing any leading/trailing underscores, spaces, and other artifacts\n",
    "df['Name'] = df['Name'].str.replace('^[_\\s]+', '', regex=True)  # Remove leading underscores and spaces\n",
    "df['Name'] = df['Name'].str.replace('[_\\s]+$', '', regex=True)  # Remove trailing underscores and spaces\n",
    "df['Name'] = df['Name'].str.replace('_', ' ', regex=True)       # Replace remaining underscores with spaces\n",
    "\n",
    "# Fix specific issues with the first row which appears to have \"Cause Cost Glass Fire\"\n",
    "if 'Cause' in df.iloc[0]['Name'] and 'Cost' in df.iloc[0]['Name']:\n",
    "    df.iloc[0, df.columns.get_loc('Name')] = 'Glass Fire'\n",
    "\n",
    "# Display the cleaned fire names\n",
    "print(\"Cleaned fire names:\")\n",
    "for i, name in enumerate(df['Name']):\n",
    "    print(f\"{i+1}. {name}\")\n",
    "\n",
    "# Update our structured data with the cleaned names\n",
    "# Convert DataFrame to dict, ensuring all values are JSON serializable\n",
    "cleaned_data = []\n",
    "for _, row in df.iterrows():\n",
    "    row_dict = {}\n",
    "    for col in df.columns:\n",
    "        # Convert any non-serializable types to strings\n",
    "        row_dict[col] = str(row[col]) if not isinstance(row[col], (str, int, float, bool, type(None))) else row[col]\n",
    "    cleaned_data.append(row_dict)\n",
    "\n",
    "structured_data['significant_incidents'] = cleaned_data\n",
    "\n",
    "# Save the updated structured data\n",
    "with open('structured_incidents_data.json', 'w') as f:\n",
    "    json.dump(structured_data, f, indent=4)\n",
    "\n",
    "print(\"\\\n",
    "Final structured data saved to 'structured_incidents_data.json'\")\n",
    "\n",
    "# Let's also create a more complete view of the data for our star schema\n",
    "print(\"\\\n",
    "Final structured data (first 5 rows):\")\n",
    "print(df[['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date', 'Contain_Control_Date', 'Size_Acres', 'Cause']].head())\n",
    "\n",
    "# Let's also create a dimension table for the states\n",
    "states = df['State'].unique()\n",
    "state_dim = pd.DataFrame({\n",
    "    'State_Code': states,\n",
    "    'Region': ['Unknown'] * len(states)  # We would need additional data to fill this\n",
    "})\n",
    "\n",
    "# Create a dimension table for the GACCs (Geographic Area Coordination Centers)\n",
    "gaccs = df['GACC'].unique()\n",
    "gacc_dim = pd.DataFrame({\n",
    "    'GACC_Code': gaccs,\n",
    "    'GACC_Name': ['Unknown'] * len(gaccs)  # We would need additional data to fill this\n",
    "})\n",
    "\n",
    "# Create a time dimension from the start dates\n",
    "# First, ensure Start_Date is a string to avoid serialization issues\n",
    "if 'Start_Date' in df.columns:\n",
    "    # Try to convert to datetime first, then back to string in a consistent format\n",
    "    try:\n",
    "        df['Start_Date'] = pd.to_datetime(df['Start_Date'], errors='coerce')\n",
    "        # Extract month and year before converting back to string\n",
    "        months = df['Start_Date'].dt.month.fillna(0).astype(int).tolist()\n",
    "        years = df['Start_Date'].dt.year.fillna(0).astype(int).tolist()\n",
    "        # Convert back to string in ISO format\n",
    "        dates = df['Start_Date'].dt.strftime('%Y-%m-%d').fillna('Unknown').tolist()\n",
    "    except:\n",
    "        # If conversion fails, just use the original strings\n",
    "        dates = df['Start_Date'].tolist()\n",
    "        months = [0] * len(dates)\n",
    "        years = [0] * len(dates)\n",
    "else:\n",
    "    dates = ['Unknown'] * len(df)\n",
    "    months = [0] * len(df)\n",
    "    years = [0] * len(df)\n",
    "\n",
    "time_dim = pd.DataFrame({\n",
    "    'Date_ID': range(1, len(df) + 1),\n",
    "    'Date': dates,\n",
    "    'Month': months,\n",
    "    'Year': years\n",
    "})\n",
    "\n",
    "# Display our dimension tables\n",
    "print(\"\\\n",
    "State Dimension Table:\")\n",
    "print(state_dim)\n",
    "\n",
    "print(\"\\\n",
    "GACC Dimension Table:\")\n",
    "print(gacc_dim)\n",
    "\n",
    "print(\"\\\n",
    "Time Dimension Table (first 5 rows):\")\n",
    "print(time_dim.head())\n",
    "\n",
    "# Save these dimension tables to JSON\n",
    "# Convert DataFrames to dicts, ensuring all values are JSON serializable\n",
    "state_dim_dict = state_dim.to_dict(orient='records')\n",
    "gacc_dim_dict = gacc_dim.to_dict(orient='records')\n",
    "time_dim_dict = time_dim.to_dict(orient='records')\n",
    "\n",
    "dimensions = {\n",
    "    'state_dimension': state_dim_dict,\n",
    "    'gacc_dimension': gacc_dim_dict,\n",
    "    'time_dimension': time_dim_dict\n",
    "}\n",
    "\n",
    "with open('dimension_tables.json', 'w') as f:\n",
    "    json.dump(dimensions, f, indent=4)\n",
    "\n",
    "print(\"\\\n",
    "Dimension tables saved to 'dimension_tables.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2293ff2f-ae05-4bd2-8215-65ccbc6bccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dimension tables saved to 'dimension_tables.json' with properly formatted dates\n",
      "structured_incidents_data.json is valid JSON\n",
      "dimension_tables.json is valid JSON\n",
      "Sample from structured_incidents_data.json:\n",
      "{\n",
      "  \"Name\": \"Glass Fire\",\n",
      "  \"Inc_Type\": \"WF\",\n",
      "  \"GACC\": \"SA\",\n",
      "  \"State\": \"TX\",\n",
      "  \"Start_Date\": \"2/25/2008\",\n",
      "  \"Contain_Control_Date\": \"3/2/2008\",\n",
      "  \"Size_Acres\": \"219,556\",\n",
      "  \"Cause\": \"H\",\n",
      "  \"Cost\": \"NR \"\n",
      "}\n",
      "Sample from dimension_tables.json (time dimension):\n",
      "{\n",
      "  \"Date_ID\": 1,\n",
      "  \"Date\": \"2008-02-25\",\n",
      "  \"Month\": 2,\n",
      "  \"Year\": 2008\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's fix the JSON serialization issues one more time, ensuring all dates are properly converted to strings\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load our structured data\n",
    "with open('structured_incidents_data.json', 'r') as f:\n",
    "    structured_data = json.load(f)\n",
    "\n",
    "# Let's also create a time dimension from the start dates\n",
    "# Create a new time dimension table with proper string formatting\n",
    "time_dim_data = []\n",
    "for i, incident in enumerate(structured_data['significant_incidents']):\n",
    "    date_str = incident.get('Start_Date', 'Unknown')\n",
    "    \n",
    "    # Try to parse the date\n",
    "    try:\n",
    "        date_obj = pd.to_datetime(date_str)\n",
    "        month = date_obj.month\n",
    "        year = date_obj.year\n",
    "        formatted_date = date_obj.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        month = 0\n",
    "        year = 0\n",
    "        formatted_date = date_str\n",
    "    \n",
    "    time_dim_data.append({\n",
    "        'Date_ID': i + 1,\n",
    "        'Date': formatted_date,\n",
    "        'Month': month,\n",
    "        'Year': year\n",
    "    })\n",
    "\n",
    "# Load the other dimension tables\n",
    "with open('dimension_tables.json', 'r') as f:\n",
    "    dimensions = json.load(f)\n",
    "\n",
    "# Update the time dimension\n",
    "dimensions['time_dimension'] = time_dim_data\n",
    "\n",
    "# Save the updated dimension tables\n",
    "with open('dimension_tables.json', 'w') as f:\n",
    "    json.dump(dimensions, f, indent=4)\n",
    "\n",
    "print(\"Updated dimension tables saved to 'dimension_tables.json' with properly formatted dates\")\n",
    "\n",
    "# Let's verify the structure of our JSON files to ensure they're valid\n",
    "try:\n",
    "    with open('structured_incidents_data.json', 'r') as f:\n",
    "        incidents_data = json.load(f)\n",
    "    print(\"structured_incidents_data.json is valid JSON\")\n",
    "    \n",
    "    with open('dimension_tables.json', 'r') as f:\n",
    "        dim_tables = json.load(f)\n",
    "    print(\"dimension_tables.json is valid JSON\")\n",
    "    \n",
    "    # Display the first few records from each file to verify content\n",
    "    print(\"\\\n",
    "Sample from structured_incidents_data.json:\")\n",
    "    print(json.dumps(incidents_data['significant_incidents'][0], indent=2))\n",
    "    \n",
    "    print(\"\\\n",
    "Sample from dimension_tables.json (time dimension):\")\n",
    "    print(json.dumps(dim_tables['time_dimension'][0], indent=2))\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a64bd3-c689-42ba-858f-0b70537a11e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c680b9-b418-4a5b-aee9-2b9e1ade364d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
