{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77cc153-b62e-4e21-b900-84e801342282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 structured incident files in ./structurejson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 34.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No 'significant_incidents' key in structured_incidents_data_2013.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2016.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2017.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2018.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2019.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2020.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2021.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2022.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2023.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2024.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total incidents collected: 206\n",
      "\n",
      "Incidents per file:\n",
      "structured_incidents_data.json: 23 incidents\n",
      "structured_incidents_data_2009.json: 27 incidents\n",
      "structured_incidents_data_2010.json: 9 incidents\n",
      "structured_incidents_data_2011.json: 41 incidents\n",
      "structured_incidents_data_2012.json: 51 incidents\n",
      "structured_incidents_data_2014.json: 9 incidents\n",
      "structured_incidents_data_2015.json: 46 incidents\n",
      "\n",
      "Saved combined dataset to 'combined_incidents.csv'\n",
      "\n",
      "Verified CSV file exists with 206 rows and 11 columns\n",
      "\n",
      "CSV file columns:\n",
      "['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date', 'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost', 'source_file', 'Last_Report_Date']\n",
      "\n",
      "First few rows of the CSV file:\n",
      "                      Name Inc_Type GACC State  Start_Date  \\\n",
      "0               Glass Fire       WF   SA    TX  2008-02-25   \n",
      "1          Klamath Theater       WF   NO    CA  2008-06-21   \n",
      "2            Basin Complex       WF   SO    CA  2008-06-21   \n",
      "3  Iron & Alps \\nComplexes       WF   NO    CA  2008-06-21   \n",
      "4       Dunn Mtn. \\nAssist       WF   NR    MT  2008-08-21   \n",
      "\n",
      "  Contain_Control_Date  Size_Acres Cause         Cost  \\\n",
      "0           2008-03-02    219556.0     H          NaN   \n",
      "1           2008-09-26    192038.0     L  126086065.0   \n",
      "2           2008-07-29    162818.0     L   78096079.0   \n",
      "3           2008-09-04    105805.0     L   73974917.0   \n",
      "4           2008-09-02    102383.0     L    2900000.0   \n",
      "\n",
      "                      source_file Last_Report_Date  \n",
      "0  structured_incidents_data.json              NaN  \n",
      "1  structured_incidents_data.json              NaN  \n",
      "2  structured_incidents_data.json              NaN  \n",
      "3  structured_incidents_data.json              NaN  \n",
      "4  structured_incidents_data.json              NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "from datetime import datetime  \n",
    "import os  \n",
    "from tqdm import tqdm  \n",
    "  \n",
    "def clean_incident_data(incident):  \n",
    "    \"\"\"Clean and standardize a single incident record with improved date handling\"\"\"  \n",
    "    cleaned = {}  \n",
    "      \n",
    "    # Copy all fields with standardized names  \n",
    "    for key, value in incident.items():  \n",
    "        # Standardize key names (remove dots, and replace spaces with underscores)  \n",
    "        clean_key = key.replace('.', '').replace(' ', '_')  \n",
    "        cleaned[clean_key] = value  \n",
    "      \n",
    "    # Clean date fields - convert to proper datetime objects  \n",
    "    date_fields = ['Start_Date', 'Contain_Control_Date']  \n",
    "    for date_field in date_fields:  \n",
    "        if date_field in cleaned and cleaned[date_field]:  \n",
    "            date_value = cleaned[date_field]  \n",
    "            try:  \n",
    "                # Try different date formats:  \n",
    "                if re.match(r'\\d{1,2}-[A-Za-z]{3}-\\d{2}', date_value):  # e.g., \"10-Jul-09\"  \n",
    "                    date_obj = datetime.strptime(date_value, '%d-%b-%y')  \n",
    "                elif re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_value):  # e.g., \"2/25/2008\"  \n",
    "                    date_obj = datetime.strptime(date_value, '%m/%d/%Y')  \n",
    "                else:  \n",
    "                    # Format unknown; set to None  \n",
    "                    cleaned[date_field] = None  \n",
    "                    continue  \n",
    "                cleaned[date_field] = date_obj  \n",
    "            except Exception as e:  \n",
    "                print(\"Error parsing date \" + date_value + \": \" + str(e))  \n",
    "                cleaned[date_field] = None  \n",
    "  \n",
    "    # Clean numeric fields  \n",
    "    # Size_Acres  \n",
    "    if 'Size_Acres' in cleaned and cleaned['Size_Acres']:  \n",
    "        try:  \n",
    "            size_str = str(cleaned['Size_Acres']).replace(',', '')  \n",
    "            cleaned['Size_Acres'] = float(size_str)  \n",
    "        except:  \n",
    "            cleaned['Size_Acres'] = None  \n",
    "  \n",
    "    # Cost field  \n",
    "    if 'Cost' in cleaned and cleaned['Cost']:  \n",
    "        try:  \n",
    "            # Remove $, commas, spaces and convert to float  \n",
    "            cost_str = str(cleaned['Cost']).replace('$', '').replace(',', '').replace(' ', '')  \n",
    "            if cost_str.lower() not in ['nr', 'n/a', 'unknown']:  \n",
    "                cleaned['Cost'] = float(cost_str)  \n",
    "            else:  \n",
    "                cleaned['Cost'] = None  \n",
    "        except:  \n",
    "            cleaned['Cost'] = None  \n",
    "  \n",
    "    return cleaned  \n",
    "  \n",
    "# Set the path to the folder containing structured incident files.  \n",
    "folder_path = './structurejson'  \n",
    "structured_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.startswith('structured_incidents_data')]  \n",
    "  \n",
    "print(\"Found \" + str(len(structured_files)) + \" structured incident files in \" + folder_path)  \n",
    "  \n",
    "all_incidents = []  \n",
    "file_counts = {}  \n",
    "  \n",
    "for file_path in tqdm(structured_files):  \n",
    "    file_name = os.path.basename(file_path)  \n",
    "    try:  \n",
    "        with open(file_path, 'r') as f:  \n",
    "            data = json.load(f)  \n",
    "          \n",
    "        if 'significant_incidents' in data:  \n",
    "            incidents = data['significant_incidents']  \n",
    "            cleaned_incidents = [clean_incident_data(incident) for incident in incidents]  \n",
    "              \n",
    "            # Add source file info  \n",
    "            for incident in cleaned_incidents:  \n",
    "                incident['source_file'] = file_name  \n",
    "            all_incidents.extend(cleaned_incidents)  \n",
    "            file_counts[file_name] = len(cleaned_incidents)  \n",
    "        else:  \n",
    "            print(\"Warning: No 'significant_incidents' key in \" + file_name)  \n",
    "    except Exception as e:  \n",
    "        print(\"Error processing \" + file_name + \": \" + str(e))  \n",
    "  \n",
    "# Convert to DataFrame  \n",
    "incidents_df = pd.DataFrame(all_incidents)  \n",
    "  \n",
    "# Display summary  \n",
    "print(\"\\nTotal incidents collected:\", len(incidents_df))  \n",
    "print(\"\\nIncidents per file:\")  \n",
    "for file_name, count in file_counts.items():  \n",
    "    print(file_name + \": \" + str(count) + \" incidents\")  \n",
    "  \n",
    "# Save the combined dataset  \n",
    "incidents_df.to_csv('combined_incidents.csv', index=False)  \n",
    "print(\"\\nSaved combined dataset to 'combined_incidents.csv'\")  \n",
    "  \n",
    "# Verify the CSV file was created and check its contents  \n",
    "if os.path.exists('combined_incidents.csv'):  \n",
    "    df_from_csv = pd.read_csv('combined_incidents.csv')  \n",
    "    print(\"\\nVerified CSV file exists with \" + str(len(df_from_csv)) + \" rows and \" + str(len(df_from_csv.columns)) + \" columns\")  \n",
    "    print(\"\\nCSV file columns:\")  \n",
    "    print(df_from_csv.columns.tolist())  \n",
    "    print(\"\\nFirst few rows of the CSV file:\")  \n",
    "    print(df_from_csv.head())  \n",
    "else:  \n",
    "    print(\"\\nError: CSV file was not created\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f94363-31d2-4db8-8184-6e1fece45de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for names with asterisks or unusual patterns:\n",
      "Row 38: Rex Creek *\n",
      "Row 46: Little Black One **\n",
      "Row 47: Crazy Mountain\n",
      "Complex **\n",
      "Row 48: Minto Flats South *\n",
      "Row 49: Railbelt Complex *\n",
      "After filling missing name:\n",
      "Name          Unknown Incident\n",
      "State                       TX\n",
      "GACC                        SA\n",
      "Start_Date          2008-03-14\n",
      "Size_Acres             67500.0\n",
      "Name: 12, dtype: object\n",
      "Remaining missing names: 0\n",
      "Saved updated dataframe to 'combined_incidents_cleaned.csv'\n",
      "Sample of cleaned data:\n",
      "            Name State GACC  Start_Date  Size_Acres Cause\n",
      "30          Cato    NM   SW  2009-06-10     55080.0     L\n",
      "65       Wildcat    TX   SA  2011-04-11    159308.0     L\n",
      "66   Las Conchas    NM   SW  2011-06-26    156593.0     H\n",
      "39  Wood River 1    AK   AK  2009-07-12    125382.0     L\n",
      "8        Complex    CA   NO  2008-06-21     82186.0     L\n"
     ]
    }
   ],
   "source": [
    "# Load the combined_incidents.csv file\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('combined_incidents.csv')\n",
    "\n",
    "# Check for rows with names containing asterisks or unusual patterns\n",
    "print(\"Checking for names with asterisks or unusual patterns:\")\n",
    "for idx, name in enumerate(df['Name']):\n",
    "    if isinstance(name, str) and ('*' in name or re.match(r'^\\d+$', name)):\n",
    "        print(f\"Row {idx}: {name}\")\n",
    "\n",
    "# Fill the missing name with a placeholder\n",
    "df['Name'] = df['Name'].fillna('Unknown Incident')\n",
    "\n",
    "# Check if the placeholder was applied correctly\n",
    "print(\"\\\n",
    "After filling missing name:\")\n",
    "print(df.iloc[12][['Name', 'State', 'GACC', 'Start_Date', 'Size_Acres']])\n",
    "\n",
    "# Check if there are any remaining missing names\n",
    "print(f\"\\\n",
    "Remaining missing names: {df['Name'].isnull().sum()}\")\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv('combined_incidents_cleaned.csv', index=False)\n",
    "print(\"\\\n",
    "Saved updated dataframe to 'combined_incidents_cleaned.csv'\")\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "print(\"\\\n",
    "Sample of cleaned data:\")\n",
    "print(df.sample(5)[['Name', 'State', 'GACC', 'Start_Date', 'Size_Acres', 'Cause']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b46dba-852f-4d59-bdea-130427c45d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of cleaned names:\n",
      "24            Bluff Creek\n",
      "49       Railbelt Complex\n",
      "164            North Star\n",
      "126    Powell SBW Complex\n",
      "168      Okanogan Complex\n",
      "137                Seeley\n",
      "96           Matador West\n",
      "121              Wellnitz\n",
      "80                Prairie\n",
      "158          July Complex\n",
      "Name: Name, dtype: object\n",
      "Saved updated dataframe to combined_incidents_cleaned_no_special_names.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV file\n",
    "df = pd.read_csv('combined_incidents_cleaned.csv')\n",
    "\n",
    "# Define a function to clean the name field by removing asterisks and trailing numbers\n",
    "import re\n",
    "\n",
    "def clean_name(name):\n",
    "    if pd.isnull(name):\n",
    "        return name\n",
    "    # Remove asterisks\n",
    "    name_cleaned = re.sub(r'\\*+', '', name)\n",
    "    # Remove trailing numbers (e.g., ' Wood River 1' becomes ' Wood River')\n",
    "    name_cleaned = re.sub(r'\\s+\\d+$', '', name_cleaned)\n",
    "    # Strip extra whitespace\n",
    "    return name_cleaned.strip()\n",
    "\n",
    "# Apply cleaning function to Name column\n",
    "df['Name'] = df['Name'].apply(clean_name)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "df.to_csv('combined_incidents_cleaned_no_special_names.csv', index=False)\n",
    "\n",
    "# Show a sample of the cleaned names\n",
    "print('Sample of cleaned names:')\n",
    "print(df['Name'].sample(10))\n",
    "\n",
    "print('\\\n",
    "Saved updated dataframe to combined_incidents_cleaned_no_special_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f38cf4a7-940e-4a9d-8475-eed1320670d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for structured_incidents_data files in the current directory...\n",
      "Found 0 structured_incidents_data files in the current directory.\n",
      "Processing 0 structured incident files from 2012 onward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 incident files from 2012 onward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed. Original files were backed up with .bak extension.\n",
      "Verifying changes in structured files:\n",
      "Verifying changes in incident files:\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# First, let's find where the structured_incidents_data files are located\n",
    "print(\"Looking for structured_incidents_data files in the current directory...\")\n",
    "\n",
    "# List all structured_incidents_data files in the current directory\n",
    "structured_files = [f for f in os.listdir('.') if f.startswith('structured_incidents_data_') and f.endswith('.json')]\n",
    "print(f\"Found {len(structured_files)} structured_incidents_data files in the current directory.\")\n",
    "\n",
    "# Function to fix partial dates by adding the year\n",
    "def fix_date(date_str, year):\n",
    "    if not date_str or pd.isna(date_str) or (isinstance(date_str, str) and date_str.strip() == \"\"):\n",
    "        return None\n",
    "    \n",
    "    # If it's already a full date (contains a year), return as is\n",
    "    if isinstance(date_str, str) and re.search(r'\\d{4}', date_str):\n",
    "        return date_str\n",
    "    \n",
    "    # Handle various date formats\n",
    "    if isinstance(date_str, str) and re.match(r'\\d{1,2}/\\d{1,2}', date_str):  # Format: M/D\n",
    "        return f\"{date_str}/{year}\"\n",
    "    elif isinstance(date_str, str) and re.match(r'\\d{1,2}-[A-Za-z]{3}', date_str):  # Format: D-Mon\n",
    "        return f\"{date_str}-{year}\"\n",
    "    elif isinstance(date_str, str) and re.match(r'[A-Za-z]+', date_str):  # Format: Month name only\n",
    "        return f\"{date_str} {year}\"\n",
    "    else:\n",
    "        # If format is unknown, return as is\n",
    "        return date_str\n",
    "\n",
    "# Process structured files from 2012 onward\n",
    "structured_pattern = re.compile(r'^structured_incidents_data_(\\d{4})\\.json$')\n",
    "structured_files_2012_on = []\n",
    "\n",
    "for f in structured_files:\n",
    "    m = structured_pattern.match(f)\n",
    "    if m and int(m.group(1)) >= 2012:\n",
    "        structured_files_2012_on.append(f)\n",
    "\n",
    "print(f\"Processing {len(structured_files_2012_on)} structured incident files from 2012 onward\")\n",
    "\n",
    "# Process each structured file\n",
    "for file_name in tqdm(structured_files_2012_on):\n",
    "    year = int(structured_pattern.match(file_name).group(1))\n",
    "    file_path = os.path.join('.', file_name)\n",
    "    \n",
    "    # Load the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Check if data is a list or dict\n",
    "    if isinstance(data, list):\n",
    "        # Process each record\n",
    "        for record in data:\n",
    "            if isinstance(record, dict):\n",
    "                # Fix date fields\n",
    "                if 'Start_Date' in record:\n",
    "                    record['Start_Date'] = fix_date(record['Start_Date'], year)\n",
    "                \n",
    "                # Handle different field names for control date\n",
    "                control_date_fields = ['Contain_Control_Date', 'Control_Date']\n",
    "                for field in control_date_fields:\n",
    "                    if field in record:\n",
    "                        record[field] = fix_date(record[field], year)\n",
    "    elif isinstance(data, dict):\n",
    "        # If it's a dict, look for lists of records\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, list):\n",
    "                for record in value:\n",
    "                    if isinstance(record, dict):\n",
    "                        # Fix date fields\n",
    "                        if 'Start_Date' in record:\n",
    "                            record['Start_Date'] = fix_date(record['Start_Date'], year)\n",
    "                        \n",
    "                        # Handle different field names for control date\n",
    "                        control_date_fields = ['Contain_Control_Date', 'Control_Date']\n",
    "                        for field in control_date_fields:\n",
    "                            if field in record:\n",
    "                                record[field] = fix_date(record[field], year)\n",
    "    \n",
    "    # Create a backup of the original file if it doesn't already exist\n",
    "    backup_file = file_path + '.bak'\n",
    "    if not os.path.exists(backup_file):\n",
    "        os.rename(file_path, backup_file)\n",
    "        # Save the updated data back to the original file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"Processed structured file {file_name} (backup saved as {backup_file})\")\n",
    "    else:\n",
    "        # If backup already exists, just overwrite the original file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"Processed structured file {file_name} (backup already existed)\")\n",
    "\n",
    "# Now let's do the same for the incident files\n",
    "incident_pattern = re.compile(r'^incidents_(\\d{4})\\.json$')\n",
    "incident_files = [f for f in os.listdir('.') if incident_pattern.match(f)]\n",
    "incident_files_2012_on = [f for f in incident_files if int(incident_pattern.match(f).group(1)) >= 2012]\n",
    "\n",
    "print(f\"\\\n",
    "Processing {len(incident_files_2012_on)} incident files from 2012 onward\")\n",
    "\n",
    "# Process each incident file\n",
    "for file_name in tqdm(incident_files_2012_on):\n",
    "    year = int(incident_pattern.match(file_name).group(1))\n",
    "    file_path = os.path.join('.', file_name)\n",
    "    \n",
    "    # Load the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Process each record\n",
    "    for record in data:\n",
    "        # Fix date fields\n",
    "        if 'Start_Date' in record:\n",
    "            record['Start_Date'] = fix_date(record['Start_Date'], year)\n",
    "        \n",
    "        # Handle different field names for control date\n",
    "        control_date_fields = ['Contain/_Control Date', 'Contain_Control_Date', 'Control_Date']\n",
    "        for field in control_date_fields:\n",
    "            if field in record:\n",
    "                record[field] = fix_date(record[field], year)\n",
    "    \n",
    "    # Create a backup of the original file if it doesn't already exist\n",
    "    backup_file = file_path + '.bak'\n",
    "    if not os.path.exists(backup_file):\n",
    "        os.rename(file_path, backup_file)\n",
    "        # Save the updated data back to the original file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"Processed {len(data)} records in {file_name} (backup saved as {backup_file})\")\n",
    "    else:\n",
    "        # If backup already exists, just overwrite the original file\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"Processed {len(data)} records in {file_name} (backup already existed)\")\n",
    "\n",
    "print(\"\\\n",
    "All files have been processed. Original files were backed up with .bak extension.\")\n",
    "\n",
    "# Let's verify the changes by examining a few records from each file type\n",
    "print(\"\\\n",
    "Verifying changes in structured files:\")\n",
    "for file_name in structured_files_2012_on[:2]:  # Check first 2 files\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, list) and len(data) > 0:\n",
    "        print(f\"\\\n",
    "Sample from {file_name}:\")\n",
    "        for i, record in enumerate(data[:3]):  # Show first 3 records\n",
    "            if 'Start_Date' in record:\n",
    "                print(f\"Record {i+1} Start_Date: {record['Start_Date']}\")\n",
    "            if 'Name' in record:\n",
    "                print(f\"Record {i+1} Name: {record['Name']}\")\n",
    "    elif isinstance(data, dict):\n",
    "        print(f\"\\\n",
    "Sample from {file_name} (dictionary format):\")\n",
    "        for key, value in list(data.items())[:2]:  # Show first 2 keys\n",
    "            print(f\"Key: {key}\")\n",
    "            if isinstance(value, list) and len(value) > 0:\n",
    "                for i, record in enumerate(value[:2]):  # Show first 2 records\n",
    "                    if isinstance(record, dict):\n",
    "                        if 'Start_Date' in record:\n",
    "                            print(f\"  Record {i+1} Start_Date: {record['Start_Date']}\")\n",
    "                        if 'Name' in record:\n",
    "                            print(f\"  Record {i+1} Name: {record['Name']}\")\n",
    "\n",
    "print(\"\\\n",
    "Verifying changes in incident files:\")\n",
    "for file_name in incident_files_2012_on[:2]:  # Check first 2 files\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        print(f\"\\\n",
    "Sample from {file_name}:\")\n",
    "        for i, record in enumerate(data[:3]):  # Show first 3 records\n",
    "            if 'Start_Date' in record:\n",
    "                print(f\"Record {i+1} Start_Date: {record['Start_Date']}\")\n",
    "            if 'Name' in record:\n",
    "                print(f\"Record {i+1} Name: {record['Name']}\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04a16bd7-4afc-4160-b07e-652f07cb1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files in the root directory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying changes in a sample file:\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
