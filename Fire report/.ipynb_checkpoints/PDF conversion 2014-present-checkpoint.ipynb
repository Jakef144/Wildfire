{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797322a-ab12-461e-bd5b-f3bc5cd610c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2014 PDF  \n",
    "pdf_path = 'Annual_Report_2014_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (similar to previous years)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2014 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the given pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting the column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Use the found table for processing  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table found:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names (replace newlines with underscore)  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw data to JSON  \n",
    "    raw_json = incidents_table.to_dict(orient='records')  \n",
    "    raw_filename = 'incidents_2014.json'  \n",
    "    with open(raw_filename, 'w') as f:  \n",
    "        json.dump(raw_json, f, indent=4)  \n",
    "    print(\"\\nSaved raw data to \" + raw_filename)  \n",
    "      \n",
    "    # Create a mapping for column names to standardize them  \n",
    "    column_mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        col_lower = col.lower()  \n",
    "        if 'name' in col_lower:  \n",
    "            column_mapping[col] = 'Name'  \n",
    "        elif 'type' in col_lower:  \n",
    "            column_mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in col_lower:  \n",
    "            column_mapping[col] = 'GACC'  \n",
    "        elif 'state' in col_lower:  \n",
    "            column_mapping[col] = 'State'  \n",
    "        elif 'start' in col_lower and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Start_Date'  \n",
    "        elif ('contain' in col_lower or 'control' in col_lower) and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Contain_Control_Date'  \n",
    "        elif 'size' in col_lower or 'acres' in col_lower:  \n",
    "            column_mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in col_lower:  \n",
    "            column_mapping[col] = 'Cause'  \n",
    "        elif 'cost' in col_lower:  \n",
    "            column_mapping[col] = 'Cost'  \n",
    "      \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(column_mapping)  \n",
    "      \n",
    "    # Apply the mapping to create a structured table  \n",
    "    structured_table = incidents_table.rename(columns=column_mapping)  \n",
    "      \n",
    "    # Add any missing columns with None values  \n",
    "    required_columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date',   \n",
    "                       'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']  \n",
    "    for col in required_columns:  \n",
    "        if col not in structured_table.columns:  \n",
    "            structured_table[col] = None  \n",
    "      \n",
    "    # Convert to list of dictionaries for JSON  \n",
    "    structured_data = structured_table.to_dict(orient='records')  \n",
    "      \n",
    "    # Create the structured JSON  \n",
    "    structured_json = {  \n",
    "        \"significant_incidents\": structured_data  \n",
    "    }  \n",
    "      \n",
    "    # Save the structured data to JSON  \n",
    "    structured_filename = 'structured_incidents_data_2014.json'  \n",
    "    with open(structured_filename, 'w') as f:  \n",
    "        json.dump(structured_json, f, indent=4)  \n",
    "    print(\"\\nSaved structured data to \" + structured_filename)  \n",
    "    print(\"Sample incident:\")  \n",
    "    print(json.dumps(structured_data[0], indent=2))  \n",
    "      \n",
    "    # Create dimension tables  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension  \n",
    "    inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "    inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension (using Start_Date values)  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2014.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"\\nCreated dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "      \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "      \n",
    "    # If we can't find the table automatically, let's try to look for text that might indicate where it is  \n",
    "    for page_num in range(start_page, end_page + 1):  \n",
    "        page = doc[page_num]  \n",
    "        text = page.get_text()  \n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "            print(\"\\nPage \" + str(page_num+1) + \" contains text about significant fires/incidents.\")  \n",
    "            print(\"First 200 characters of the page:\")  \n",
    "            print(text[:200])  \n",
    "  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b991ec9-2a10-4767-8f03-8c094512a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2015 PDF  \n",
    "pdf_path = 'annual_report_2015_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (similar to previous years)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2015 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the given pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting the column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Use the found table for processing  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table found:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names (replace newlines with underscore and strip whitespace)  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw table data to JSON  \n",
    "    raw_json = incidents_table.to_dict(orient='records')  \n",
    "    raw_filename = 'incidents_2015.json'  \n",
    "    with open(raw_filename, 'w') as f:  \n",
    "        json.dump(raw_json, f, indent=4)  \n",
    "    print(\"\\nSaved raw table data to \" + raw_filename)  \n",
    "      \n",
    "    # Create a mapping for column names to standardize them  \n",
    "    column_mapping = {}  \n",
    "      \n",
    "    # Inspect the actual columns to create the mapping  \n",
    "    print(\"\\nActual columns in the 2015 table:\")  \n",
    "    print(incidents_table.columns.tolist())  \n",
    "      \n",
    "    # Try to automatically map columns based on keywords  \n",
    "    for col in incidents_table.columns:  \n",
    "        col_lower = col.lower()  \n",
    "        if 'name' in col_lower:  \n",
    "            column_mapping[col] = 'Name'  \n",
    "        elif 'type' in col_lower:  \n",
    "            column_mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in col_lower:  \n",
    "            column_mapping[col] = 'GACC'  \n",
    "        elif 'state' in col_lower:  \n",
    "            column_mapping[col] = 'State'  \n",
    "        elif 'start' in col_lower and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Start_Date'  \n",
    "        elif ('contain' in col_lower or 'control' in col_lower) and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Contain_Control_Date'  \n",
    "        elif 'size' in col_lower or 'acres' in col_lower:  \n",
    "            column_mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in col_lower:  \n",
    "            column_mapping[col] = 'Cause'  \n",
    "        elif 'cost' in col_lower:  \n",
    "            column_mapping[col] = 'Cost'  \n",
    "      \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(column_mapping)  \n",
    "      \n",
    "    # Apply the mapping if it's complete enough  \n",
    "    if len(column_mapping) >= 5:  # Assuming we need at least 5 key columns  \n",
    "        structured_table = incidents_table.rename(columns=column_mapping)  \n",
    "          \n",
    "        # For any missing columns in our mapping, add them with None values  \n",
    "        required_columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date',   \n",
    "                           'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']  \n",
    "          \n",
    "        for col in required_columns:  \n",
    "            if col not in structured_table.columns:  \n",
    "                structured_table[col] = None  \n",
    "                print(\"Added missing column: \" + col)  \n",
    "          \n",
    "        # Convert to list of dictionaries for JSON  \n",
    "        structured_data = structured_table.to_dict(orient='records')  \n",
    "          \n",
    "        # Create the structured data format  \n",
    "        structured_json = {  \n",
    "            \"significant_incidents\": structured_data  \n",
    "        }  \n",
    "          \n",
    "        # Save to JSON  \n",
    "        structured_filename = 'structured_incidents_data_2015.json'  \n",
    "        with open(structured_filename, 'w') as f:  \n",
    "            json.dump(structured_json, f, indent=4)  \n",
    "          \n",
    "        print(\"\\nSaved structured data to \" + structured_filename)  \n",
    "        print(\"Sample incident:\")  \n",
    "        print(json.dumps(structured_data[0], indent=2))  \n",
    "          \n",
    "        # Create dimension tables  \n",
    "        # State dimension  \n",
    "        states = structured_table['State'].dropna().unique()  \n",
    "        state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "          \n",
    "        # GACC dimension  \n",
    "        gaccs = structured_table['GACC'].dropna().unique()  \n",
    "        gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "          \n",
    "        # Incident Type dimension  \n",
    "        inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "          \n",
    "        # Cause dimension  \n",
    "        causes = structured_table['Cause'].dropna().unique()  \n",
    "        cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "          \n",
    "        # Time dimension (using Start_Date values)  \n",
    "        dates = structured_table['Start_Date'].dropna().unique()  \n",
    "        time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "          \n",
    "        dimension_tables = {  \n",
    "            'state_dimension': state_dimension,  \n",
    "            'gacc_dimension': gacc_dimension,  \n",
    "            'inc_type_dimension': inc_type_dimension,  \n",
    "            'cause_dimension': cause_dimension,  \n",
    "            'time_dimension': time_dimension  \n",
    "        }  \n",
    "          \n",
    "        dimension_filename = 'dimension_tables_2015.json'  \n",
    "        with open(dimension_filename, 'w') as f:  \n",
    "            json.dump(dimension_tables, f, indent=4)  \n",
    "        print(\"\\nCreated dimension tables and saved to \" + dimension_filename)  \n",
    "        print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "        print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "        print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "        print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "        print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "    else:  \n",
    "        print(\"\\nCould not create a complete column mapping. Manual mapping needed.\")  \n",
    "          \n",
    "        # Let's create a more detailed view of the table to help with manual mapping  \n",
    "        print(\"\\nDetailed view of the table:\")  \n",
    "        print(incidents_table.head(5))  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "      \n",
    "    # If we can't find the table automatically, let's try to look for text that might indicate where it is  \n",
    "    for page_num in range(start_page, end_page + 1):  \n",
    "        page = doc[page_num]  \n",
    "        text = page.get_text()  \n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "            print(\"\\nPage \" + str(page_num+1) + \" contains text about significant fires/incidents.\")  \n",
    "            print(\"First 200 characters of the page:\")  \n",
    "            print(text[:200])  \n",
    "  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a613343-0d8b-4c8c-8e2e-dc847b92cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2016 PDF...\n",
      "Found 4 tables on pages: [10, 11, 13]\n",
      "Table 1 columns: international:, through the nifc-ciffc agreement the u.s. provided two heavy air, col2\n",
      "Table 2 columns: through the nifc-ciffc agreement canada provided two air tanker groups to the u.s., col1\n",
      "Table 3 columns: name, gacc, state, start\n",
      "date, last\n",
      "report\n",
      "date, size in\n",
      "acres, cause*, estimated\n",
      "cost\n",
      "\n",
      "Table 3 on page 13 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Last\\nReport\\nDate', 'Size In\\nAcres', 'Cause*', 'Estimated\\nCost']\n",
      "Significant incidents table extracted:\n",
      "             Name GACC State Start\\nDate Last\\nReport\\nDate Size In\\nAcres  \\\n",
      "0  Anderson Creek   SA    OK        3/23                4/4        367,740   \n",
      "1         Pioneer   GB    ID        7/18              10/27        188,404   \n",
      "2        Range 12   NW    WA        7/30                8/8        176,600   \n",
      "3       Soberanes   SO    CA        7/22              10/23        132,127   \n",
      "4         Hot Pot   GB    NV         7/2               7/10        122,292   \n",
      "\n",
      "  Cause* Estimated\\nCost  \n",
      "0      U      $1,750,000  \n",
      "1      U     $90,000,000  \n",
      "2      U      $1,700,000  \n",
      "3      H    $262,500,000  \n",
      "4      U      $3,402,259  \n",
      "Raw incidents table saved to incidents_2016.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Last_Report_Date': 'Last_Report_Date', 'Size In_Acres': 'Size_Acres', 'Cause*': 'Cause', 'Estimated_Cost': 'Estimated_Cost'}\n",
      "Structured incidents data saved to structured_incidents_data_2016.json\n",
      "Created dimension tables and saved to dimension_tables_2016.json\n",
      "- State dimension: 12 states\n",
      "- GACC dimension: 7 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 19 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2016 PDF  \n",
    "pdf_path = 'annual_report_2016_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (similar to previous years)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2016 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the given pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting the column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(f\"Table {i+1} columns: {column_str}\")  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Use the identified table for processing  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"Significant incidents table extracted:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names: strip spaces and newline  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw extracted table as JSON (filename: incidents_2016.json)  \n",
    "    raw_filename = 'incidents_2016.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"Raw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # Mapping for structured data. Adjust mapping if needed.  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ', '_').replace('\\n', '_').replace('(', '').replace(')', '')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'inc' in std:  \n",
    "            mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'last' in std:  \n",
    "            mapping[col] = 'Last_Report_Date'  \n",
    "        elif 'acre' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "      \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Apply mapping to get structured data  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "      \n",
    "    # Save structured data to JSON file  \n",
    "    structured_filename = 'structured_incidents_data_2016.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"Structured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # Generate dimension tables  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension (if present, otherwise create empty)  \n",
    "    if 'Inc_Type' in structured_table.columns:  \n",
    "        inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "    else:  \n",
    "        inc_type_dimension = []  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension using Start_Date column  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2016.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"Created dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "    # Fallback: inspect pages for text hints  \n",
    "    for page_num in range(start_page, end_page+1):  \n",
    "        page = doc[page_num]  \n",
    "        text = page.get_text()  \n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "            print(\"\\nPage \" + str(page_num+1) + \" contains text about significant fires/incidents.\")  \n",
    "            print(\"First 200 characters of the page:\")  \n",
    "            print(text[:200])  \n",
    "  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446faa9d-55a9-4fed-8600-9c2f88d2ceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2017 PDF...\n",
      "Found 4 tables on pages: [10, 11, 12, 14]\n",
      "Table 1 columns: col0, through the nifc-ciffc agreement canada provided\n",
      "Table 2 columns: name, gacc, state, start\n",
      "date, last\n",
      "report\n",
      "date, size in\n",
      "acres, cause*, estimated cost\n",
      "\n",
      "Table 2 on page 11 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Last\\nReport\\nDate', 'Size In\\nAcres', 'Cause*', 'Estimated Cost']\n",
      "Significant incidents table extracted:\n",
      "                   Name GACC State Start\\nDate  Last\\nReport\\nDate  \\\n",
      "0  NW Oklahoma\\nComplex   SA    OK         3/7                3/24   \n",
      "1              Perryton   SA    TX         3/6                3/13   \n",
      "2                Thomas   SO    CA        12/4  Active\\ninto\\n2018   \n",
      "3    Lodgepole\\nComplex   NR    MT        7/20                8/11   \n",
      "4         Roosters Comb   GB    NV         7/9                7/24   \n",
      "\n",
      "  Size In\\nAcres Cause* Estimated Cost  \n",
      "0        779,292      U      3,200,000  \n",
      "1        318,156      H             NR  \n",
      "2        270,000      U    123,836,000  \n",
      "3        270,000      U      9,800,000  \n",
      "4        218,380      U      4,000,000  \n",
      "Raw incidents table saved to incidents_2017.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Last_Report_Date': 'Last_Report_Date', 'Size In_Acres': 'Size_Acres', 'Cause*': 'Cause', 'Estimated Cost': 'Estimated_Cost'}\n",
      "Structured incidents data saved to structured_incidents_data_2017.json\n",
      "Created dimension tables and saved to dimension_tables_2017.json\n",
      "- State dimension: 11 states\n",
      "- GACC dimension: 7 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 25 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2017 PDF  \n",
    "pdf_path = 'annual_report_2017_508_0.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (similar to previous years)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2017 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the given pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting the column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(f\"Table {i+1} columns: {column_str}\")  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Use the identified table for processing  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"Significant incidents table extracted:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names: strip spaces and newline  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw extracted table as JSON (filename: incidents_2017.json)  \n",
    "    raw_filename = 'incidents_2017.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"Raw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # Mapping for structured data. Adjust mapping if needed.  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ', '_').replace('\\n', '_').replace('(', '').replace(')', '')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'inc' in std:  \n",
    "            mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'last' in std or 'control' in std or 'contain' in std:  \n",
    "            mapping[col] = 'Last_Report_Date'  \n",
    "        elif 'acre' in std or 'size' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "      \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Apply mapping to get structured data  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "      \n",
    "    # Save structured data to JSON file  \n",
    "    structured_filename = 'structured_incidents_data_2017.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"Structured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # Generate dimension tables  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension (check if exists, otherwise create empty)  \n",
    "    if 'Inc_Type' in structured_table.columns:  \n",
    "        inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "    else:  \n",
    "        inc_type_dimension = []  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension using Start_Date column  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2017.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"Created dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "    # Fallback: inspect pages for text hints  \n",
    "    for page_num in range(start_page, end_page+1):  \n",
    "        page = doc[page_num]  \n",
    "        text = page.get_text()  \n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "            print(\"\\nPage \" + str(page_num+1) + \" contains text about significant fires/incidents.\")  \n",
    "            print(\"First 200 characters of the page:\")  \n",
    "            print(text[:200])  \n",
    "              \n",
    "    # If we still can't find it, let's expand our search to more pages  \n",
    "    print(\"\\nExpanding search to more pages...\")  \n",
    "    for page_num in range(0, doc.page_count):  \n",
    "        if page_num < start_page or page_num > end_page:  # Only check pages we haven't checked yet  \n",
    "            page = doc[page_num]  \n",
    "            text = page.get_text()  \n",
    "            if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "                print(\"\\nPage \" + str(page_num+1) + \" contains text about significant fires/incidents.\")  \n",
    "                print(\"First 200 characters of the page:\")  \n",
    "                print(text[:200])  \n",
    "                  \n",
    "                # Try to extract tables from this page  \n",
    "                tables = extract_tables_from_page(page)  \n",
    "                if tables:  \n",
    "                    print(f\"Found {len(tables)} tables on page {page_num+1}\")  \n",
    "                    for i, table in enumerate(tables):  \n",
    "                        print(f\"Table {i+1} columns: {', '.join(table.columns.tolist())}\")  \n",
    "  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c46b22-68a2-4e0c-b47c-9c08ead11979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2018 PDF...\n",
      "Found 2 tables on pages: [9, 10]\n",
      "Table 1 columns: name, gacc, state, start\n",
      "date, contain or last\n",
      "report date, size\n",
      "(acres), cause*, estimated\n",
      "cost\n",
      "\n",
      "Table 1 on page 9 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Contain or Last\\nReport Date', 'Size\\n(acres)', 'Cause*', 'Estimated\\nCost']\n",
      "\n",
      "Significant incidents table extracted:\n",
      "                Name GACC State Start\\nDate Contain or Last\\nReport Date  \\\n",
      "0  Mendocino Complex   NO    CA        7/27                         9/18   \n",
      "1             Martin   GB    NV         7/5                         7/21   \n",
      "2               Rhea   SA    OK        4/12                         4/26   \n",
      "3    South Sugarloaf   GB    NV        8/17                        10/10   \n",
      "4               Carr   NO    CA        7/23                         8/30   \n",
      "\n",
      "  Size\\n(acres) Cause* Estimated\\nCost  \n",
      "0       459,123      U     220,000,000  \n",
      "1       435,569      U      10,000,000  \n",
      "2       286,196      U       3,800,000  \n",
      "3       233,458      L      20,000,000  \n",
      "4       229,651      U     162,289,294  \n",
      "Raw incidents table saved to incidents_2018.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Contain or Last_Report Date': 'Last_Report_Date', 'Size_(acres)': 'Size_Acres', 'Cause*': 'Cause', 'Estimated_Cost': 'Estimated_Cost'}\n",
      "Structured incidents data saved to structured_incidents_data_2018.json\n",
      "Created dimension tables and saved to dimension_tables_2018.json\n",
      "- State dimension: 11 states\n",
      "- GACC dimension: 7 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 22 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2018 PDF  \n",
    "pdf_path = 'annual_report_ 2018_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (similar to previous years)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2018 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the given pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # Use 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(\"Table \" + str(i+1) + \" columns: \" + column_str)  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Process the identified table  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table extracted:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names: strip spaces and replace newlines  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw table as JSON  \n",
    "    raw_filename = 'incidents_2018.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"Raw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # Mapping for structured data  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ', '_').replace('\\n', '_').replace('(', '').replace(')', '')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'inc' in std:  \n",
    "            mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'last' in std:  \n",
    "            mapping[col] = 'Last_Report_Date'  \n",
    "        elif 'acre' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Create structured data  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "      \n",
    "    # Save structured data as JSON  \n",
    "    structured_filename = 'structured_incidents_data_2018.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"Structured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # Generate dimension tables  \n",
    "      \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension, if available  \n",
    "    if 'Inc_Type' in structured_table.columns:  \n",
    "        inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "    else:  \n",
    "        inc_type_dimension = []  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension using Start_Date  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2018.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"Created dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "    # Fallback: inspect pages for text hints and extra tables  \n",
    "    for page_num in range(start_page, end_page+1):  \n",
    "        page = doc[page_num]  \n",
    "        text = page.get_text()  \n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "            print(\"\\nPage \" + str(page_num+1) + \" appears to contain relevant text for fires/incidents.\")  \n",
    "            print(\"First 200 characters of the page:\")  \n",
    "            print(text[:200])  \n",
    "      \n",
    "    print(\"\\nExpanding search to additional pages...\")  \n",
    "    for page_num in range(0, doc.page_count):  \n",
    "        if page_num < start_page or page_num > end_page:  \n",
    "            page = doc[page_num]  \n",
    "            text = page.get_text()  \n",
    "            if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "                print(\"\\nPage \" + str(page_num+1) + \" appears to contain relevant text for fires/incidents.\")  \n",
    "                print(\"First 200 characters of the page:\")  \n",
    "                print(text[:200])  \n",
    "                # Attempt to extract tables from this additional page  \n",
    "                tables = extract_tables_from_page(page)  \n",
    "                if tables:  \n",
    "                    print(\"Found \" + str(len(tables)) + \" tables on page \" + str(page_num+1))  \n",
    "                    for i, table in enumerate(tables):  \n",
    "                        print(\"Table \" + str(i+1) + \" columns: \" + ', '.join(table.columns.tolist()))  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcdc6bfa-a273-474d-9ee5-6f872daad41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2019 PDF...\n",
      "Found 2 tables on pages: [10, 11]\n",
      "Table 1 columns: name, gacc, state, start\n",
      "date, contain or last\n",
      "report date, size\n",
      "(acres), cause*, estimated\n",
      "cost\n",
      "\n",
      "Table 1 on page 10 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Contain or Last\\nReport Date', 'Size\\n(acres)', 'Cause*', 'Estimated\\nCost']\n",
      "\n",
      "Significant incidents table extracted:\n",
      "             Name GACC State Start\\nDate Contain or Last\\nReport Date  \\\n",
      "0  Old Grouch Top   AK    AK         6/5                          8/1   \n",
      "1     Frozen Calf   AK    AK        6/24                         7/11   \n",
      "2      Hess Creek   AK    AK        6/21                          8/1   \n",
      "3       Swan Lake   AK    AK         6/5                         10/2   \n",
      "4   Bearnose Hill   AK    AK        6/29                         7/11   \n",
      "\n",
      "  Size\\n(acres) Cause* Estimated\\nCost  \n",
      "0       307,969      L         $61,000  \n",
      "1       240,543      L      $4,332,806  \n",
      "2       189,369      L      $3,005,369  \n",
      "3       167,183      L     $48,101,094  \n",
      "4       130,768      L      $2,108,024  \n",
      "Raw incidents table saved to incidents_2019.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Contain or Last_Report Date': 'Last_Report_Date', 'Size_(acres)': 'Size_Acres', 'Cause*': 'Cause', 'Estimated_Cost': 'Estimated_Cost'}\n",
      "Structured incidents data saved to structured_incidents_data_2019.json\n",
      "Created dimension tables and saved to dimension_tables_2019.json\n",
      "- State dimension: 6 states\n",
      "- GACC dimension: 6 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 18 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2019 PDF  \n",
    "pdf_path = 'annual_report_2019_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (pages 8 to 16)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2019 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the specified pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # Use 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting table headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(\"Table \" + str(i+1) + \" columns: \" + column_str)  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Process the identified table  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table extracted:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names: Remove extra spaces and newline characters  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw extracted table as JSON  \n",
    "    raw_filename = 'incidents_2019.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"Raw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # Mapping for structured data based on probable column names in the incidents table  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ', '_').replace('\\n', '_').replace('(', '').replace(')', '')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'inc' in std:  \n",
    "            mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'last' in std:  \n",
    "            mapping[col] = 'Last_Report_Date'  \n",
    "        elif 'acre' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "      \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Apply column mapping to generate structured data  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "      \n",
    "    # Save structured data to JSON  \n",
    "    structured_filename = 'structured_incidents_data_2019.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"Structured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # Generate dimension tables  \n",
    "  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "  \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "  \n",
    "    # Incident Type dimension (if exists)  \n",
    "    if 'Inc_Type' in structured_table.columns:  \n",
    "        inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "    else:  \n",
    "        inc_type_dimension = []  \n",
    "  \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "  \n",
    "    # Time dimension using Start_Date column  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "  \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "  \n",
    "    dimension_filename = 'dimension_tables_2019.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"Created dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "    # Fallback: Inspect additional pages for text hints  \n",
    "    for page_num in range(start_page, end_page+1):  \n",
    "        page = doc[page_num]  \n",
    "        text = page.get_text()  \n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "            print(\"\\nPage \" + str(page_num+1) + \" appears to contain relevant text for fires/incidents.\")  \n",
    "            print(\"First 200 characters of the page:\")  \n",
    "            print(text[:200])  \n",
    "      \n",
    "    print(\"\\nExpanding search to additional pages...\")  \n",
    "    for page_num in range(0, doc.page_count):  \n",
    "        if page_num < start_page or page_num > end_page:  \n",
    "            page = doc[page_num]  \n",
    "            text = page.get_text()  \n",
    "            if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "                print(\"\\nPage \" + str(page_num+1) + \" appears to contain relevant text for fires/incidents.\")  \n",
    "                print(\"First 200 characters of the page:\")  \n",
    "                print(text[:200])  \n",
    "                # Attempt to extract tables from this page  \n",
    "                tables = extract_tables_from_page(page)  \n",
    "                if tables:  \n",
    "                    print(\"Found \" + str(len(tables)) + \" tables on page \" + str(page_num+1))  \n",
    "                    for i, table in enumerate(tables):  \n",
    "                        print(\"Table \" + str(i+1) + \" columns: \" + ', '.join(table.columns.tolist()))  \n",
    "                          \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ff7f35-d5fe-4a87-8b40-bb9ce1967842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2020 PDF...\n",
      "Found 2 tables on pages: [10, 11]\n",
      "Table 1 columns: name, gacc, state, start\n",
      "date, contain or last\n",
      "report date, size\n",
      "(acres), cause*, estimated\n",
      "cost\n",
      "\n",
      "Table 1 on page 10 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Contain or Last\\nReport Date', 'Size\\n(acres)', 'Cause*', 'Estimated\\nCost']\n",
      "\n",
      "Significant incidents table extracted:\n",
      "                     Name GACC State Start\\nDate Contain or Last\\nReport Date  \\\n",
      "0          August Complex   NO    CA        8/17                        11/11   \n",
      "1  SCU Lightning\\nComplex   NO    CA        8/16                         9/14   \n",
      "2             SHF Elkhorn   NO    CA        8/29                          9/9   \n",
      "3                   Creek   SO    CA         9/4                        12/17   \n",
      "4  LNU Lightning\\nComplex   NO    CA        8/17                         10/1   \n",
      "\n",
      "  Size\\n(acres) Cause* Estimated\\nCost  \n",
      "0     1,032,648      U    $115,511,218  \n",
      "1       396,624      U     $69,412,351  \n",
      "2       391,493      L              NR  \n",
      "3       379,895      U    $193,000,000  \n",
      "4       363,220      U     $94,646,381  \n",
      "Raw incidents table saved to incidents_2020.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Contain or Last_Report Date': 'Last_Report_Date', 'Size_(acres)': 'Size_Acres', 'Cause*': 'Cause', 'Estimated_Cost': 'Estimated_Cost'}\n",
      "Structured incidents data saved to structured_incidents_data_2020.json\n",
      "Created dimension tables and saved to dimension_tables_2020.json\n",
      "- State dimension: 8 states\n",
      "- GACC dimension: 6 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 4 causes\n",
      "- Time dimension: 21 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2020 PDF  \n",
    "pdf_path = 'annual_report_2020.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (pages 8 to 16)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2020 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the given pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # Use 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting table headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(\"Table \" + str(i+1) + \" columns: \" + column_str)  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Use the identified table for processing  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table extracted:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names: remove surrounding spaces and replace newline with underscore  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw extracted table as JSON (filename: incidents_2020.json)  \n",
    "    raw_filename = 'incidents_2020.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"Raw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # Create a column mapping for structured data based on column names we expect  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ', '_').replace('\\n', '_').replace('(', '').replace(')', '')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'inc' in std:  \n",
    "            mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'last' in std:  \n",
    "            mapping[col] = 'Last_Report_Date'  \n",
    "        elif 'acre' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Apply the mapping to create a structured table  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "      \n",
    "    # Save the structured table as JSON (filename: structured_incidents_data_2020.json)  \n",
    "    structured_filename = 'structured_incidents_data_2020.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"Structured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # Generate dimension tables  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension (if available)  \n",
    "    if 'Inc_Type' in structured_table.columns:  \n",
    "        inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "    else:  \n",
    "        inc_type_dimension = []  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension using Start_Date  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2020.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"Created dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "    # Fallback: inspect pages for text hints and additional tables  \n",
    "    for page_num in range(start_page, end_page+1):  \n",
    "        page = doc[page_num]  \n",
    "        text = page.get_text()  \n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "            print(\"\\nPage \" + str(page_num+1) + \" appears to contain relevant text for fires/incidents.\")  \n",
    "            print(\"First 200 characters of the page:\")  \n",
    "            print(text[:200])  \n",
    "      \n",
    "    print(\"\\nExpanding search to additional pages...\")  \n",
    "    for page_num in range(0, doc.page_count):  \n",
    "        if page_num < start_page or page_num > end_page:  \n",
    "            page = doc[page_num]  \n",
    "            text = page.get_text()  \n",
    "            if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "                print(\"\\nPage \" + str(page_num+1) + \" appears to contain relevant text for fires/incidents.\")  \n",
    "                print(\"First 200 characters of the page:\")  \n",
    "                print(text[:200])  \n",
    "                # Attempt to extract tables from this page  \n",
    "                tables = extract_tables_from_page(page)  \n",
    "                if tables:  \n",
    "                    print(\"Found \" + str(len(tables)) + \" tables on page \" + str(page_num+1))  \n",
    "                    for i, table in enumerate(tables):  \n",
    "                        print(\"Table \" + str(i+1) + \" columns: \" + ', '.join(table.columns.tolist()))  \n",
    "  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6b26ffc-7901-4f36-acea-dc39a2cb36c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2021 PDF...\n",
      "Found 2 tables on pages: [11, 12]\n",
      "Table 1 columns: name, gacc, state, start\n",
      "date, contain or last\n",
      "report date, size\n",
      "(acres), cause*, estimated\n",
      "cost\n",
      "\n",
      "Table 1 on page 11 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Contain or Last\\nReport Date', 'Size\\n(acres)', 'Cause*', 'Estimated\\nCost']\n",
      "\n",
      "Significant incidents table extracted:\n",
      "            Name GACC State Start\\nDate Contain or Last\\nReport Date  \\\n",
      "0          Dixie   NO    CA        7/13                        10/23   \n",
      "1        Bootleg   NW    OR         7/6                         8/13   \n",
      "2       Monument   NO    CA        7/31                        10/25   \n",
      "3         Caldor   NO    CA        8/14                        10/20   \n",
      "4  River Complex   NO    CA        7/30                        10/24   \n",
      "\n",
      "  Size\\n(acres) Cause* Estimated\\nCost  \n",
      "0       963,309      U    $637,428,216  \n",
      "1       413,717      L    $100,900,000  \n",
      "2       223,124      L    $163,739,291  \n",
      "3       221,835      H    $271,147,512  \n",
      "4       199,359      L     $95,340,595  \n",
      "Raw incidents table saved to incidents_2021.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Contain or Last_Report Date': 'Contain_or_Last_Report_Date', 'Size_(acres)': 'Size_Acres', 'Cause*': 'Cause', 'Estimated_Cost': 'Estimated_Cost'}\n",
      "Structured incidents data saved to structured_incidents_data_2021.json\n",
      "Created dimension tables and saved to dimension_tables_2021.json\n",
      "- State dimension: 9 states\n",
      "- GACC dimension: 7 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 21 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2021 PDF  \n",
    "pdf_path = 'annual_report_2021.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (pages 8 to 16)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2021 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the specified pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # 1-indexed page numbers  \n",
    "  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting table headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(\"Table \" + str(i+1) + \" columns: \" + column_str)  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Use the identified table for processing  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table extracted:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names: remove newlines and excess spaces  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw extracted table as JSON (filename: incidents_2021.json)  \n",
    "    raw_filename = 'incidents_2021.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"Raw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # Mapping for structured data  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ', '_').replace('\\n', '_').replace('(', '').replace(')', '')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'contain' in std or 'last' in std:  \n",
    "            mapping[col] = 'Contain_or_Last_Report_Date'  \n",
    "        elif 'acre' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Apply mapping to get structured data  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "      \n",
    "    structured_filename = 'structured_incidents_data_2021.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"Structured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # Generate dimension tables  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension, if available  \n",
    "    if 'Inc_Type' in structured_table.columns:  \n",
    "        inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "    else:  \n",
    "        inc_type_dimension = []  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension using Start_Date column  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2021.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"Created dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "    # Fallback: Expand search to additional pages if needed  \n",
    "    for page_num in range(0, doc.page_count):  \n",
    "        if page_num < start_page or page_num > end_page:  \n",
    "            page = doc[page_num]  \n",
    "            text = page.get_text()  \n",
    "            if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):  \n",
    "                print(\"\\nPage \" + str(page_num+1) + \" contains text about significant fires/incidents.\")  \n",
    "                print(\"First 200 characters of the page:\")  \n",
    "                print(text[:200])  \n",
    "                # Try to extract tables from the page  \n",
    "                tables = extract_tables_from_page(page)  \n",
    "                if tables:  \n",
    "                    print(\"Found \" + str(len(tables)) + \" tables on page \" + str(page_num+1))  \n",
    "                    for i, table in enumerate(tables):  \n",
    "                        print(\"Table \" + str(i+1) + \" columns: \" + ', '.join(table.columns.tolist()))  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2e177ba-9ca9-421e-9c95-aa146f30472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing annual_report.2022.pdf; searching pages 8 to 16\n",
      "Found 2 tables on pages: [9, 10]\n",
      "Table 1 columns: name, gacc, state, start\n",
      "date, last\n",
      "report\n",
      "date, size in\n",
      "acres, cause*, estimated cost\n",
      "\n",
      "Table 1 on page 9 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Last\\nReport\\nDate', 'Size In\\nAcres', 'Cause*', 'Estimated Cost']\n",
      "\n",
      "Significant incidents table extracted (raw preview):\n",
      "               Name GACC State Start\\nDate Last\\nReport\\nDate Size In\\nAcres  \\\n",
      "0      Lime Complex   AK    AK        6/15               7/26        865,625   \n",
      "1      Hermits Peak   SW    NM         4/7              10/20        341,735   \n",
      "2             Black   SW    NM        5/13              11/10        325,136   \n",
      "3  Paradise Complex   AK    AK         7/7               8/11        275,703   \n",
      "4       Tatlawiksuk   AK    AK         6/5               7/22        229,439   \n",
      "\n",
      "  Cause* Estimated Cost  \n",
      "0      U    $12,726,992  \n",
      "1      H   $330,100,293  \n",
      "2      H    $60,190,000  \n",
      "3      U     $2,782,896  \n",
      "4      L        $75,965  \n",
      "\n",
      "Raw incidents table saved to incidents_2022.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Last_Report_Date': 'Last_Report_Date', 'Size In_Acres': 'Size_Acres', 'Cause*': 'Cause', 'Estimated Cost': 'Estimated_Cost'}\n",
      "\n",
      "Structured incidents data saved to structured_incidents_data_2022.json\n",
      "\n",
      "Created dimension tables and saved to dimension_tables_2022.json\n",
      "- State dimension: 7 states\n",
      "- GACC dimension: 6 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 28 dates\n",
      "\n",
      "Process completed for annual_report.2022.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# -------------------------------  \n",
    "# Load and process annual_report.2022.pdf  \n",
    "# -------------------------------  \n",
    "pdf_path = 'annual_report.2022.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (pages 8 to 16, i.e., indices 7 to 15)  \n",
    "start_page = 7   # page 8 (0-indexed)  \n",
    "end_page = 15    # page 16 (0-indexed)  \n",
    "  \n",
    "print(\"Processing annual_report.2022.pdf; searching pages \" + str(start_page+1) + \" to \" + str(end_page+1))  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from specified page range  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # saving 1-indexed page numbers  \n",
    "  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(\"Table \" + str(i+1) + \" columns: \" + column_str)  \n",
    "    # Check for typical keywords indicative of the incidents table  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is None:  \n",
    "    print(\"\\nSignificant incidents table not found in expected pages. Exiting processing.\")  \n",
    "else:  \n",
    "    # Process the identified table  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table extracted (raw preview):\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # -------------------------------  \n",
    "    # Step 1: Clean the table columns  \n",
    "    # -------------------------------  \n",
    "    incidents_table.columns = [col.strip().replace('\\n','_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw incidents table as JSON  \n",
    "    raw_filename = 'incidents_2022.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"\\nRaw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # -------------------------------  \n",
    "    # Step 2: Mapping column names to standardized names  \n",
    "    # -------------------------------  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ','_').replace('\\n','_').replace('(','').replace(')','')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'report' in std:  \n",
    "            mapping[col] = 'Last_Report_Date'  \n",
    "        elif 'acre' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Apply mapping for standardized (structured) data  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "    structured_filename = 'structured_incidents_data_2022.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"\\nStructured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # -------------------------------  \n",
    "    # Step 3: Generate Dimension Tables  \n",
    "    # -------------------------------  \n",
    "    # Create state dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Assuming Inc_Type is not available, so skip incident type dimension  \n",
    "    inc_type_dimension = []  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension from Start_Date (as string; further transformation may be applied if needed)  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2022.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"\\nCreated dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "      \n",
    "print(\"\\nProcess completed for annual_report.2022.pdf\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db5f2f7-fb13-4180-8082-b9c8681dda40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing annual_report_2023_508_0.pdf; searching pages 8 to 16\n",
      "Found 6 tables on pages: [9, 10, 15, 16]\n",
      "Table 1 columns: gacc, single\n",
      "residences, mixed\n",
      "commercial-\n",
      "residential, multiple\n",
      "residences, nonresidential\n",
      "commercial\n",
      "property, other\n",
      "minor\n",
      "structures, total\n",
      "Table 2 columns: name, gacc, state, start\n",
      "date, last\n",
      "report\n",
      "date, size in\n",
      "acres, cause*\n",
      "\n",
      "Table 2 on page 10 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Last\\nReport\\nDate', 'Size In\\nAcres', 'Cause*']\n",
      "\n",
      "Raw incidents table saved to incidents_2023.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start\\nDate': 'Start_Date', 'Last\\nReport\\nDate': 'Last_Report_Date', 'Size In\\nAcres': 'Size_Acres', 'Cause*': 'Cause'}\n",
      "Structured incidents data saved to structured_incidents_data_2023.json\n",
      "\n",
      "Created dimension tables and saved to dimension_tables_2023.json\n",
      "- State dimension: 5 states\n",
      "- GACC dimension: 6 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 9 dates\n",
      "\n",
      "Process completed for annual_report_2023_508_0.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# -------------------------------  \n",
    "# Load and process annual_report_2023_508_0.pdf  \n",
    "# -------------------------------  \n",
    "pdf_path = 'annual_report_2023_508_0.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (pages 8 to 16, i.e., indices 7 to 15)  \n",
    "start_page = 7   # page 8 (0-indexed)  \n",
    "end_page = 15    # page 16 (0-indexed)  \n",
    "  \n",
    "print(\"Processing annual_report_2023_508_0.pdf; searching pages \" + str(start_page+1) + \" to \" + str(end_page+1))  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from specified page range  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # saving 1-indexed page numbers  \n",
    "  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    print(\"Table \" + str(i+1) + \" columns: \" + column_str)  \n",
    "    # Check for typical keywords indicative of the incidents table  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is None:  \n",
    "    print(\"\\nSignificant incidents table not found in expected pages. Exiting processing.\")  \n",
    "else:  \n",
    "    # Process the identified table  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "      \n",
    "    # Clean column names: strip spaces and newlines  \n",
    "    incidents_table.columns = [col.strip().replace('\\\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw extracted table as JSON  \n",
    "    raw_filename = 'incidents_2023.json'  \n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)  \n",
    "    print(\"\\nRaw incidents table saved to \" + raw_filename)  \n",
    "      \n",
    "    # Apply mapping to standardize column names  \n",
    "    mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        std = col.lower().replace(' ', '_').replace('\\\\n', '_').replace('(', '').replace(')', '')  \n",
    "        if 'name' in std:  \n",
    "            mapping[col] = 'Name'  \n",
    "        elif 'gacc' in std:  \n",
    "            mapping[col] = 'GACC'  \n",
    "        elif 'state' in std:  \n",
    "            mapping[col] = 'State'  \n",
    "        elif 'start' in std:  \n",
    "            mapping[col] = 'Start_Date'  \n",
    "        elif 'report' in std:  \n",
    "            mapping[col] = 'Last_Report_Date'  \n",
    "        elif 'acre' in std:  \n",
    "            mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in std:  \n",
    "            mapping[col] = 'Cause'  \n",
    "        elif 'cost' in std:  \n",
    "            mapping[col] = 'Estimated_Cost'  \n",
    "        else:  \n",
    "            mapping[col] = col  \n",
    "              \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(mapping)  \n",
    "      \n",
    "    # Apply mapping for structured data  \n",
    "    structured_table = incidents_table.rename(columns=mapping)  \n",
    "    structured_filename = 'structured_incidents_data_2023.json'  \n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)  \n",
    "    print(\"Structured incidents data saved to \" + structured_filename)  \n",
    "      \n",
    "    # Generate dimension tables  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident type dimension (if available; not assumed here)  \n",
    "    inc_type_dimension = []  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension from Start_Date (as string; further transformation can be done if needed)  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2023.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"\\nCreated dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "      \n",
    "print(\"\\nProcess completed for annual_report_2023_508_0.pdf\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2facd82-aff3-4927-8cb1-0a74b9f6b4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing annual_report_2024.pdf; searching all 57 pages\n",
      "Found 68 tables on pages: [13, 17, 18, 18, 19, 19, 20, 22, 22, 23, 41, 41, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 54, 54, 54]\n",
      "Table 1 on page 13 columns: gacc, single\n",
      "residences, mixed\n",
      "commercial-\n",
      "residential, multiple\n",
      "residences, nonresidential\n",
      "commercial\n",
      "property, other\n",
      "minor\n",
      "structures, total\n",
      "Table 2 on page 17 columns: name, gacc, state, start\n",
      "date, last\n",
      "report\n",
      "date, size in\n",
      "acres, cause*\n",
      "Table 2 on page 17 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Last\\nReport\\nDate', 'Size In\\nAcres', 'Cause*']\n",
      "Significant incidents table extracted:\n",
      "               Name GACC State Start\\nDate Last\\nReport\\nDate Size In\\nAcres  \\\n",
      "0       Betty's Way   RM    NE        2/26               3/11         69,810   \n",
      "1  Smokehouse Creek   SA    TX        2/26               3/17      1,054,153   \n",
      "2           Catesby   SA    OK        2/27               3/15         89,688   \n",
      "3          McDonald   AK    AK         6/8               7/20        152,227   \n",
      "4          Midnight   AK    AK        6/19                7/9         52,550   \n",
      "\n",
      "  Cause*  \n",
      "0      U  \n",
      "1      H  \n",
      "2      H  \n",
      "3      L  \n",
      "4      L  \n",
      "Raw incidents table saved to incidents_2024.json\n",
      "Proposed column mapping:\n",
      "{'_N_a_m_e_': '_N_a_m_e_', '_G_A_C_C_': '_G_A_C_C_', '_S_t_a_t_e_': '_S_t_a_t_e_', '_S_t_a_r_t_\\n_D_a_t_e_': '_S_t_a_r_t_\\n_D_a_t_e_', '_L_a_s_t_\\n_R_e_p_o_r_t_\\n_D_a_t_e_': '_L_a_s_t_\\n_R_e_p_o_r_t_\\n_D_a_t_e_', '_S_i_z_e_ _I_n_\\n_A_c_r_e_s_': '_S_i_z_e_ _I_n_\\n_A_c_r_e_s_', '_C_a_u_s_e_*_': '_C_a_u_s_e_*_'}\n",
      "Structured incidents data saved to structured_incidents_data_2024.json\n",
      "Created dimension tables and saved to dimension_tables_2024.json\n",
      "- State dimension: 0 states\n",
      "- GACC dimension: 0 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 0 causes\n",
      "- Time dimension: 0 dates\n",
      "Process completed for annual_report_2024.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# -------------------------------\n",
    "# Load and process annual_report_2024.pdf\n",
    "# -------------------------------\n",
    "pdf_path = 'annual_report_2024.pdf'\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "print(\"Processing annual_report_2024.pdf; searching all \" + str(doc.page_count) + \" pages\")\n",
    "\n",
    "# Function to extract tables from a page using PyMuPDF's table extraction\n",
    "def extract_tables_from_page(page):\n",
    "    tables = page.find_tables()\n",
    "    if tables and tables.tables:\n",
    "        return [table.to_pandas() for table in tables.tables]\n",
    "    return []\n",
    "\n",
    "all_tables = []\n",
    "table_pages = []\n",
    "\n",
    "# Extract tables from all pages\n",
    "for page_num in range(doc.page_count):\n",
    "    page = doc[page_num]\n",
    "    tables = extract_tables_from_page(page)\n",
    "    if tables:\n",
    "        for table in tables:\n",
    "            all_tables.append(table)\n",
    "            table_pages.append(page_num+1)  # saving 1-indexed page numbers\n",
    "\n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))\n",
    "\n",
    "# Identify the significant incidents table by column headers\n",
    "significant_table_index = None\n",
    "for i, table in enumerate(all_tables):\n",
    "    columns = table.columns.tolist()\n",
    "    column_str = ', '.join(columns).lower()\n",
    "    print(\"Table \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" columns: \" + column_str)\n",
    "    \n",
    "    # Check for typical keywords indicative of the incidents table\n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):\n",
    "        significant_table_index = i\n",
    "        print(\"\\\n",
    "Table \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")\n",
    "        print(\"Columns:\", columns)\n",
    "        break\n",
    "\n",
    "# If not found by column names, try to find by looking at the content\n",
    "if significant_table_index is None:\n",
    "    print(\"\\\n",
    "Searching for tables with fire names or incident data...\")\n",
    "    for i, table in enumerate(all_tables):\n",
    "        # Check if the table has at least 3 columns and more than 2 rows\n",
    "        if len(table.columns) >= 3 and len(table) > 2:\n",
    "            # Convert to string to check content\n",
    "            table_str = table.to_string().lower()\n",
    "            # Look for common fire names or patterns\n",
    "            if any(keyword in table_str for keyword in ['fire', 'complex', 'wildfire', 'incident']):\n",
    "                significant_table_index = i\n",
    "                print(\"\\\n",
    "Table \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" may contain fire incident data.\")\n",
    "                print(\"Sample content:\")\n",
    "                print(table.head(3))\n",
    "                break\n",
    "\n",
    "if significant_table_index is None:\n",
    "    print(\"\\\n",
    "No table identified as significant incidents table. Printing all table headers for review:\")\n",
    "    for i, table in enumerate(all_tables):\n",
    "        print(\"\\\n",
    "Table \" + str(i+1) + \" on page \" + str(table_pages[i]) + \":\")\n",
    "        print(table.head(2))\n",
    "    \n",
    "    print(\"\\\n",
    "Significant incidents table not found in the document. Exiting processing.\")\n",
    "    print(\"\\\n",
    "Process completed for annual_report_2024.pdf\")\n",
    "else:\n",
    "    # Process the identified table\n",
    "    incidents_table = all_tables[significant_table_index]\n",
    "    print(\"\\\n",
    "Significant incidents table extracted:\")\n",
    "    print(incidents_table.head())\n",
    "    \n",
    "    # Clean column names: strip spaces and newlines\n",
    "    incidents_table.columns = [col.strip().replace('\\\n",
    "', '_') for col in incidents_table.columns]\n",
    "    \n",
    "    # Save the raw extracted table as JSON\n",
    "    raw_filename = 'incidents_2024.json'\n",
    "    incidents_table.to_json(raw_filename, orient='records', indent=4)\n",
    "    print(\"Raw incidents table saved to \" + raw_filename)\n",
    "    \n",
    "    # Apply mapping to standardize column names\n",
    "    mapping = {}\n",
    "    for col in incidents_table.columns:\n",
    "        std = col.lower().replace(' ', '_').replace('\\\n",
    "', '_').replace('(', '').replace(')', '')\n",
    "        if 'name' in std:\n",
    "            mapping[col] = 'Name'\n",
    "        elif 'gacc' in std:\n",
    "            mapping[col] = 'GACC'\n",
    "        elif 'state' in std:\n",
    "            mapping[col] = 'State'\n",
    "        elif 'start' in std:\n",
    "            mapping[col] = 'Start_Date'\n",
    "        elif 'report' in std:\n",
    "            mapping[col] = 'Last_Report_Date'\n",
    "        elif 'acre' in std:\n",
    "            mapping[col] = 'Size_Acres'\n",
    "        elif 'cause' in std:\n",
    "            mapping[col] = 'Cause'\n",
    "        elif 'cost' in std:\n",
    "            mapping[col] = 'Estimated_Cost'\n",
    "        else:\n",
    "            mapping[col] = col\n",
    "    \n",
    "    print(\"\\\n",
    "Proposed column mapping:\")\n",
    "    print(mapping)\n",
    "    \n",
    "    # Apply mapping for structured data\n",
    "    structured_table = incidents_table.rename(columns=mapping)\n",
    "    structured_filename = 'structured_incidents_data_2024.json'\n",
    "    structured_table.to_json(structured_filename, orient='records', indent=4)\n",
    "    print(\"Structured incidents data saved to \" + structured_filename)\n",
    "    \n",
    "    # Generate dimension tables\n",
    "    # State dimension\n",
    "    states = structured_table['State'].dropna().unique() if 'State' in structured_table.columns else []\n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]\n",
    "    \n",
    "    # GACC dimension\n",
    "    gaccs = structured_table['GACC'].dropna().unique() if 'GACC' in structured_table.columns else []\n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]\n",
    "    \n",
    "    # Incident type dimension (if available; not assumed here)\n",
    "    inc_type_dimension = []\n",
    "    \n",
    "    # Cause dimension\n",
    "    causes = structured_table['Cause'].dropna().unique() if 'Cause' in structured_table.columns else []\n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]\n",
    "    \n",
    "    # Time dimension from Start_Date (as string; further transformation can be done if needed)\n",
    "    dates = structured_table['Start_Date'].dropna().unique() if 'Start_Date' in structured_table.columns else []\n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]\n",
    "    \n",
    "    dimension_tables = {\n",
    "        'state_dimension': state_dimension,\n",
    "        'gacc_dimension': gacc_dimension,\n",
    "        'inc_type_dimension': inc_type_dimension,\n",
    "        'cause_dimension': cause_dimension,\n",
    "        'time_dimension': time_dimension\n",
    "    }\n",
    "    \n",
    "    dimension_filename = 'dimension_tables_2024.json'\n",
    "    with open(dimension_filename, 'w') as f:\n",
    "        json.dump(dimension_tables, f, indent=4)\n",
    "    print(\"\\\n",
    "Created dimension tables and saved to \" + dimension_filename)\n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")\n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")\n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")\n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")\n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")\n",
    "    \n",
    "    print(\"\\\n",
    "Process completed for annual_report_2024.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
