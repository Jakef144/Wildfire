{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79ce5678-f17f-409d-b1cd-2848bd8df53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the original data:\n",
      "              Name GACC State  Start_Date Last_Report_Date  Size_Acres Cause  \\\n",
      "0  Buzzard Complex   NW    OR  2014-07-14       2014-09-11    395747.0     L   \n",
      "1  Carlton Complex   NW    WA  2014-07-14       2014-08-28    256108.0     L   \n",
      "2      Funny River   AK    AK  2014-05-19       2014-08-14    195858.0     H   \n",
      "\n",
      "         Cost Inc_Type    Year  Duration_Days  Start_Month Cost_Source  \n",
      "0  11062411.0       WF  2014.0             59            7    Original  \n",
      "1  68800000.0       WF  2014.0             45            7    Original  \n",
      "2  11496627.0       WF  2014.0             87            5    Original  \n",
      "Creating star schema tables...\n",
      "Star schema tables created successfully.\n",
      "Star schema tables saved to CSV files.\n",
      "- dim_time.csv\n",
      "- dim_location.csv\n",
      "- dim_cause.csv\n",
      "- dim_incident_type.csv\n",
      "- fact_incident.csv\n",
      "Time Dimension Preview:\n",
      "    full_date  date_id  year  month  day  quarter month_name day_of_week\n",
      "0  2008-02-25        1  2008      2   25        1   February      Monday\n",
      "1  2008-03-02        2  2008      3    2        1      March      Sunday\n",
      "2  2008-03-14        3  2008      3   14        1      March      Friday\n",
      "Location Dimension Preview:\n",
      "   location_id GACC State\n",
      "0            1   NW    OR\n",
      "1            2   NW    WA\n",
      "2            3   AK    AK\n",
      "Cause Dimension Preview:\n",
      "  cause_code  cause_id cause_description\n",
      "0          L         1         Lightning\n",
      "1          H         2             Human\n",
      "2          U         3           Unknown\n",
      "Incident Type Dimension Preview:\n",
      "  inc_type_code  inc_type_id inc_type_description\n",
      "0            WF            1             Wildfire\n",
      "Fact Table Preview:\n",
      "   incident_id    incident_name  start_date_id  end_date_id  location_id  \\\n",
      "0            1  Buzzard Complex            252          258            1   \n",
      "1            2  Carlton Complex            252          257            2   \n",
      "2            3      Funny River            249          255            3   \n",
      "\n",
      "   cause_id  inc_type_id  size_acres        cost  duration_days cost_source  \n",
      "0         1            1    395747.0  11062411.0             59    Original  \n",
      "1         1            1    256108.0  68800000.0             45    Original  \n",
      "2         2            1    195858.0  11496627.0             87    Original  \n"
     ]
    }
   ],
   "source": [
    "# Let's recreate our star schema and prepare it for PostgreSQL\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv('cleaned_incident_data_imputed.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows to confirm data is loaded correctly\n",
    "print(\"Preview of the original data:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Replace any instance of 'Unknown' with 'U' in the Cause column\n",
    "df['Cause'] = df['Cause'].replace('Unknown', 'U')\n",
    "\n",
    "print(\"\\\n",
    "Creating star schema tables...\")\n",
    "\n",
    "# ----- Create Dimension Tables -----\n",
    "\n",
    "# 1. Time Dimension: combine unique dates from Start_Date and Last_Report_Date\n",
    "all_dates = pd.concat([df['Start_Date'], df['Last_Report_Date']]).unique()\n",
    "# sort dates for consistency\n",
    "sorted_dates = sorted(all_dates)\n",
    "time_dim = pd.DataFrame({'full_date': sorted_dates})\n",
    "time_dim['date_id'] = range(1, len(time_dim) + 1)\n",
    "time_dim['year'] = pd.to_datetime(time_dim['full_date']).dt.year\n",
    "time_dim['month'] = pd.to_datetime(time_dim['full_date']).dt.month\n",
    "time_dim['day'] = pd.to_datetime(time_dim['full_date']).dt.day\n",
    "time_dim['quarter'] = pd.to_datetime(time_dim['full_date']).dt.quarter\n",
    "time_dim['month_name'] = pd.to_datetime(time_dim['full_date']).dt.month_name()\n",
    "time_dim['day_of_week'] = pd.to_datetime(time_dim['full_date']).dt.day_name()\n",
    "\n",
    "# 2. Location Dimension\n",
    "location_dim = df[['GACC', 'State']].drop_duplicates().reset_index(drop=True)\n",
    "location_dim['location_id'] = range(1, len(location_dim) + 1)\n",
    "location_dim = location_dim[['location_id', 'GACC', 'State']]\n",
    "\n",
    "# 3. Cause Dimension\n",
    "cause_dim = pd.DataFrame({'cause_code': df['Cause'].unique()})\n",
    "cause_dim['cause_id'] = range(1, len(cause_dim) + 1)\n",
    "cause_descriptions = {\n",
    "    'L': 'Lightning',\n",
    "    'H': 'Human',\n",
    "    'U': 'Unknown',\n",
    "    'N': 'Natural (non-lightning)'\n",
    "}\n",
    "cause_dim['cause_description'] = cause_dim['cause_code'].map(cause_descriptions)\n",
    "\n",
    "# 4. Incident Type Dimension\n",
    "inc_type_dim = pd.DataFrame({'inc_type_code': df['Inc_Type'].unique()})\n",
    "inc_type_dim['inc_type_id'] = range(1, len(inc_type_dim) + 1)\n",
    "inc_type_descriptions = {\n",
    "    'WF': 'Wildfire'\n",
    "}\n",
    "inc_type_dim['inc_type_description'] = inc_type_dim['inc_type_code'].map(inc_type_descriptions)\n",
    "\n",
    "# ----- Create Fact Table -----\n",
    "\n",
    "# Create mapping dictionaries from dimension tables\n",
    "start_date_mapping = dict(zip(time_dim['full_date'], time_dim['date_id']))\n",
    "location_mapping = { (row['GACC'], row['State']) : row['location_id'] for _, row in location_dim.iterrows() }\n",
    "cause_mapping = dict(zip(cause_dim['cause_code'], cause_dim['cause_id']))\n",
    "inc_type_mapping = dict(zip(inc_type_dim['inc_type_code'], inc_type_dim['inc_type_id']))\n",
    "\n",
    "fact_table = pd.DataFrame()\n",
    "fact_table['incident_id'] = range(1, len(df) + 1)\n",
    "fact_table['incident_name'] = df['Name']\n",
    "fact_table['start_date_id'] = df['Start_Date'].map(start_date_mapping)\n",
    "fact_table['end_date_id'] = df['Last_Report_Date'].map(start_date_mapping)\n",
    "fact_table['location_id'] = df.apply(lambda x: location_mapping.get((x['GACC'], x['State'])), axis=1)\n",
    "fact_table['cause_id'] = df['Cause'].map(cause_mapping)\n",
    "fact_table['inc_type_id'] = df['Inc_Type'].map(inc_type_mapping)\n",
    "fact_table['size_acres'] = df['Size_Acres']\n",
    "fact_table['cost'] = df['Cost']\n",
    "fact_table['duration_days'] = df['Duration_Days']\n",
    "fact_table['cost_source'] = df['Cost_Source']\n",
    "\n",
    "print(\"Star schema tables created successfully.\")\n",
    "\n",
    "# Let's save these tables to CSV files for now\n",
    "time_dim.to_csv('dim_time.csv', index=False)\n",
    "location_dim.to_csv('dim_location.csv', index=False)\n",
    "cause_dim.to_csv('dim_cause.csv', index=False)\n",
    "inc_type_dim.to_csv('dim_incident_type.csv', index=False)\n",
    "fact_table.to_csv('fact_incident.csv', index=False)\n",
    "\n",
    "print(\"Star schema tables saved to CSV files.\")\n",
    "print(\"- dim_time.csv\")\n",
    "print(\"- dim_location.csv\")\n",
    "print(\"- dim_cause.csv\")\n",
    "print(\"- dim_incident_type.csv\")\n",
    "print(\"- fact_incident.csv\")\n",
    "\n",
    "# Display previews of each dimension table\n",
    "print(\"\\\n",
    "Time Dimension Preview:\")\n",
    "print(time_dim.head(3))\n",
    "print(\"\\\n",
    "Location Dimension Preview:\")\n",
    "print(location_dim.head(3))\n",
    "print(\"\\\n",
    "Cause Dimension Preview:\")\n",
    "print(cause_dim.head(3))\n",
    "print(\"\\\n",
    "Incident Type Dimension Preview:\")\n",
    "print(inc_type_dim.head(3))\n",
    "print(\"\\\n",
    "Fact Table Preview:\")\n",
    "print(fact_table.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6ea6f27-9d80-431a-80b9-a5e480cbed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Dimension Preview:\n",
      "    full_date  date_id  year  month  day  quarter month_name day_of_week\n",
      "0  2008-02-25        1  2008      2   25        1   February      Monday\n",
      "1  2008-03-02        2  2008      3    2        1      March      Sunday\n",
      "2  2008-03-14        3  2008      3   14        1      March      Friday\n",
      "Location Dimension Preview:\n",
      "   location_id GACC State\n",
      "0            1   NW    OR\n",
      "1            2   NW    WA\n",
      "2            3   AK    AK\n",
      "Cause Dimension Preview:\n",
      "  cause_code  cause_id cause_description\n",
      "0          L         1         Lightning\n",
      "1          H         2             Human\n",
      "2          U         3           Unknown\n",
      "Incident Type Dimension Preview:\n",
      "  inc_type_code  inc_type_id inc_type_description\n",
      "0            WF            1             Wildfire\n",
      "Fact Table Preview:\n",
      "   incident_id    incident_name  start_date_id  end_date_id  location_id  \\\n",
      "0            1  Buzzard Complex            252          258            1   \n",
      "1            2  Carlton Complex            252          257            2   \n",
      "2            3      Funny River            249          255            3   \n",
      "\n",
      "   cause_id  inc_type_id  size_acres        cost  duration_days cost_source  \n",
      "0         1            1    395747.0  11062411.0             59    Original  \n",
      "1         1            1    256108.0  68800000.0             45    Original  \n",
      "2         2            1    195858.0  11496627.0             87    Original  \n",
      "All tables have been successfully uploaded to PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "# Let's create a script to upload our star schema to PostgreSQL\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load the CSV files we created earlier\n",
    "time_dim = pd.read_csv('dim_time.csv')\n",
    "location_dim = pd.read_csv('dim_location.csv')\n",
    "cause_dim = pd.read_csv('dim_cause.csv')\n",
    "inc_type_dim = pd.read_csv('dim_incident_type.csv')\n",
    "fact_table = pd.read_csv('fact_incident.csv')\n",
    "\n",
    "# Print a preview of each table to confirm they loaded correctly\n",
    "print(\"Time Dimension Preview:\")\n",
    "print(time_dim.head(3))\n",
    "print(\"\\\n",
    "Location Dimension Preview:\")\n",
    "print(location_dim.head(3))\n",
    "print(\"\\\n",
    "Cause Dimension Preview:\")\n",
    "print(cause_dim.head(3))\n",
    "print(\"\\\n",
    "Incident Type Dimension Preview:\")\n",
    "print(inc_type_dim.head(3))\n",
    "print(\"\\\n",
    "Fact Table Preview:\")\n",
    "print(fact_table.head(3))\n",
    "\n",
    "username = 'postgres'\n",
    "password = '1Reinhardt!'  # Replace with your actual password\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "database = 'wildfire_db'\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Create the engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Write the tables to PostgreSQL\n",
    "time_dim.to_sql('dim_time', engine, if_exists='replace', index=False)\n",
    "location_dim.to_sql('dim_location', engine, if_exists='replace', index=False)\n",
    "cause_dim.to_sql('dim_cause', engine, if_exists='replace', index=False)\n",
    "inc_type_dim.to_sql('dim_incident_type', engine, if_exists='replace', index=False)\n",
    "fact_table.to_sql('fact_incident', engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"All tables have been successfully uploaded to PostgreSQL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c13c7f22-8ef1-4149-ac1a-1454cf927c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'port'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 72\u001b[0m\n\u001b[0;32m     68\u001b[0m fact_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost_source\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCost_Source\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# ----- PostgreSQL Upload -----  \u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Create engine; replace with your actual connection details  \u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgresql://username:password@host:port/database_name\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Write the tables to PostgreSQL (if the tables already exist, they will be replaced)  \u001b[39;00m\n\u001b[0;32m     75\u001b[0m time_dim\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_time\u001b[39m\u001b[38;5;124m'\u001b[39m, engine, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    275\u001b[0m         _warn_with_version(\n\u001b[0;32m    276\u001b[0m             messages[m],\n\u001b[0;32m    277\u001b[0m             versions[m],\n\u001b[0;32m    278\u001b[0m             version_warnings[m],\n\u001b[0;32m    279\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    280\u001b[0m         )\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:546\u001b[0m, in \u001b[0;36mcreate_engine\u001b[1;34m(url, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty_in_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# create url.URL object\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m u \u001b[38;5;241m=\u001b[39m _url\u001b[38;5;241m.\u001b[39mmake_url(url)\n\u001b[0;32m    548\u001b[0m u, plugins, kwargs \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_instantiate_plugins(kwargs)\n\u001b[0;32m    550\u001b[0m entrypoint \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39m_get_entrypoint()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\url.py:842\u001b[0m, in \u001b[0;36mmake_url\u001b[1;34m(name_or_url)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given a string, produce a new URL instance.\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \n\u001b[0;32m    828\u001b[0m \u001b[38;5;124;03mThe format of the URL generally follows `RFC-1738\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    838\u001b[0m \n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_url, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_url(name_or_url)\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_url, URL) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m    844\u001b[0m     name_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_sqla_is_testing_if_this_is_a_mock_object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    845\u001b[0m ):\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mArgumentError(\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string or URL object, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_or_url\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\url.py:903\u001b[0m, in \u001b[0;36m_parse_url\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    900\u001b[0m     name \u001b[38;5;241m=\u001b[39m components\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m components[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 903\u001b[0m         components[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(components[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m URL\u001b[38;5;241m.\u001b[39mcreate(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponents)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'port'"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "from sqlalchemy import create_engine  \n",
    "  \n",
    "# Load the CSV data  \n",
    "df = pd.read_csv('cleaned_incident_data_imputed.csv', encoding='utf-8')  \n",
    "  \n",
    "# Replace any instance of 'Unknown' with 'U' in the Cause column  \n",
    "df['Cause'] = df['Cause'].replace('Unknown', 'U')  \n",
    "  \n",
    "# ----- Create Dimension Tables -----  \n",
    "  \n",
    "# 1. Time Dimension: combine unique dates from Start_Date and Last_Report_Date  \n",
    "all_dates = pd.concat([df['Start_Date'], df['Last_Report_Date']]).unique()  \n",
    "# sort dates for consistency  \n",
    "sorted_dates = sorted(all_dates)  \n",
    "time_dim = pd.DataFrame({'full_date': sorted_dates})  \n",
    "time_dim['date_id'] = range(1, len(time_dim) + 1)  \n",
    "time_dim['year'] = pd.to_datetime(time_dim['full_date']).dt.year  \n",
    "time_dim['month'] = pd.to_datetime(time_dim['full_date']).dt.month  \n",
    "time_dim['day'] = pd.to_datetime(time_dim['full_date']).dt.day  \n",
    "time_dim['quarter'] = pd.to_datetime(time_dim['full_date']).dt.quarter  \n",
    "time_dim['month_name'] = pd.to_datetime(time_dim['full_date']).dt.month_name()  \n",
    "time_dim['day_of_week'] = pd.to_datetime(time_dim['full_date']).dt.day_name()  \n",
    "  \n",
    "# 2. Location Dimension  \n",
    "location_dim = df[['GACC', 'State']].drop_duplicates().reset_index(drop=True)  \n",
    "location_dim['location_id'] = range(1, len(location_dim) + 1)  \n",
    "location_dim = location_dim[['location_id', 'GACC', 'State']]  \n",
    "  \n",
    "# 3. Cause Dimension  \n",
    "cause_dim = pd.DataFrame({'cause_code': df['Cause'].unique()})  \n",
    "cause_dim['cause_id'] = range(1, len(cause_dim) + 1)  \n",
    "cause_descriptions = {  \n",
    "    'L': 'Lightning',  \n",
    "    'H': 'Human',  \n",
    "    'U': 'Unknown',  \n",
    "    'N': 'Natural (non-lightning)'  \n",
    "}  \n",
    "cause_dim['cause_description'] = cause_dim['cause_code'].map(cause_descriptions)  \n",
    "  \n",
    "# 4. Incident Type Dimension  \n",
    "inc_type_dim = pd.DataFrame({'inc_type_code': df['Inc_Type'].unique()})  \n",
    "inc_type_dim['inc_type_id'] = range(1, len(inc_type_dim) + 1)  \n",
    "inc_type_descriptions = {  \n",
    "    'WF': 'Wildfire'  \n",
    "}  \n",
    "inc_type_dim['inc_type_description'] = inc_type_dim['inc_type_code'].map(inc_type_descriptions)  \n",
    "  \n",
    "# ----- Create Fact Table -----  \n",
    "  \n",
    "# Create mapping dictionaries from dimension tables  \n",
    "start_date_mapping = dict(zip(time_dim['full_date'], time_dim['date_id']))  \n",
    "location_mapping = { (row['GACC'], row['State']) : row['location_id'] for _, row in location_dim.iterrows() }  \n",
    "cause_mapping = dict(zip(cause_dim['cause_code'], cause_dim['cause_id']))  \n",
    "inc_type_mapping = dict(zip(inc_type_dim['inc_type_code'], inc_type_dim['inc_type_id']))  \n",
    "  \n",
    "fact_table = pd.DataFrame()  \n",
    "fact_table['incident_id'] = range(1, len(df) + 1)  \n",
    "fact_table['incident_name'] = df['Name']  \n",
    "fact_table['start_date_id'] = df['Start_Date'].map(start_date_mapping)  \n",
    "fact_table['end_date_id'] = df['Last_Report_Date'].map(start_date_mapping)  \n",
    "fact_table['location_id'] = df.apply(lambda x: location_mapping.get((x['GACC'], x['State'])), axis=1)  \n",
    "fact_table['cause_id'] = df['Cause'].map(cause_mapping)  \n",
    "fact_table['inc_type_id'] = df['Inc_Type'].map(inc_type_mapping)  \n",
    "fact_table['size_acres'] = df['Size_Acres']  \n",
    "fact_table['cost'] = df['Cost']  \n",
    "fact_table['duration_days'] = df['Duration_Days']  \n",
    "fact_table['cost_source'] = df['Cost_Source']  \n",
    "  \n",
    "# ----- PostgreSQL Upload -----  \n",
    "# Create engine; replace with your actual connection details  \n",
    "engine = create_engine('postgresql://username:password@host:port/database_name')  \n",
    "  \n",
    "# Write the tables to PostgreSQL (if the tables already exist, they will be replaced)  \n",
    "time_dim.to_sql('dim_time', engine, if_exists='replace', index=False)  \n",
    "location_dim.to_sql('dim_location', engine, if_exists='replace', index=False)  \n",
    "cause_dim.to_sql('dim_cause', engine, if_exists='replace', index=False)  \n",
    "inc_type_dim.to_sql('dim_incident_type', engine, if_exists='replace', index=False)  \n",
    "fact_table.to_sql('fact_incident', engine, if_exists='replace', index=False)  \n",
    "  \n",
    "print('Star schema tables have been created and uploaded to PostgreSQL successfully.')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
