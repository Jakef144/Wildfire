{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbc35ec-ea68-45f2-9166-e93e676118c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in c:\\users\\jakef\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (2.2.2)\n",
      "Requirement already satisfied: numpy>1.24.4 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (1.26.4)\n",
      "Requirement already satisfied: distro in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables detected: 0\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Install tabula-py (if not already installed)  \n",
    "%pip install tabula-py  \n",
    "  \n",
    "import tabula  \n",
    "import pandas as pd  \n",
    "  \n",
    "# Define the PDF file and the pages we want to extract the table from  \n",
    "pdf_path = \"annual_report_2017_508_0.pdf\"  \n",
    "pages = \"9-10\"  # pages 9 and 10  \n",
    "  \n",
    "# Use tabula to read the table from the specified pages  \n",
    "# This will return a list of DataFrames  \n",
    "tables = tabula.read_pdf(pdf_path, pages=pages, multiple_tables=True)  \n",
    "  \n",
    "# Let's see how many tables were extracted and show a preview of the first few rows of each  \n",
    "print(\"Number of tables detected:\", len(tables))  \n",
    "for idx, table in enumerate(tables):  \n",
    "    print(f\"\\nTable {idx+1} preview:\")  \n",
    "    print(table.head())  \n",
    "  \n",
    "print(\"\\nDone\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228bd045-be25-48d2-9c5f-c10970c46df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in c:\\users\\jakef\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (2.2.2)\n",
      "Requirement already satisfied: numpy>1.24.4 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (1.26.4)\n",
      "Requirement already satisfied: distro in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Number of tables detected with lattice=True: 12\n",
      "\n",
      "Table 1 preview:\n",
      "Empty DataFrame\n",
      "5]lifornia that would begin in early November and continue into December. Precipitation inmer\n",
      "Index: []\n",
      "\n",
      "Table 2 preview:\n",
      "Empty DataFrame\n",
      "6]sidences,4,002minorstructures,229commercialstructuresand10mixed,065five more than inrage).ts of\n",
      "Index: []\n",
      "\n",
      "Table 3 preview:\n",
      "  commercial/residentialstructures.Thisiswellabovetheannualaverageof1,545\\rresidences, 1,236 minor structures, and 55 commercial structures destroyed by wildfire\\rwith this year ranking 1st in total structures lost (data from 1999 to present). California\\raccounted for the highest number of structures lost in one state in 2017: 7,778 residences,\\r178 commercial structures and 3,056 minor structures. Florida was second with 44\\rresidences, 1 mixed commercial and residential and 282 minor structures.\\r\\rRequests for firefighting resources placed to the National Interagency Coordination\\rCenter during the 2017 fire season were higher than the 10-year average in most\\rcategories.  Filled requests for crews, engines, overhead, helicopters and heavy air\\rtankers exceeded their respective 10-year averages.\\r\\rNational Type 1 teams were mobilized 55 times (up from 31 in 2016) and spent 829 days\\ron assignments (up from 392 days in 2016).  All 16 national teams had between one and\\rsix assignments each.  Type 2 Teams were mobilized 137 times (up from 192 in 2016),\\rfor a total of 1,873 days assigned to incidents (up from 1,046 days in 2016). (Figures\\rinclude both national and state teams.) Area Command teams were mobilized four times\\r(upfromzeroassignmentsin2016).NationalIncidentManagementOrganizations\\r(NIMO) mobilized 8 times in 2017 to both wildland fire and non-fire incidents (up from 7\\rtimes in 2016).\\r\\r\\rMilitary and International Resource Mobilizations\\r\\rMilitary: On July 22nd, one MAFFS unit was activated through a Request for Assistance\\r(RFA) to the Department of Defense. This request was filled with one MAFFS from the\\r152nd Airlift Wing and was positioned at Fresno, CA in support wildland fire operations.\\rOn July 30th one MAFFS from the 153rd Airlift Wing and one MAFFS from the 302nd Airlift\\rWing was positioned at Fresno, CA. On September 8th one additional MAFFS from the\\r202nd Airlift wing was positioned at Fresno, CA. On September 16th all MAFFS were\\rreleased back to the DOD.\\r\\rMAFFS units primarily provided retardant delivery to the Northern and Southern California\\rGeographic Areas while employed from July 22 through September 16.  These units\\rdelivered a total of 820,115 gallons of retardant while conducting 293 sorties.  This is up\\rfrom 2016 when 165 sorties were flown delivering 395,632 gallons of retardant.\\r\\rOn August 14th one RC-26 aircraft and support personnel from the 141st Aerial Refueling\\rWing (Washington Air National Guard) was deployed to Fairchild AFB (Spokane, WA) in\\rsupport of wildland fire operations. A second RC-26 aircraft and support personnel from\\rthe 162nd Fighter Wing (Arizona Air National Guard) was deployed to Mahlon Sweet Field\\r(Eugene, OR) in support of wildland fire operations. Both aircraft provided incident\\rawareness and assessment through infrared sensing capability to wildland fire incident\\rmanagement personnel until released on September 15th.\\r\\r7\n",
      "0                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "1  Military: On July 22nd, one MAFFS unit was act...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "2  (RFA) to the Department of Defense. This reque...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "3  152nd Airlift Wing and was positioned at Fresn...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "4  On July 30th one MAFFS from the 153rd Airlift ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "\n",
      "Table 4 preview:\n",
      "  commercial/residentialstructures.Thisiswellabovetheannualaverageof1,545\\rresidences, 1,236 minor structures, and 55 commercial structures destroyed by wildfire\\rwith this year ranking 1st in total structures lost (data from 1999 to present). California\\raccounted for the highest number of structures lost in one state in 2017: 7,778 residences,\\r178 commercial structures and 3,056 minor structures. Florida was second with 44\\rresidences, 1 mixed commercial and residential and 282 minor structures.\\r\\rRequests for firefighting resources placed to the National Interagency Coordination\\rCenter during the 2017 fire season were higher than the 10-year average in most\\rcategories.  Filled requests for crews, engines, overhead, helicopters and heavy air\\rtankers exceeded their respective 10-year averages.\\r\\rNational Type 1 teams were mobilized 55 times (up from 31 in 2016) and spent 829 days\\ron assignments (up from 392 days in 2016).  All 16 national teams had between one and\\rsix assignments each.  Type 2 Teams were mobilized 137 times (up from 192 in 2016),\\rfor a total of 1,873 days assigned to incidents (up from 1,046 days in 2016). (Figures\\rinclude both national and state teams.) Area Command teams were mobilized four times\\r(upfromzeroassignmentsin2016).NationalIncidentManagementOrganizations\\r(NIMO) mobilized 8 times in 2017 to both wildland fire and non-fire incidents (up from 7\\rtimes in 2016).\\r\\r\\rMilitary and International Resource Mobilizations\\r\\rMilitary: On July 22nd, one MAFFS unit was activated through a Request for Assistance\\r(RFA) to the Department of Defense. This request was filled with one MAFFS from the\\r152nd Airlift Wing and was positioned at Fresno, CA in support wildland fire operations.\\rOn July 30th one MAFFS from the 153rd Airlift Wing and one MAFFS from the 302nd Airlift\\rWing was positioned at Fresno, CA. On September 8th one additional MAFFS from the\\r202nd Airlift wing was positioned at Fresno, CA. On September 16th all MAFFS were\\rreleased back to the DOD.\\r\\rMAFFS units primarily provided retardant delivery to the Northern and Southern California\\rGeographic Areas while employed from July 22 through September 16.  These units\\rdelivered a total of 820,115 gallons of retardant while conducting 293 sorties.  This is up\\rfrom 2016 when 165 sorties were flown delivering 395,632 gallons of retardant.\\r\\rOn August 14th one RC-26 aircraft and support personnel from the 141st Aerial Refueling\\rWing (Washington Air National Guard) was deployed to Fairchild AFB (Spokane, WA) in\\rsupport of wildland fire operations. A second RC-26 aircraft and support personnel from\\rthe 162nd Fighter Wing (Arizona Air National Guard) was deployed to Mahlon Sweet Field\\r(Eugene, OR) in support of wildland fire operations. Both aircraft provided incident\\rawareness and assessment through infrared sensing capability to wildland fire incident\\rmanagement personnel until released on September 15th.\\r\\r7\n",
      "0                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "1  Military: On July 22nd, one MAFFS unit was act...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "2  (RFA) to the Department of Defense. This reque...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "3  152nd Airlift Wing and was positioned at Fresn...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "4  On July 30th one MAFFS from the 153rd Airlift ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "\n",
      "Table 5 preview:\n",
      "  On September 11th two hundred and forty-five soldiers from the 23rd Brigade Engineer\\rBattalion and 1-23 Infantry Battalion out of Fort Lewis, Washington were deployed in\\rsupport of the Umpqua North Complex. They were released back to Fort Lewis on\\rSeptember 26th.\\r\\rInternational: On September 7th through the NIFC-CIFFC Agreement Canada provided\\rone CL-415 scooper group from Ontario to the U.S.  On September 11th Ontario provided\\roneadditionalCL-415scoopergroup.Bothscoopergroupswereassignedtothe\\rNorthern Rockies Geographic Area. On September 16th both CL-415 scooper groups\\rwere released back to Ontario.\\r\\r8  \\\n",
      "0  On September 11th two hundred and forty-five s...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "1  Battalion and 1-23 Infantry Battalion out of F...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "2  support of the Umpqua North Complex. They were...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "3                                    September 26th.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "4                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "\n",
      "                                         Unnamed: 0  \n",
      "0                                               NaN  \n",
      "1                                               NaN  \n",
      "2                                               NaN  \n",
      "3                                               NaN  \n",
      "4  through the NIFC-CIFFC Agreement Canada provided  \n",
      "\n",
      "Table 6 preview:\n",
      "  On September 11th two hundred and forty-five soldiers from the 23rd Brigade Engineer\\rBattalion and 1-23 Infantry Battalion out of Fort Lewis, Washington were deployed in\\rsupport of the Umpqua North Complex. They were released back to Fort Lewis on\\rSeptember 26th.\\r\\rInternational: On September 7th through the NIFC-CIFFC Agreement Canada provided\\rone CL-415 scooper group from Ontario to the U.S.  On September 11th Ontario provided\\roneadditionalCL-415scoopergroup.Bothscoopergroupswereassignedtothe\\rNorthern Rockies Geographic Area. On September 16th both CL-415 scooper groups\\rwere released back to Ontario.\\r\\r8  \\\n",
      "0                                                NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "1  one CL-415 scooper group from Ontario to the U...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "2  oneadditionalCL-415scoopergroup.Bothscoopergro...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "3  Northern Rockies Geographic Area. On September...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "4                      were released back to Ontario                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "\n",
      "                                         Unnamed: 0  \n",
      "0  through the NIFC-CIFFC Agreement Canada provided  \n",
      "1                                               NaN  \n",
      "2                                               NaN  \n",
      "3                                               NaN  \n",
      "4                                               NaN  \n",
      "\n",
      "Table 7 preview:\n",
      "  On September 11th two hundred and forty-five soldiers from the 23rd Brigade Engineer\\rBattalion and 1-23 Infantry Battalion out of Fort Lewis, Washington were deployed in\\rsupport of the Umpqua North Complex. They were released back to Fort Lewis on\\rSeptember 26th.\\r\\rInternational: On September 7th through the NIFC-CIFFC Agreement Canada provided\\rone CL-415 scooper group from Ontario to the U.S.  On September 11th Ontario provided\\roneadditionalCL-415scoopergroup.Bothscoopergroupswereassignedtothe\\rNorthern Rockies Geographic Area. On September 16th both CL-415 scooper groups\\rwere released back to Ontario.\\r\\r8\n",
      "0  On September 11th two hundred and forty-five s...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "1  Battalion and 1-23 Infantry Battalion out of F...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "2  support of the Umpqua North Complex. They were...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "3                                    September 26th.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "\n",
      "Table 8 preview:\n",
      "  Significant Wildland Fires\\rFires and Complexes over 40,000 acres in 2017\\r\\rLast\\rNameGACCStateStart\\rReportSize In\\rDateCause*Estimated Cost\\rDateAcres\\rNW Oklahoma\\rComplexSAOK3/73/24779,292U3,200,000\\rPerrytonSATX3/63/13318,156HNR\\rActive\\rThomasSOCA12/4into270,000U123,836,000\\r2018\\rLodgepole\\rComplexNRMT7/208/11270,000U9,800,000\\rRoosters CombGBNV7/97/24218,380U4,000,000\\rChetco BarNWOR7/1210/26191,125L72,000,000\\rRice RidgeNRMT7/2410/17160,187L49,251,000\\rFour Seasons\\rComplexGBNV7/167/25159,986U4,573,000\\rWest MimsSAGA4/66/29152,515L45,500,000\\rLefors EastSATX3/73/10135,000HNR\\rDiamond CreekNWWA7/2310/5128,272U14,760,000\\rCentral LNU\\rComplexNOCA10/910/10110,720U102,000,000\\rSartin DrawNRMT8/309/499,735L1,500,000\\rTruckeeGBNV7/37/1198,960U2,120,260\\rTohakum 2GBNV8/297/1194,221L3,456,000\\rCampbell RiverAKAK6/2610/1293,520L1,056,688\\rOakNOCA8/129/2386,830LNR\\rHighlineGBID7/289/2184,619L2,167,864\\rLong ValleyGBCA7/117/2183,733U11,953,000\\rModoc July\\rComplexNOCA7/248/1383,120U35,200,000\\rDetwilerSOCA7/168/781,826U90,000,000\\rWhite Mountain\\rCreekAKAK7/310/578,857L96,310\\rEclipse ComplexNOCA8/1510/878,698U46,006,691\\rBrush FlatNRMT8/319/473,797L400,000\\rBrianheadGBUT6/177/2471,673H3,660,000\\rNena SpringsNWOR8/88/2968,007H8,900,000\\rSalmon August\\rComplexNOCA8/1310/465,889U39,805,700\\rDry GulchGBNV7/47/1164,000U2,700,000\\rMeyersNRMT7/1410/1362,034L32,800,000\\rSnowstormGBNV7/147/1560,000U482,448\\r9  \\\n",
      "0                          Significant Wildland Fire                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "1                                               Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "2                               NW Oklahoma\\rComplex                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "3                                           Perryton                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "4                                             Thomas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "\n",
      "  Unnamed: 0 Unnamed: 1   Unnamed: 2          Unnamed: 3      Unnamed: 4  \\\n",
      "0        NaN        NaN          NaN                 NaN             NaN   \n",
      "1       GACC      State  Start\\rDate  Last\\rReport\\rDate  Size In\\rAcres   \n",
      "2         SA         OK          3/7                3/24         779,292   \n",
      "3         SA         TX          3/6                3/13         318,156   \n",
      "4         SO         CA         12/4  Active\\rinto\\r2018         270,000   \n",
      "\n",
      "  Unnamed: 5      Unnamed: 6  \n",
      "0        NaN             NaN  \n",
      "1     Cause*  Estimated Cost  \n",
      "2          U       3,200,000  \n",
      "3          H              NR  \n",
      "4          U     123,836,000  \n",
      "\n",
      "Table 9 preview:\n",
      "  Significant Wildland Fires\\rFires and Complexes over 40,000 acres in 2017\\r\\rLast\\rNameGACCStateStart\\rReportSize In\\rDateCause*Estimated Cost\\rDateAcres\\rNW Oklahoma\\rComplexSAOK3/73/24779,292U3,200,000\\rPerrytonSATX3/63/13318,156HNR\\rActive\\rThomasSOCA12/4into270,000U123,836,000\\r2018\\rLodgepole\\rComplexNRMT7/208/11270,000U9,800,000\\rRoosters CombGBNV7/97/24218,380U4,000,000\\rChetco BarNWOR7/1210/26191,125L72,000,000\\rRice RidgeNRMT7/2410/17160,187L49,251,000\\rFour Seasons\\rComplexGBNV7/167/25159,986U4,573,000\\rWest MimsSAGA4/66/29152,515L45,500,000\\rLefors EastSATX3/73/10135,000HNR\\rDiamond CreekNWWA7/2310/5128,272U14,760,000\\rCentral LNU\\rComplexNOCA10/910/10110,720U102,000,000\\rSartin DrawNRMT8/309/499,735L1,500,000\\rTruckeeGBNV7/37/1198,960U2,120,260\\rTohakum 2GBNV8/297/1194,221L3,456,000\\rCampbell RiverAKAK6/2610/1293,520L1,056,688\\rOakNOCA8/129/2386,830LNR\\rHighlineGBID7/289/2184,619L2,167,864\\rLong ValleyGBCA7/117/2183,733U11,953,000\\rModoc July\\rComplexNOCA7/248/1383,120U35,200,000\\rDetwilerSOCA7/168/781,826U90,000,000\\rWhite Mountain\\rCreekAKAK7/310/578,857L96,310\\rEclipse ComplexNOCA8/1510/878,698U46,006,691\\rBrush FlatNRMT8/319/473,797L400,000\\rBrianheadGBUT6/177/2471,673H3,660,000\\rNena SpringsNWOR8/88/2968,007H8,900,000\\rSalmon August\\rComplexNOCA8/1310/465,889U39,805,700\\rDry GulchGBNV7/47/1164,000U2,700,000\\rMeyersNRMT7/1410/1362,034L32,800,000\\rSnowstormGBNV7/147/1560,000U482,448\\r9\n",
      "0                          Significant Wildland Fire                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "\n",
      "Table 10 preview:\n",
      "  Significant Wildland Fires\\rFires and Complexes over 40,000 acres in 2017\\r\\rLast\\rNameGACCStateStart\\rReportSize In\\rDateCause*Estimated Cost\\rDateAcres\\rNW Oklahoma\\rComplexSAOK3/73/24779,292U3,200,000\\rPerrytonSATX3/63/13318,156HNR\\rActive\\rThomasSOCA12/4into270,000U123,836,000\\r2018\\rLodgepole\\rComplexNRMT7/208/11270,000U9,800,000\\rRoosters CombGBNV7/97/24218,380U4,000,000\\rChetco BarNWOR7/1210/26191,125L72,000,000\\rRice RidgeNRMT7/2410/17160,187L49,251,000\\rFour Seasons\\rComplexGBNV7/167/25159,986U4,573,000\\rWest MimsSAGA4/66/29152,515L45,500,000\\rLefors EastSATX3/73/10135,000HNR\\rDiamond CreekNWWA7/2310/5128,272U14,760,000\\rCentral LNU\\rComplexNOCA10/910/10110,720U102,000,000\\rSartin DrawNRMT8/309/499,735L1,500,000\\rTruckeeGBNV7/37/1198,960U2,120,260\\rTohakum 2GBNV8/297/1194,221L3,456,000\\rCampbell RiverAKAK6/2610/1293,520L1,056,688\\rOakNOCA8/129/2386,830LNR\\rHighlineGBID7/289/2184,619L2,167,864\\rLong ValleyGBCA7/117/2183,733U11,953,000\\rModoc July\\rComplexNOCA7/248/1383,120U35,200,000\\rDetwilerSOCA7/168/781,826U90,000,000\\rWhite Mountain\\rCreekAKAK7/310/578,857L96,310\\rEclipse ComplexNOCA8/1510/878,698U46,006,691\\rBrush FlatNRMT8/319/473,797L400,000\\rBrianheadGBUT6/177/2471,673H3,660,000\\rNena SpringsNWOR8/88/2968,007H8,900,000\\rSalmon August\\rComplexNOCA8/1310/465,889U39,805,700\\rDry GulchGBNV7/47/1164,000U2,700,000\\rMeyersNRMT7/1410/1362,034L32,800,000\\rSnowstormGBNV7/147/1560,000U482,448\\r9  \\\n",
      "0                                               Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "1                               NW Oklahoma\\rComplex                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "2                                           Perryton                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "3                                             Thomas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "4                                 Lodgepole\\rComplex                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "\n",
      "  Unnamed: 0 Unnamed: 1   Unnamed: 2          Unnamed: 3      Unnamed: 4  \\\n",
      "0       GACC      State  Start\\rDate  Last\\rReport\\rDate  Size In\\rAcres   \n",
      "1         SA         OK          3/7                3/24         779,292   \n",
      "2         SA         TX          3/6                3/13         318,156   \n",
      "3         SO         CA         12/4  Active\\rinto\\r2018         270,000   \n",
      "4         NR         MT         7/20                8/11         270,000   \n",
      "\n",
      "  Unnamed: 5      Unnamed: 6  \n",
      "0     Cause*  Estimated Cost  \n",
      "1          U       3,200,000  \n",
      "2          H              NR  \n",
      "3          U     123,836,000  \n",
      "4          U       9,800,000  \n",
      "\n",
      "Table 11 preview:\n",
      "  NameGACCStateStartLast\\rDateReportSize In\\rDateAcresCause*Estimated Cost\\rRankin Ranch\\rRd.SATX3/233/2560,000HNR\\rPowerlineGBID8/48/1355,529U4,512,207\\rNowitnaAKAK7/148/3155,254L307,802\\rLegion LakeRMSD12/1112/1954,023H2,200,000\\rLolo PeakNRMT7/1510/653,902L48,500,000\\rCinder ButteNWOR8/28/1652,531H4,400,000\\rNorse PeakNWWA8/1110/552,056U19,760,000\\rSouthern LNU\\rComplexNOCA10/910/2751,624U48,509,895\\rMammoth CaveGBID8/48/849,912H560,000\\rGarzaSOCA7/97/1048,889U15,453,433\\rEagle CreekNWOR9/210/2748,831H20,251,700\\rFryeSWAZ6/78/3148,443L20,000,000\\rSawmillSWAZ4/235/346,991H8,200,000\\rBear MountainAKAK7/911/1646,715LNR\\rSapphire\\rComplexNRMT7/2410/443,733U35,406,736\\rUmpqua North\\rComplexNWAK8/1110/1243,158U42,890,561\\rPowerlineNRID7/157/2942,558L2,958,255\\rHighway 200\\rComplexNRMT9/110/642,442L12,500,000\\rEarthstoneGBNV7/37/1141,545U3,194,000\\rBoulder CreekAKAK7/210/1240,906L917,520\\rHelmetAKAK7/68/2440,202L26,978\\rL  Lightning     H  Human     U  Unknown/Under Investigation     OT  Other     NR  Not Reported\\r\\rInformation in the above table was derived from ICS-209 reports submitted in the Fire and Aviation\\rManagement Web Applications system (FAMWEB). Information shown may not reflect final official figures.\\r\\r10  \\\n",
      "0                                               Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "1                                  Rankin Ranch\\rRd.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "2                                          Powerline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "3                                            Nowitna                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "4                                        Legion Lake                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "\n",
      "  Unnamed: 0 Unnamed: 1   Unnamed: 2          Unnamed: 3      Unnamed: 4  \\\n",
      "0       GACC      State  Start\\rDate  Last\\rReport\\rDate  Size In\\rAcres   \n",
      "1         SA         TX         3/23                3/25          60,000   \n",
      "2         GB         ID          8/4                8/13          55,529   \n",
      "3         AK         AK         7/14                8/31          55,254   \n",
      "4         RM         SD        12/11               12/19          54,023   \n",
      "\n",
      "  Unnamed: 5      Unnamed: 6  \n",
      "0     Cause*  Estimated Cost  \n",
      "1          H              NR  \n",
      "2          U       4,512,207  \n",
      "3          L         307,802  \n",
      "4          H       2,200,000  \n",
      "\n",
      "Table 12 preview:\n",
      "  NameGACCStateStartLast\\rDateReportSize In\\rDateAcresCause*Estimated Cost\\rRankin Ranch\\rRd.SATX3/233/2560,000HNR\\rPowerlineGBID8/48/1355,529U4,512,207\\rNowitnaAKAK7/148/3155,254L307,802\\rLegion LakeRMSD12/1112/1954,023H2,200,000\\rLolo PeakNRMT7/1510/653,902L48,500,000\\rCinder ButteNWOR8/28/1652,531H4,400,000\\rNorse PeakNWWA8/1110/552,056U19,760,000\\rSouthern LNU\\rComplexNOCA10/910/2751,624U48,509,895\\rMammoth CaveGBID8/48/849,912H560,000\\rGarzaSOCA7/97/1048,889U15,453,433\\rEagle CreekNWOR9/210/2748,831H20,251,700\\rFryeSWAZ6/78/3148,443L20,000,000\\rSawmillSWAZ4/235/346,991H8,200,000\\rBear MountainAKAK7/911/1646,715LNR\\rSapphire\\rComplexNRMT7/2410/443,733U35,406,736\\rUmpqua North\\rComplexNWAK8/1110/1243,158U42,890,561\\rPowerlineNRID7/157/2942,558L2,958,255\\rHighway 200\\rComplexNRMT9/110/642,442L12,500,000\\rEarthstoneGBNV7/37/1141,545U3,194,000\\rBoulder CreekAKAK7/210/1240,906L917,520\\rHelmetAKAK7/68/2440,202L26,978\\rL  Lightning     H  Human     U  Unknown/Under Investigation     OT  Other     NR  Not Reported\\r\\rInformation in the above table was derived from ICS-209 reports submitted in the Fire and Aviation\\rManagement Web Applications system (FAMWEB). Information shown may not reflect final official figures.\\r\\r10  \\\n",
      "0                                               Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "1                                  Rankin Ranch\\rRd.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "2                                          Powerline                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "3                                            Nowitna                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "4                                        Legion Lake                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      "\n",
      "  Unnamed: 0 Unnamed: 1   Unnamed: 2          Unnamed: 3      Unnamed: 4  \\\n",
      "0       GACC      State  Start\\rDate  Last\\rReport\\rDate  Size In\\rAcres   \n",
      "1         SA         TX         3/23                3/25          60,000   \n",
      "2         GB         ID          8/4                8/13          55,529   \n",
      "3         AK         AK         7/14                8/31          55,254   \n",
      "4         RM         SD        12/11               12/19          54,023   \n",
      "\n",
      "  Unnamed: 5      Unnamed: 6  \n",
      "0     Cause*  Estimated Cost  \n",
      "1          H              NR  \n",
      "2          U       4,512,207  \n",
      "3          L         307,802  \n",
      "4          H       2,200,000  \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import tabula  \n",
    "import pandas as pd  \n",
    "  \n",
    "pdf_path = \"annual_report_2017_508_0.pdf\"  \n",
    "pages = \"7-12\"  \n",
    "  \n",
    "# Use tabula with lattice mode enabled  \n",
    "tables = tabula.read_pdf(pdf_path, pages=pages, multiple_tables=True, lattice=True)  \n",
    "  \n",
    "print(\"Number of tables detected with lattice=True:\", len(tables))  \n",
    "for idx, table in enumerate(tables):  \n",
    "    print(f\"\\nTable {idx+1} preview:\")  \n",
    "    print(table.head())  \n",
    "  \n",
    "print(\"\\nDone\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98f4c2d-e3e5-4f44-9800-51fb2f5903c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from pages 7 to 13...\n",
      "Page 7 preview (first 500 chars):\n",
      "5 \n",
      " \n",
      "In the East, precipitation was generally within 25 percent of normal for the summer \n",
      "months. One exception was along the Gulf Coast of Texas and Louisiana in late August \n",
      "where Hurricane Harvey, a rapidly intensifying Category 4 hurricane, made landfall near \n",
      "Rockport, Texas, on August 25. The storm was among the wettest ever. An area of 3,643 \n",
      "square miles between the Louisiana state line with Texas and Houston recorded at least \n",
      "40 inches of rain between August 23 and August 30. The highe\n",
      "Page 8 preview (first 500 chars):\n",
      "6 \n",
      " \n",
      "fell across the western states but did not extend into central and southern portions of \n",
      "California. Fuels returned to a critically dry state rapidly and significant North Wind events \n",
      "across northern and central California produced several large fires in early November. A \n",
      "month later, strong Santa Ana winds led to the development of multiple large fires in \n",
      "central and southern California. Beneficial precipitation and higher humidity did not come \n",
      "to the region until mid to late December.\n",
      "Page 9 preview (first 500 chars):\n",
      "7 \n",
      " \n",
      "commercial/residential structures. This is well above the annual average of 1,545 \n",
      "residences, 1,236 minor structures, and 55 commercial structures destroyed by wildfire \n",
      "with this year ranking 1st in total structures lost (data from 1999 to present). California \n",
      "accounted for the highest number of structures lost in one state in 2017: 7,778 residences, \n",
      "178 commercial structures and 3,056 minor structures. Florida was second with 44 \n",
      "residences, 1 mixed commercial and residential and 282 m\n",
      "Page 10 preview (first 500 chars):\n",
      "8 \n",
      " \n",
      "On September 11th two hundred and forty-five soldiers from the 23rd Brigade Engineer \n",
      "Battalion and 1-23 Infantry Battalion out of Fort Lewis, Washington were deployed in \n",
      "support of the Umpqua North Complex. They were released back to Fort Lewis on \n",
      "September 26th. \n",
      " \n",
      "International: On September 7th through the NIFC-CIFFC Agreement Canada provided \n",
      "one CL-415 scooper group from Ontario to the U.S.  On September 11th Ontario provided \n",
      "one additional CL-415 scooper group. Both scooper groups\n",
      "Page 11 preview (first 500 chars):\n",
      "9 \n",
      " \n",
      "Significant Wildland Fires \n",
      "Fires and Complexes over 40,000 acres in 2017 \n",
      " \n",
      "Name \n",
      "GACC \n",
      "State \n",
      "Start \n",
      "Date \n",
      "Last \n",
      "Report \n",
      "Date \n",
      "Size In \n",
      "Acres \n",
      "Cause* \n",
      "Estimated Cost \n",
      "NW Oklahoma \n",
      "Complex \n",
      "SA \n",
      "OK \n",
      "3/7 \n",
      "3/24 \n",
      "779,292 \n",
      "U \n",
      "3,200,000 \n",
      "Perryton \n",
      "SA \n",
      "TX \n",
      "3/6 \n",
      "3/13 \n",
      "318,156 \n",
      "H \n",
      "NR \n",
      "Thomas \n",
      "SO \n",
      "CA \n",
      "12/4 \n",
      "Active \n",
      "into \n",
      "2018 \n",
      "270,000 \n",
      "U \n",
      "123,836,000 \n",
      "Lodgepole \n",
      "Complex \n",
      "NR \n",
      "MT \n",
      "7/20 \n",
      "8/11 \n",
      "270,000 \n",
      "U \n",
      "9,800,000 \n",
      "Roosters Comb \n",
      "GB \n",
      "NV \n",
      "7/9 \n",
      "7/24 \n",
      "218,380 \n",
      "U \n",
      "4,000,000 \n",
      "Chetco Bar \n",
      "NW \n",
      "OR \n",
      "7/12 \n",
      "10/2\n",
      "Page 12 preview (first 500 chars):\n",
      "10 \n",
      " \n",
      "Name \n",
      "GACC \n",
      "State \n",
      "Start \n",
      "Date \n",
      "Last \n",
      "Report \n",
      "Date \n",
      "Size In \n",
      "Acres \n",
      "Cause* \n",
      "Estimated Cost \n",
      "Rankin Ranch \n",
      "Rd. \n",
      "SA \n",
      "TX \n",
      "3/23 \n",
      "3/25 \n",
      "60,000 \n",
      "H \n",
      "NR \n",
      "Powerline \n",
      "GB \n",
      "ID \n",
      "8/4 \n",
      "8/13 \n",
      "55,529 \n",
      "U \n",
      "4,512,207 \n",
      "Nowitna \n",
      "AK \n",
      "AK \n",
      "7/14 \n",
      "8/31 \n",
      "55,254 \n",
      "L \n",
      "307,802 \n",
      "Legion Lake \n",
      "RM \n",
      "SD \n",
      "12/11 \n",
      "12/19 \n",
      "54,023 \n",
      "H \n",
      "2,200,000 \n",
      "Lolo Peak \n",
      "NR \n",
      "MT \n",
      "7/15 \n",
      "10/6 \n",
      "53,902 \n",
      "L \n",
      "48,500,000 \n",
      "Cinder Butte \n",
      "NW \n",
      "OR \n",
      "8/2 \n",
      "8/16 \n",
      "52,531 \n",
      "H \n",
      "4,400,000 \n",
      "Norse Peak \n",
      "NW \n",
      "WA \n",
      "8/11 \n",
      "10/5 \n",
      "52,056 \n",
      "U \n",
      "19,760,000 \n",
      "Southern LNU \n",
      "Complex \n",
      "NO \n",
      "Page 13 preview (first 500 chars):\n",
      "11 \n",
      " \n",
      "Significant Fire Activity \n",
      "Significant fires are defined in the National Mobilization Guide as fires that are a minimum \n",
      "of 100 acres in timber fuel types, 300 acres in grass and brush fuel types, or are managed \n",
      "by a Type 1 or 2 Incident Management Team, WFMT or NIMO.  \n",
      " \n",
      "There were 1,409 large or significant wildfires and complexes reported in 2017 (derived \n",
      "from ICS-209 reports submitted through FAMWEB). Significant wildfires represented \n",
      "about 2 percent of total wildfires reported nati\n",
      "Text extraction complete.\n",
      "\n",
      "Identified cost lines:\n",
      "No cost lines found. You may need to adjust the regex pattern.\n",
      "\n",
      "Loading CSV file...\n",
      "CSV loaded. Showing incidents from 2017 with 'NR' cost:\n",
      "                      Name Cost\n",
      "32    NW Oklahoma\\nComplex   NR\n",
      "33                Perryton   NR\n",
      "35      Lodgepole\\nComplex   NR\n",
      "39   Four Seasons\\nComplex   NR\n",
      "41             Lefors East   NR\n",
      "43    Central LNU\\nComplex   NR\n",
      "47          Campbell River   NR\n",
      "48                     Oak   NR\n",
      "51     Modoc July\\nComplex   NR\n",
      "53   White Mountain\\nCreek   NR\n",
      "58  Salmon August\\nComplex   NR\n",
      "\n",
      "Total incidents updated: 0\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_manual.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import re  \n",
    "import pandas as pd  \n",
    "import os  \n",
    "  \n",
    "# Define file paths  \n",
    "pdf_path = \"annual_report_2017_508_0.pdf\"  \n",
    "csv_path = \"combined_incidents_clean_final_updated_2021_v6.csv\"  \n",
    "output_csv = \"combined_incidents_clean_final_updated_manual.csv\"  \n",
    "  \n",
    "# Function to normalize incident names by stripping, lowercasing, and removing extra spaces  \n",
    "def normalize_name(name):  \n",
    "    return re.sub(r'\\\\s+', ' ', name.strip().lower())  \n",
    "  \n",
    "# Extract text from pages 7 to 13  \n",
    "document = fitz.open(pdf_path)  \n",
    "extracted_text = \"\"  \n",
    "print(\"Extracting text from pages 7 to 13...\")  \n",
    "for page_num in range(6, 13):  # pages 7-13 (0-indexed)  \n",
    "    page = document.load_page(page_num)  \n",
    "    page_text = page.get_text()  \n",
    "    print(\"Page\", page_num+1, \"preview (first 500 chars):\")  \n",
    "    print(page_text[:500])  \n",
    "    extracted_text += \"\\n\" + page_text  \n",
    "document.close()  \n",
    "print(\"Text extraction complete.\\n\")  \n",
    "  \n",
    "# Use regex to locate lines that likely contain incident names and cost information.  \n",
    "# For example, we search for lines with a cost number pattern (e.g. 3,200,000).   \n",
    "# We assume the cost is a number which might include commas, and that the incident name appears before it.  \n",
    "#  \n",
    "# This is a rough approach and might need adjustments based on your PDF's table layout.  \n",
    "#  \n",
    "# Example pattern: (incident name) ... (cost)  \n",
    "cost_pattern = re.compile(r'(?P<incident>[\\w\\s\\.\\-]+?)\\\\s+(?P<cost>\\\\d{1,3}(?:,\\\\d{3})+(?:\\\\.\\\\d+)?)(?!\\\\S)', re.MULTILINE)  \n",
    "  \n",
    "matches = cost_pattern.finditer(extracted_text)  \n",
    "cost_lookup = {}  \n",
    "print(\"Identified cost lines:\")  \n",
    "for match in matches:  \n",
    "    incident_name = match.group(\"incident\")  \n",
    "    cost_val = match.group(\"cost\")  \n",
    "    normalized_incident = normalize_name(incident_name)  \n",
    "    cost_lookup[normalized_incident] = cost_val  \n",
    "    print(\"Incident:\", incident_name, \"-> Cost:\", cost_val)  \n",
    "  \n",
    "if not cost_lookup:  \n",
    "    print(\"No cost lines found. You may need to adjust the regex pattern.\")  \n",
    "  \n",
    "# Load the CSV file and update cost values for 2017 incidents where cost is 'NR'  \n",
    "print(\"\\nLoading CSV file...\")  \n",
    "df = pd.read_csv(csv_path)  \n",
    "print(\"CSV loaded. Showing incidents from 2017 with 'NR' cost:\")  \n",
    "df_2017_nr = df[(df['Year'] == 2017) & (df['Cost'] == 'NR')]  \n",
    "print(df_2017_nr[['Name', 'Cost']])  \n",
    "  \n",
    "update_count = 0  \n",
    "for idx, row in df[df['Year'] == 2017].iterrows():  \n",
    "    normalized_incident = normalize_name(row['Name'])  \n",
    "    # Try to find a cost value by matching incident names  \n",
    "    if normalized_incident in cost_lookup:  \n",
    "        cost = cost_lookup[normalized_incident]  \n",
    "        # Only update if cost is not empty and not already set  \n",
    "        if str(cost).upper() != 'NR' and str(cost).strip() != '':  \n",
    "            df.at[idx, 'Cost'] = cost  \n",
    "            update_count += 1  \n",
    "            print(\"Updated incident:\", row['Name'], \"with cost:\", cost)  \n",
    "  \n",
    "print(\"\\nTotal incidents updated:\", update_count)  \n",
    "  \n",
    "# Save the updated CSV  \n",
    "df.to_csv(output_csv, index=False)  \n",
    "print(\"Updated CSV saved to:\", output_csv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ea1055-ea88-4681-834b-8616a083a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\hanna\\anaconda3\\lib\\site-packages (0.11.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
      "Extracting tables from pages 7 to 13 using pdfplumber...\n",
      "\n",
      "--- Page 7 ---\n",
      "Found 0 tables on this page.\n",
      "\n",
      "--- Page 8 ---\n",
      "Found 0 tables on this page.\n",
      "\n",
      "--- Page 9 ---\n",
      "Found 1 tables on this page.\n",
      "\n",
      "Table 1 preview (first 5 rows):\n",
      "                                                    \n",
      "0  Military: On July 22nd, one MAFFS unit was act...\n",
      "1  (RFA) to the Department of Defense. This reque...\n",
      "2  152nd Airlift Wing and was positioned at Fresn...\n",
      "3  On July 30th one MAFFS from the 153rd Airlift ...\n",
      "4  Wing was positioned at Fresno, CA. On Septembe...\n",
      "\n",
      "--- Page 10 ---\n",
      "Found 2 tables on this page.\n",
      "\n",
      "Table 1 preview (first 5 rows):\n",
      "  On September 11th two hundred and forty-five soldiers from the 23rd Brigade Engineer\n",
      "0  Battalion and 1-23 Infantry Battalion out of F...                                  \n",
      "1  support of the Umpqua North Complex. They were...                                  \n",
      "2                                    September 26th.                                  \n",
      "\n",
      "Table 2 preview (first 5 rows):\n",
      "                                                None  \\\n",
      "0  one CL-415 scooper group from Ontario to the U...   \n",
      "1  one additional CL-415 scooper group. Both scoo...   \n",
      "2  Northern Rockies Geographic Area. On September...   \n",
      "3                     were released back to Ontario.   \n",
      "\n",
      "  through the NIFC-CIFFC Agreement Canada provided  \n",
      "0                                             None  \n",
      "1                                             None  \n",
      "2                                             None  \n",
      "3                                             None  \n",
      "\n",
      "--- Page 11 ---\n",
      "Found 1 tables on this page.\n",
      "\n",
      "Table 1 preview (first 5 rows):\n",
      "                   Name GACC State Start\\nDate  Last\\nReport\\nDate  \\\n",
      "0  NW Oklahoma\\nComplex   SA    OK         3/7                3/24   \n",
      "1              Perryton   SA    TX         3/6                3/13   \n",
      "2                Thomas   SO    CA        12/4  Active\\ninto\\n2018   \n",
      "3    Lodgepole\\nComplex   NR    MT        7/20                8/11   \n",
      "4         Roosters Comb   GB    NV         7/9                7/24   \n",
      "\n",
      "  Size In\\nAcres Cause* Estimated Cost  \n",
      "0        779,292      U      3,200,000  \n",
      "1        318,156      H             NR  \n",
      "2        270,000      U    123,836,000  \n",
      "3        270,000      U      9,800,000  \n",
      "4        218,380      U      4,000,000  \n",
      "\n",
      "--- Page 12 ---\n",
      "Found 1 tables on this page.\n",
      "\n",
      "Table 1 preview (first 5 rows):\n",
      "                Name GACC State Start\\nDate Last\\nReport\\nDate Size In\\nAcres  \\\n",
      "0  Rankin Ranch\\nRd.   SA    TX        3/23               3/25         60,000   \n",
      "1          Powerline   GB    ID         8/4               8/13         55,529   \n",
      "2            Nowitna   AK    AK        7/14               8/31         55,254   \n",
      "3        Legion Lake   RM    SD       12/11              12/19         54,023   \n",
      "4          Lolo Peak   NR    MT        7/15               10/6         53,902   \n",
      "\n",
      "  Cause* Estimated Cost  \n",
      "0      H             NR  \n",
      "1      U      4,512,207  \n",
      "2      L        307,802  \n",
      "3      H      2,200,000  \n",
      "4      L     48,500,000  \n",
      "\n",
      "--- Page 13 ---\n",
      "Found 0 tables on this page.\n",
      "\n",
      "Using table with name column: Name and cost column: Estimated Cost\n",
      "\n",
      "Found 27 incident cost entries from the PDF.\n",
      "\n",
      "First 10 entries in incident->cost lookup:\n",
      "nw oklahoma complex -> 3,200,000\n",
      "thomas -> 123,836,000\n",
      "lodgepole complex -> 9,800,000\n",
      "roosters comb -> 4,000,000\n",
      "chetco bar -> 72,000,000\n",
      "rice ridge -> 49,251,000\n",
      "four seasons complex -> 4,573,000\n",
      "west mims -> 45,500,000\n",
      "diamond creek -> 14,760,000\n",
      "central lnu complex -> 102,000,000\n",
      "\n",
      "Loading CSV file...\n",
      "CSV loaded. Showing incidents from 2017 with 'NR' cost:\n",
      "                      Name Cost\n",
      "32    NW Oklahoma\\nComplex   NR\n",
      "33                Perryton   NR\n",
      "35      Lodgepole\\nComplex   NR\n",
      "39   Four Seasons\\nComplex   NR\n",
      "41             Lefors East   NR\n",
      "43    Central LNU\\nComplex   NR\n",
      "47          Campbell River   NR\n",
      "48                     Oak   NR\n",
      "51     Modoc July\\nComplex   NR\n",
      "53   White Mountain\\nCreek   NR\n",
      "58  Salmon August\\nComplex   NR\n",
      "Updated incident: NW Oklahoma\n",
      "Complex with cost: 3,200,000\n",
      "Updated incident: Thomas with cost: 123,836,000\n",
      "Updated incident: Lodgepole\n",
      "Complex with cost: 9,800,000\n",
      "Updated incident: Roosters Comb with cost: 4,000,000\n",
      "Updated incident: Chetco Bar with cost: 72,000,000\n",
      "Updated incident: Rice Ridge with cost: 49,251,000\n",
      "Updated incident: Four Seasons\n",
      "Complex with cost: 4,573,000\n",
      "Updated incident: West Mims with cost: 45,500,000\n",
      "Updated incident: Diamond Creek with cost: 14,760,000\n",
      "Updated incident: Central LNU\n",
      "Complex with cost: 102,000,000\n",
      "Updated incident: Sartin Draw with cost: 1,500,000\n",
      "Updated incident: Truckee with cost: 2,120,260\n",
      "Updated incident: Tohakum 2 with cost: 3,456,000\n",
      "Updated incident: Campbell River with cost: 1,056,688\n",
      "Updated incident: Highline with cost: 2,167,864\n",
      "Updated incident: Long Valley with cost: 11,953,000\n",
      "Updated incident: Modoc July\n",
      "Complex with cost: 35,200,000\n",
      "Updated incident: Detwiler with cost: 90,000,000\n",
      "Updated incident: White Mountain\n",
      "Creek with cost: 96,310\n",
      "Updated incident: Eclipse Complex with cost: 46,006,691\n",
      "Updated incident: Brush Flat with cost: 400,000\n",
      "Updated incident: Brianhead with cost: 3,660,000\n",
      "Updated incident: Nena Springs with cost: 8,900,000\n",
      "Updated incident: Salmon August\n",
      "Complex with cost: 39,805,700\n",
      "Updated incident: Dry Gulch with cost: 2,700,000\n",
      "Updated incident: Meyers with cost: 32,800,000\n",
      "Updated incident: Snowstorm with cost: 482,448\n",
      "\n",
      "Total incidents updated: 27\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_manual.csv\n"
     ]
    }
   ],
   "source": [
    "# Install pdfplumber if not already installed  \n",
    "%pip install pdfplumber  \n",
    "  \n",
    "import pdfplumber  \n",
    "import pandas as pd  \n",
    "import re  \n",
    "import os  \n",
    "  \n",
    "# Define file paths  \n",
    "pdf_path = \"annual_report_2017_508_0.pdf\"  \n",
    "csv_path = \"combined_incidents_clean_final_updated_2021_v6.csv\"  \n",
    "output_csv = \"combined_incidents_clean_final_updated_manual.csv\"  \n",
    "  \n",
    "# Function to normalize incident names by stripping, lowercasing, and removing extra spaces  \n",
    "def normalize_name(name):  \n",
    "    return re.sub(r'\\s+', ' ', str(name).strip().lower())  \n",
    "  \n",
    "# Attempt to extract tables using pdfplumber on pages 7-13  \n",
    "all_tables = []  \n",
    "print(\"Extracting tables from pages 7 to 13 using pdfplumber...\")  \n",
    "with pdfplumber.open(pdf_path) as pdf:  \n",
    "    for i in range(6, 13):  # pages 7 to 13 (0-indexed)  \n",
    "        if i < len(pdf.pages):  \n",
    "            page = pdf.pages[i]  \n",
    "            print(\"\\n--- Page\", i+1, \"---\")  \n",
    "            tables = page.extract_tables()  \n",
    "            print(\"Found\", len(tables), \"tables on this page.\")  \n",
    "            for idx, table in enumerate(tables):  \n",
    "                df_temp = pd.DataFrame(table[1:], columns=table[0])  \n",
    "                print(\"\\nTable\", idx+1, \"preview (first 5 rows):\")  \n",
    "                print(df_temp.head())  \n",
    "                all_tables.append(df_temp)  \n",
    "        else:  \n",
    "            print(\"Page\", i+1, \"does not exist in this PDF.\")  \n",
    "              \n",
    "# If multiple tables were extracted, you'll need to confirm which one has the cost data.  \n",
    "# For demonstration, we assume that one of the tables has columns for incident name and estimated cost.  \n",
    "# To help us build a lookup, we loop through each table and search for a column header containing 'Name'  \n",
    "# and one containing 'Cost'. Adjust these conditions as needed.  \n",
    "cost_lookup = {}  \n",
    "for table in all_tables:  \n",
    "    col_names = [col.lower() for col in table.columns if isinstance(col, str)]  \n",
    "    if any('name' in col for col in col_names) and any('cost' in col for col in col_names):  \n",
    "        # Identify columns; this is a crude check; you may need to adjust based on exact column names.  \n",
    "        name_col = [col for col in table.columns if isinstance(col, str) and 'name' in col.lower()][0]  \n",
    "        cost_col = [col for col in table.columns if isinstance(col, str) and 'cost' in col.lower()][0]  \n",
    "        print(\"\\nUsing table with name column:\", name_col, \"and cost column:\", cost_col)  \n",
    "          \n",
    "        # Build lookup dictionary from normalized incident name to cost  \n",
    "        for idx, row in table.iterrows():  \n",
    "            incident = row[name_col]  \n",
    "            cost = row[cost_col]  \n",
    "            if incident and cost and (str(cost).strip().upper() != 'NR'):  \n",
    "                normalized_incident = normalize_name(incident)  \n",
    "                cost_lookup[normalized_incident] = str(cost).strip()  \n",
    "        break  \n",
    "  \n",
    "print(\"\\nFound\", len(cost_lookup), \"incident cost entries from the PDF.\")  \n",
    "  \n",
    "# OPTIONALLY: print first few entries in the lookup for inspection  \n",
    "print(\"\\nFirst 10 entries in incident->cost lookup:\")  \n",
    "count = 0  \n",
    "for k, v in cost_lookup.items():  \n",
    "    print(k, \"->\", v)  \n",
    "    count += 1  \n",
    "    if count >= 10:  \n",
    "        break  \n",
    "  \n",
    "# Load the CSV file and update cost values for 2017 incidents where cost is 'NR'  \n",
    "print(\"\\nLoading CSV file...\")  \n",
    "df = pd.read_csv(csv_path)  \n",
    "print(\"CSV loaded. Showing incidents from 2017 with 'NR' cost:\")  \n",
    "df_2017_nr = df[(df['Year'] == 2017) & (df['Cost'] == 'NR')]  \n",
    "print(df_2017_nr[['Name', 'Cost']])  \n",
    "  \n",
    "update_count = 0  \n",
    "for idx, row in df[df['Year'] == 2017].iterrows():  \n",
    "    normalized_incident = normalize_name(row['Name'])  \n",
    "    # Try to find a cost value by matching incident names  \n",
    "    if normalized_incident in cost_lookup:  \n",
    "        cost = cost_lookup[normalized_incident]  \n",
    "        # Only update if cost is not empty and not already set to an actual value (not 'NR')  \n",
    "        if str(cost).upper() != 'NR' and str(cost).strip() != '':  \n",
    "            df.at[idx, 'Cost'] = cost  \n",
    "            update_count += 1  \n",
    "            print(\"Updated incident:\", row['Name'], \"with cost:\", cost)  \n",
    "  \n",
    "print(\"\\nTotal incidents updated:\", update_count)  \n",
    "  \n",
    "# Save the updated CSV  \n",
    "df.to_csv(output_csv, index=False)  \n",
    "print(\"Updated CSV saved to:\", output_csv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1b7c0e-4711-488c-8c92-992299db509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tables from the 2008 report using pdfplumber...\n",
      "\n",
      "Total pages in PDF: 76\n",
      "\n",
      "----- Page 1 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 2 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 3 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 4 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 5 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 6 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 7 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 8 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 9 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 10 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 11 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 12 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 13 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 14 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 15 -----\n",
      "Found 1 tables on this page.\n",
      "\n",
      "Table 1 preview (first 5 rows):\n",
      "                     Name Inc.\\nType GACC State Start Date  \\\n",
      "0              Glass Fire         WF   SA    TX  2/25/2008   \n",
      "1         Klamath Theater         WF   NO    CA  6/21/2008   \n",
      "2           Basin Complex         WF   SO    CA  6/21/2008   \n",
      "3  Iron & Alps\\nComplexes         WF   NO    CA  6/21/2008   \n",
      "4       Dunn Mtn.\\nAssist         WF   NR    MT  8/21/2008   \n",
      "\n",
      "  Contain or\\nControl\\nDate Size\\n(Acres) Cause          Cost  \n",
      "0                  3/2/2008       219,556     H            NR  \n",
      "1                 9/26/2008       192,038     L  $126,086,065  \n",
      "2                 7/29/2008       162,818     L   $78,096,079  \n",
      "3                  9/4/2008       105,805     L   $73,974,917  \n",
      "4                  9/2/2008       102,383     L    $2,900,000  \n",
      "\n",
      "----- Page 16 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 17 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 18 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 19 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 20 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 21 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 22 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 23 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 24 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 25 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 26 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 27 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 28 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 29 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 30 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 31 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 32 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 33 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 34 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 35 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 36 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 37 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 38 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 39 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 40 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 41 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 42 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 43 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 44 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 45 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 46 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 47 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 48 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 49 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 50 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 51 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 52 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 53 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 54 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 55 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 56 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 57 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 58 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 59 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 60 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 61 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 62 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 63 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 64 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 65 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 66 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 67 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 68 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 69 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 70 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 71 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 72 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 73 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 74 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 75 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "----- Page 76 -----\n",
      "Found 0 tables on this page.\n",
      "\n",
      "Selected table for cost extraction:\n",
      "                     Name Inc.\\nType GACC State Start Date  \\\n",
      "0              Glass Fire         WF   SA    TX  2/25/2008   \n",
      "1         Klamath Theater         WF   NO    CA  6/21/2008   \n",
      "2           Basin Complex         WF   SO    CA  6/21/2008   \n",
      "3  Iron & Alps\\nComplexes         WF   NO    CA  6/21/2008   \n",
      "4       Dunn Mtn.\\nAssist         WF   NR    MT  8/21/2008   \n",
      "\n",
      "  Contain or\\nControl\\nDate Size\\n(Acres) Cause          Cost  \n",
      "0                  3/2/2008       219,556     H            NR  \n",
      "1                 9/26/2008       192,038     L  $126,086,065  \n",
      "2                 7/29/2008       162,818     L   $78,096,079  \n",
      "3                  9/4/2008       105,805     L   $73,974,917  \n",
      "4                  9/2/2008       102,383     L    $2,900,000  \n",
      "\n",
      "Using column Name for incident names and Cost for cost values.\n",
      "\n",
      "Extracted 15 incident cost entries from the PDF.\n",
      "\n",
      "First 10 entries in incident -> cost lookup:\n",
      "klamath theater -> $126,086,065\n",
      "basin complex -> $78,096,079\n",
      "iron & alps complexes -> $73,974,917\n",
      "dunn mtn. assist -> $2,900,000\n",
      "lime complex -> $59,329,698\n",
      "shu lightning complex -> $56,438,391\n",
      "siskiyou / blue 2 complex -> $65,692,836\n",
      "indians -> $42,500,000\n",
      "gunbarrel -> $11,200,000\n",
      "stiles complex -> $71,644\n",
      "\n",
      "Loading CSV file to update missing cost values...\n",
      "Incident records from 2008 with missing cost values:\n",
      "Empty DataFrame\n",
      "Columns: [Name, Cost]\n",
      "Index: []\n",
      "Updated incident: Klamath Theater with cost: $126,086,065\n",
      "Updated incident: Basin Complex with cost: $78,096,079\n",
      "Updated incident: Iron & Alps \n",
      "Complexes with cost: $73,974,917\n",
      "Updated incident: Dunn Mtn. \n",
      "Assist with cost: $2,900,000\n",
      "Updated incident: Lime Complex with cost: $59,329,698\n",
      "Updated incident: SHU Lightning \n",
      "Complex with cost: $56,438,391\n",
      "Updated incident: Indians with cost: $42,500,000\n",
      "Updated incident: Gunbarrel with cost: $11,200,000\n",
      "Updated incident: Stiles Complex with cost: $71,644\n",
      "Updated incident: BTU Lightning \n",
      "Complex with cost: $94,825,683\n",
      "Updated incident: MEU Lightning \n",
      "Complex with cost: $66,000,000\n",
      "Updated incident: East Slide Rock \n",
      "Ridge with cost: $8,873,000\n",
      "Updated incident: Evans Road with cost: $18,249,415\n",
      "\n",
      "Total incidents updated: 13\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_manual_2008.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber  \n",
    "import pandas as pd  \n",
    "import re  \n",
    "  \n",
    "# Define file paths  \n",
    "pdf_path = \"annual_report_2008_508.pdf\"  \n",
    "csv_path = \"combined_incidents_clean_final_updated_2021_v6.csv\"  # Adjust if needed  \n",
    "output_csv = \"combined_incidents_clean_final_updated_manual_2008.csv\"  \n",
    "  \n",
    "# Function to normalize incident names (lowercase, strip, and reduce whitespace)  \n",
    "def normalize_name(name):  \n",
    "    return re.sub(r'\\s+', ' ', str(name).strip().lower())  \n",
    "  \n",
    "# Extract tables from the PDF pages  \n",
    "all_tables = []  \n",
    "print(\"Extracting tables from the 2008 report using pdfplumber...\\n\")  \n",
    "with pdfplumber.open(pdf_path) as pdf:  \n",
    "    total_pages = len(pdf.pages)  \n",
    "    print(\"Total pages in PDF:\", total_pages)  \n",
    "    # Loop through all pages (or adjust the range if you know where the tables are)  \n",
    "    for i in range(total_pages):  \n",
    "        page = pdf.pages[i]  \n",
    "        print(\"\\n----- Page\", i+1, \"-----\")  \n",
    "        tables = page.extract_tables()  \n",
    "        print(\"Found\", len(tables), \"tables on this page.\")  \n",
    "        for idx, table in enumerate(tables):  \n",
    "            if table and len(table) > 1:  \n",
    "                df_temp = pd.DataFrame(table[1:], columns=table[0])  \n",
    "                print(\"\\nTable\", idx+1, \"preview (first 5 rows):\")  \n",
    "                print(df_temp.head())  \n",
    "                all_tables.append(df_temp)  \n",
    "            else:  \n",
    "                print(\"Table\", idx+1, \"is empty or incomplete.\")  \n",
    "  \n",
    "# -------------------------------------------------------------------------  \n",
    "# At this point, inspect the output cells to determine which table(s)  \n",
    "# contain the incident names and cost values. Adjust the following  \n",
    "# code to select the appropriate table. For demonstration, we assume  \n",
    "# that one of the tables (e.g., the first candidate) contains the cost data.  \n",
    "# -------------------------------------------------------------------------  \n",
    "# For example, lets assume the desired table is the first candidate with a column header  \n",
    "# containing 'Cost' or 'Estimat'. You can adjust these conditions as needed.  \n",
    "cost_table = None  \n",
    "for table in all_tables:  \n",
    "    for col in table.columns:  \n",
    "        if isinstance(col, str) and (\"cost\" in col.lower() or \"estim\" in col.lower()):  \n",
    "            cost_table = table.copy()  \n",
    "            break  \n",
    "    if cost_table is not None:  \n",
    "        break  \n",
    "  \n",
    "if cost_table is None:  \n",
    "    print(\"\\nNo table with cost information detected. Please manually inspect the printed tables.\")  \n",
    "else:  \n",
    "    print(\"\\nSelected table for cost extraction:\")  \n",
    "    print(cost_table.head())  \n",
    "  \n",
    "    # You might need to adjust the column names.  \n",
    "    # For example, assume the table has columns 'Name' and 'Estimated Cost'  \n",
    "    # If these differ, update the variables below accordingly.  \n",
    "    name_col = None  \n",
    "    cost_col = None  \n",
    "    for col in cost_table.columns:  \n",
    "        if isinstance(col, str):  \n",
    "            if \"name\" in col.lower():  \n",
    "                name_col = col  \n",
    "            if \"cost\" in col.lower() or \"estim\" in col.lower():  \n",
    "                cost_col = col  \n",
    "  \n",
    "    if name_col is None or cost_col is None:  \n",
    "        print(\"\\nUnable to find the appropriate columns. Please update the column selection manually.\")  \n",
    "    else:  \n",
    "        print(\"\\nUsing column\", name_col, \"for incident names and\", cost_col, \"for cost values.\")  \n",
    "  \n",
    "        # Build a lookup dictionary from incident names to cost values  \n",
    "        cost_lookup = {}  \n",
    "        for idx, row in cost_table.iterrows():  \n",
    "            incident = row[name_col]  \n",
    "            cost = row[cost_col]  \n",
    "            if incident and cost and (str(cost).strip().upper() != 'NR'):  \n",
    "                normalized_incident = normalize_name(incident)  \n",
    "                cost_lookup[normalized_incident] = str(cost).strip()  \n",
    "  \n",
    "        print(\"\\nExtracted\", len(cost_lookup), \"incident cost entries from the PDF.\")  \n",
    "        print(\"\\nFirst 10 entries in incident -> cost lookup:\")  \n",
    "        count = 0  \n",
    "        for k, v in cost_lookup.items():  \n",
    "            print(k, \"->\", v)  \n",
    "            count += 1  \n",
    "            if count >= 10:  \n",
    "                break  \n",
    "  \n",
    "        # -------------------------------------------------------------------------  \n",
    "        # Now load your CSV, update the cost values for incidents (e.g., for 2008 if that's the case)  \n",
    "        # Make sure to adjust the 'Year' filter if necessary.  \n",
    "        # -------------------------------------------------------------------------  \n",
    "        print(\"\\nLoading CSV file to update missing cost values...\")  \n",
    "        df = pd.read_csv(csv_path)  \n",
    "        # Adjust the filter below appropriately. For example, if the CSV file contains the year 2008:  \n",
    "        df_target = df[df['Year'] == 2008]  \n",
    "        print(\"Incident records from 2008 with missing cost values:\")  \n",
    "        print(df_target[df_target['Cost'] == 'NR'][['Name', 'Cost']])  \n",
    "  \n",
    "        update_count = 0  \n",
    "        for idx, row in df_target.iterrows():  \n",
    "            normalized_incident = normalize_name(row['Name'])  \n",
    "            if normalized_incident in cost_lookup:  \n",
    "                cost = cost_lookup[normalized_incident]  \n",
    "                if str(cost).upper() != 'NR' and str(cost).strip() != '':  \n",
    "                    df.at[idx, 'Cost'] = cost  \n",
    "                    update_count += 1  \n",
    "                    print(\"Updated incident:\", row['Name'], \"with cost:\", cost)  \n",
    "  \n",
    "        print(\"\\nTotal incidents updated:\", update_count)  \n",
    "  \n",
    "        # Save the updated CSV  \n",
    "        df.to_csv(output_csv, index=False)  \n",
    "        print(\"Updated CSV saved to:\", output_csv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f1fa74-9248-4322-9c7b-41ffca286d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cost data from the 2022 report...\n",
      "Extracted 31 cost entries from page 9\n",
      "First few lookup entries:\n",
      "lime complex -> 12726992\n",
      "hermits peak -> 330100293\n",
      "black -> 60190000\n",
      "paradise complex -> 2782896\n",
      "tatlawiksuk -> 75965\n",
      "nc hpai 2022 -> 484012\n",
      "bean complex -> 11656365\n",
      "hog butte -> 628717\n",
      "double creek -> 39500000\n",
      "east fork -> 6150000\n",
      "\n",
      "Loading CSV file...\n",
      "Found 40 records for 2022.\n",
      "Records with missing costs: 40\n",
      "Updated: Lime Complex with cost: 12726992\n",
      "Updated: Hermits Peak with cost: 330100293\n",
      "Updated: Black with cost: 60190000\n",
      "Updated: Paradise Complex with cost: 2782896\n",
      "Updated: Tatlawiksuk with cost: 75965\n",
      "Updated: NC HPAI 2022 with cost: 484012\n",
      "Updated: Bean Complex with cost: 11656365\n",
      "No match for: Koktuli River\n",
      "Updated: Hog Butte with cost: 628717\n",
      "Updated: Double Creek with cost: 39500000\n",
      "Updated: East Fork with cost: 6150000\n",
      "No match for: Poorman Complex\n",
      "Updated: Moose with cost: 98200000\n",
      "Updated: Cedar Creek with cost: 133750000\n",
      "Updated: Aghaluk Mountain with cost: 12726992\n",
      "Updated: Door Mountain with cost: 500\n",
      "Updated: Chitanana with cost: 1148818\n",
      "Updated: Dalton Highway\n",
      "Complex with cost: 6971097\n",
      "Updated: Pike Creek with cost: 76037\n",
      "No match for: Swift River\n",
      "No match for: Apoon Pass\n",
      "Updated: Mosquito with cost: 181100000\n",
      "Updated: Clear with cost: 2363465\n",
      "Updated: Bitzshitini with cost: 216545\n",
      "Updated: Camp Creek with cost: 45000\n",
      "Updated: Donut with cost: 45000\n",
      "Updated: Middle Tanana\n",
      "Complex with cost: 3434457\n",
      "No match for: North Fork\n",
      "Updated: McKinney with cost: 96580000\n",
      "Updated: Cooks Peak with cost: 12500000\n",
      "No match for: Lansing Creek\n",
      "Updated: Eastland Complex with cost: 7\n",
      "No match for: Door Creek\n",
      "Updated: Schilling Creek with cost: 50000\n",
      "No match for: Titnuk Creek\n",
      "No match for: Borrega\n",
      "Updated: Fourth of July\n",
      "Creek with cost: 70912\n",
      "Updated: Radio Creek with cost: 75000\n",
      "Updated: Kiknik with cost: 3033\n",
      "Updated: Cerro Pelado with cost: 46800000\n",
      "\n",
      "Total incidents updated: 31\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_manual_2022.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber  \n",
    "import pandas as pd  \n",
    "import re  \n",
    "  \n",
    "# Define file paths  \n",
    "pdf_path = \"annual_report.2022.pdf\"  \n",
    "csv_path = \"combined_incidents_clean_final_updated_2021_v6.csv\"  \n",
    "output_csv = \"combined_incidents_clean_final_updated_manual_2022.csv\"  \n",
    "  \n",
    "# Function to normalize incident names (convert to string, strip, lower, and reduce whitespace)  \n",
    "def normalize_name(name):  \n",
    "    return re.sub(r'\\\\s+', ' ', str(name).strip().lower())  \n",
    "  \n",
    "# Extract the table that includes the estimated cost.  \n",
    "# You mentioned that the preview output showed two pages with 1 table each,  \n",
    "# one of which appears to have cost data with a column 'Estimated Cost'.  \n",
    "# We'll scan through all pages and pick the first table where that column seems to be present.  \n",
    "cost_lookup = {}  \n",
    "  \n",
    "print(\"Extracting cost data from the 2022 report...\")  \n",
    "with pdfplumber.open(pdf_path) as pdf:  \n",
    "    for i, page in enumerate(pdf.pages):  \n",
    "        tables = page.extract_tables()  \n",
    "        if not tables:  \n",
    "            continue  \n",
    "        for table in tables:  \n",
    "            # Convert to DataFrame if table has headers and at least 2 rows.  \n",
    "            if len(table) < 2:  \n",
    "                continue  \n",
    "            df_temp = pd.DataFrame(table[1:], columns=table[0])  \n",
    "            # Use lower-case column names for matching.  \n",
    "            df_temp.columns = [str(col).strip().lower() for col in df_temp.columns]  \n",
    "            # Check if table has an 'estimated cost' column.  \n",
    "            possible_cost_cols = [col for col in df_temp.columns if 'cost' in col]  \n",
    "            if possible_cost_cols:  \n",
    "                cost_col = possible_cost_cols[0]  # choose the first matching column  \n",
    "                # We assume the incident name is in a column that includes 'name'  \n",
    "                possible_name_cols = [col for col in df_temp.columns if 'name' in col]  \n",
    "                if not possible_name_cols:  \n",
    "                    continue  \n",
    "                name_col = possible_name_cols[0]  \n",
    "                  \n",
    "                # Build lookup dictionary from normalized incident name to cleaned cost.  \n",
    "                for idx, row in df_temp.iterrows():  \n",
    "                    incident = row[name_col]  \n",
    "                    cost = row[cost_col]  \n",
    "                    # Clean cost: remove currency symbols and commas, then reformat if desired  \n",
    "                    if incident and cost:  \n",
    "                        # Only add if cost looks like a number (has digits)  \n",
    "                        if any(char.isdigit() for char in str(cost)):  \n",
    "                            cleaned_cost = str(cost).replace('$', '').replace(',', '').strip()  \n",
    "                            norm_incident = normalize_name(incident)  \n",
    "                            cost_lookup[norm_incident] = cleaned_cost  \n",
    "                print(\"Extracted\", len(cost_lookup), \"cost entries from page\", i+1)  \n",
    "                # Uncomment the following if you only expect one table to contain cost info:  \n",
    "                # break  \n",
    "        if cost_lookup:  \n",
    "            break  \n",
    "  \n",
    "if not cost_lookup:  \n",
    "    print(\"No cost data found in the PDF. Please adjust the extraction logic and column matching.\")  \n",
    "else:  \n",
    "    print(\"First few lookup entries:\")  \n",
    "    count = 0  \n",
    "    for key, value in cost_lookup.items():  \n",
    "        print(key, \"->\", value)  \n",
    "        count += 1  \n",
    "        if count >= 10:  \n",
    "            break  \n",
    "  \n",
    "# Load the CSV file and update cost values for 2022 records with missing costs (, NR, or empty)  \n",
    "print(\"\\nLoading CSV file...\")  \n",
    "df = pd.read_csv(csv_path)  \n",
    "  \n",
    "# For 2022, we found missing cost values (they are , check output above).  \n",
    "df_2022 = df[df['Year'] == 2022]  \n",
    "print(\"Found\", len(df_2022), \"records for 2022.\")  \n",
    "missing_cost = df_2022[(df_2022['Cost'].isna()) | (df_2022['Cost'] == 'NR') | (df_2022['Cost'] == '')]  \n",
    "print(\"Records with missing costs:\", len(missing_cost))  \n",
    "  \n",
    "update_count = 0  \n",
    "for idx, row in df_2022.iterrows():  \n",
    "    norm_name = normalize_name(row['Name'])  \n",
    "    if norm_name in cost_lookup:  \n",
    "        cost = cost_lookup[norm_name]  \n",
    "        # Update the main CSV DataFrame using index in original df.  \n",
    "        df.at[row.name, 'Cost'] = cost  \n",
    "        update_count += 1  \n",
    "        print(\"Updated:\", row['Name'], \"with cost:\", cost)  \n",
    "    else:  \n",
    "        print(\"No match for:\", row['Name'])  \n",
    "  \n",
    "print(\"\\nTotal incidents updated:\", update_count)  \n",
    "  \n",
    "# Save updated CSV  \n",
    "df.to_csv(output_csv, index=False)  \n",
    "print(\"Updated CSV saved to:\", output_csv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4ac487-138f-4d95-b337-4c2fd9c03776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cost data from the 2022 report...\n",
      "Found cost data in table 1 on page 9\n",
      "Name column: Name, Cost column: Estimated Cost\n",
      "Found cost data in table 1 on page 10\n",
      "Name column: Name, Cost column: Estimated Cost\n",
      "Extracted 45 incident costs from the PDF\n",
      "Sample of extracted costs:\n",
      "lime complex: $12726992\n",
      "hermits peak: $330100293\n",
      "black: $60190000\n",
      "paradise complex: $2782896\n",
      "tatlawiksuk: $75965\n",
      "nc hpai 2022: $484012\n",
      "bean complex: $11656365\n",
      "koktuli river: NR\n",
      "hog butte: $628717\n",
      "double creek: $39500000\n",
      "Loaded CSV with 470 records\n",
      "Found 40 records for 2022\n",
      "Updated: Lime Complex with cost: $12726992\n",
      "Updated: Hermits Peak with cost: $330100293\n",
      "Updated: Black with cost: $60190000\n",
      "Updated: Paradise Complex with cost: $2782896\n",
      "Updated: Tatlawiksuk with cost: $75965\n",
      "Updated: NC HPAI 2022 with cost: $484012\n",
      "Updated: Bean Complex with cost: $11656365\n",
      "Updated: Hog Butte with cost: $628717\n",
      "Updated: Double Creek with cost: $39500000\n",
      "Updated: East Fork with cost: $6150000\n",
      "Updated: Moose with cost: $98200000\n",
      "Updated: Cedar Creek with cost: $133750000\n",
      "Updated: Aghaluk Mountain with cost: $12726992\n",
      "Updated: Door Mountain with cost: $500\n",
      "Updated: Chitanana with cost: $1148818\n",
      "Updated: Dalton Highway\n",
      "Complex with cost: $6971097\n",
      "Updated: Pike Creek with cost: $76037\n",
      "Updated: Mosquito with cost: $181100000\n",
      "Updated: Clear with cost: $2363465\n",
      "Updated: Bitzshitini with cost: $216545\n",
      "Updated: Camp Creek with cost: $45000\n",
      "Updated: Donut with cost: $45000\n",
      "Updated: Middle Tanana\n",
      "Complex with cost: $3434457\n",
      "Updated: McKinney with cost: $96580000\n",
      "Updated: Cooks Peak with cost: $12500000\n",
      "Updated: Eastland Complex with cost: $7\n",
      "Updated: Schilling Creek with cost: $50000\n",
      "Updated: Fourth of July\n",
      "Creek with cost: $70912\n",
      "Updated: Radio Creek with cost: $75000\n",
      "Updated: Kiknik with cost: $3033\n",
      "Updated: Cerro Pelado with cost: $46800000\n",
      "Total incidents updated with actual costs: 31\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_2022_with_costs.csv\n",
      "Sample of updated 2022 records:\n",
      "                Name        Cost\n",
      "82      Lime Complex   $12726992\n",
      "83      Hermits Peak  $330100293\n",
      "84             Black   $60190000\n",
      "85  Paradise Complex    $2782896\n",
      "86       Tatlawiksuk      $75965\n",
      "87      NC HPAI 2022     $484012\n",
      "88      Bean Complex   $11656365\n",
      "89     Koktuli River         NaN\n",
      "90         Hog Butte     $628717\n",
      "91      Double Creek   $39500000\n"
     ]
    }
   ],
   "source": [
    "# Now let's extract the cost data from the 2022 annual report and update our CSV\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define file paths\n",
    "pdf_path = \"annual_report.2022.pdf\"\n",
    "csv_path = \"combined_incidents_clean_final_updated_manual_2022.csv\"\n",
    "output_csv = \"combined_incidents_clean_final_updated_2022_with_costs.csv\"\n",
    "\n",
    "# Function to normalize incident names (convert to string, strip, lower, and reduce whitespace)\n",
    "def normalize_name(name):\n",
    "    return re.sub(r'\\s+', ' ', str(name).strip().lower())\n",
    "\n",
    "# Function to clean cost values (remove $ and commas, convert to numeric)\n",
    "def clean_cost(cost):\n",
    "    if pd.isna(cost) or cost == '' or str(cost).strip() == '':\n",
    "        return 'NR'\n",
    "    cost_str = str(cost).strip()\n",
    "    if cost_str.upper() == 'NR':\n",
    "        return 'NR'\n",
    "    # Remove $ and commas, then add $ back\n",
    "    cost_str = cost_str.replace('$', '').replace(',', '')\n",
    "    return '$' + cost_str\n",
    "\n",
    "# Extract tables from the PDF\n",
    "cost_lookup = {}\n",
    "print(\"Extracting cost data from the 2022 report...\")\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        tables = page.extract_tables()\n",
    "        if not tables:\n",
    "            continue\n",
    "            \n",
    "        for table_idx, table in enumerate(tables):\n",
    "            if len(table) < 2:  # Skip tables with no data rows\n",
    "                continue\n",
    "                \n",
    "            # Convert to DataFrame\n",
    "            df_table = pd.DataFrame(table[1:], columns=table[0])\n",
    "            \n",
    "            # Check if this table has cost data\n",
    "            cost_col = None\n",
    "            name_col = None\n",
    "            \n",
    "            for col in df_table.columns:\n",
    "                if col and 'cost' in str(col).lower():\n",
    "                    cost_col = col\n",
    "                if col and 'name' in str(col).lower():\n",
    "                    name_col = col\n",
    "            \n",
    "            if cost_col and name_col:\n",
    "                print(f\"Found cost data in table {table_idx+1} on page {i+1}\")\n",
    "                print(f\"Name column: {name_col}, Cost column: {cost_col}\")\n",
    "                \n",
    "                # Build lookup dictionary\n",
    "                for _, row in df_table.iterrows():\n",
    "                    incident_name = row[name_col]\n",
    "                    cost_value = row[cost_col]\n",
    "                    \n",
    "                    if pd.notna(incident_name) and pd.notna(cost_value) and str(incident_name).strip() != '':\n",
    "                        norm_name = normalize_name(incident_name)\n",
    "                        clean_cost_val = clean_cost(cost_value)\n",
    "                        cost_lookup[norm_name] = clean_cost_val\n",
    "                        \n",
    "print(f\"Extracted {len(cost_lookup)} incident costs from the PDF\")\n",
    "print(\"\\\n",
    "Sample of extracted costs:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup.items():\n",
    "    print(f\"{name}: {cost}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"\\\n",
    "Loaded CSV with {len(df)} records\")\n",
    "\n",
    "# Update costs for 2022 records\n",
    "update_count = 0\n",
    "df_2022 = df[df['Year'] == 2022]\n",
    "print(f\"Found {len(df_2022)} records for 2022\")\n",
    "\n",
    "for idx, row in df_2022.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup:\n",
    "        cost = cost_lookup[norm_name]\n",
    "        if cost != 'NR':  # Only update if we have a real cost value\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count += 1\n",
    "            print(f\"Updated: {row['Name']} with cost: {cost}\")\n",
    "\n",
    "print(f\"\\\n",
    "Total incidents updated with actual costs: {update_count}\")\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Updated CSV saved to: {output_csv}\")\n",
    "\n",
    "# Show a sample of the updated 2022 records\n",
    "print(\"\\\n",
    "Sample of updated 2022 records:\")\n",
    "updated_df = pd.read_csv(output_csv)\n",
    "updated_2022 = updated_df[updated_df['Year'] == 2022]\n",
    "print(updated_2022[['Name', 'Cost']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9503fe5f-84ff-4ff4-a27b-5f64d5672408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cost data from the 2016 annual report...\n",
      "Found cost data in table 1 on page 11\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Extracted 19 incident cost entries from the 2016 report.\n",
      "\n",
      "Sample of extracted costs:\n",
      "anderson creek: $1750000\n",
      "pioneer: $90000000\n",
      "range 12: $1700000\n",
      "soberanes: $262500000\n",
      "hot pot: $3402259\n",
      "alatna complex: $1040018\n",
      "virginia mountains complex: $8000200\n",
      "dulbi flats: NR\n",
      "350 complex: $900000\n",
      "henrys creek: $4320000\n",
      "\n",
      "Loaded CSV with 470 records.\n",
      "Found 19 records for Year 2016.\n",
      "Updated: Anderson Creek with cost: $1750000\n",
      "Updated: Pioneer with cost: $90000000\n",
      "Updated: Range 12 with cost: $1700000\n",
      "Updated: Soberanes with cost: $262500000\n",
      "Updated: Hot Pot with cost: $3402259\n",
      "Updated: Alatna Complex with cost: $1040018\n",
      "Updated: Virginia Mountains Complex with cost: $8000200\n",
      "Updated: 350 Complex with cost: $900000\n",
      "Updated: Henrys Creek with cost: $4320000\n",
      "Updated: Erskine with cost: $23000000\n",
      "Updated: Chimney with cost: $78300000\n",
      "Updated: Cedar with cost: $14000000\n",
      "Updated: Maple with cost: $6267928\n",
      "Updated: Big Mud with cost: $59268\n",
      "Updated: North with cost: $4800000\n",
      "Updated: Rail with cost: $34900000\n",
      "Updated: Cottonwood CA with cost: $170000\n",
      "\n",
      "Total incidents updated with actual costs for 2016: 17\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_2016.csv\n",
      "\n",
      "Sample of updated 2016 records:\n",
      "                           Name        Cost\n",
      "131              Anderson Creek    $1750000\n",
      "132                     Pioneer   $90000000\n",
      "133                    Range 12    $1700000\n",
      "134                   Soberanes  $262500000\n",
      "135                     Hot Pot    $3402259\n",
      "136              Alatna Complex    $1040018\n",
      "137  Virginia Mountains Complex    $8000200\n",
      "138                 Dulbi Flats          NR\n",
      "139                 350 Complex     $900000\n",
      "140                Henrys Creek    $4320000\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber  \n",
    "import pandas as pd  \n",
    "import re  \n",
    "  \n",
    "# Define file paths for the 2016 report and input CSV (make sure to use the CSV that contains all the years)  \n",
    "pdf_path_2016 = \"annual_report_2016_508.pdf\"  \n",
    "csv_path = \"combined_incidents_clean_final_updated_manual_2022.csv\"  # assuming this file contains multiple years  \n",
    "output_csv = \"combined_incidents_clean_final_updated_2016.csv\"  \n",
    "  \n",
    "# Function to normalize incident names (convert to string, strip, lower, and reduce whitespace)  \n",
    "def normalize_name(name):  \n",
    "    return re.sub(r'\\s+', ' ', str(name).strip().lower())  \n",
    "  \n",
    "# Function to clean cost values (remove $ and commas then add $ back; if blank return 'NR')  \n",
    "def clean_cost(cost):  \n",
    "    if pd.isna(cost) or str(cost).strip() == '':  \n",
    "        return 'NR'  \n",
    "    cost_str = str(cost).strip()  \n",
    "    if cost_str.upper() == 'NR':  \n",
    "        return 'NR'  \n",
    "    # Remove $ and commas then add $ back  \n",
    "    clean_val = cost_str.replace('$', '').replace(',', '')  \n",
    "    return '$' + clean_val  \n",
    "  \n",
    "# Extract the cost table from the 2016 PDF  \n",
    "cost_lookup_2016 = {}  \n",
    "print(\"Extracting cost data from the 2016 annual report...\")  \n",
    "with pdfplumber.open(pdf_path_2016) as pdf:  \n",
    "    for i, page in enumerate(pdf.pages):  \n",
    "        tables = page.extract_tables()  \n",
    "        if not tables:  \n",
    "            continue       \n",
    "        for table_idx, table in enumerate(tables):  \n",
    "            if len(table) < 2:  \n",
    "                continue  \n",
    "            df_table = pd.DataFrame(table[1:], columns=table[0])  \n",
    "            # Identify columns for incident names and cost by searching header names (lowercased)  \n",
    "            cost_col = None  \n",
    "            name_col = None  \n",
    "            for col in df_table.columns:  \n",
    "                if col and 'cost' in str(col).lower():  \n",
    "                    cost_col = col  \n",
    "                if col and 'name' in str(col).lower():  \n",
    "                    name_col = col  \n",
    "            if cost_col and name_col:  \n",
    "                print(\"Found cost data in table \" + str(table_idx+1) + \" on page \" + str(i+1))  \n",
    "                print(\"Name column: \" + str(name_col) + \", Cost column: \" + str(cost_col))  \n",
    "                # Build lookup dictionary  \n",
    "                for _, row in df_table.iterrows():  \n",
    "                    incident_name = row[name_col]  \n",
    "                    cost_value = row[cost_col]  \n",
    "                    if pd.notna(incident_name) and pd.notna(cost_value) and str(incident_name).strip() != '':  \n",
    "                        norm_name = normalize_name(incident_name)  \n",
    "                        clean_cost_val = clean_cost(cost_value)  \n",
    "                        cost_lookup_2016[norm_name] = clean_cost_val  \n",
    "  \n",
    "print(\"Extracted\", len(cost_lookup_2016), \"incident cost entries from the 2016 report.\")  \n",
    "print(\"\\nSample of extracted costs:\")  \n",
    "count = 0  \n",
    "for name, cost in cost_lookup_2016.items():  \n",
    "    print(name + \": \" + cost)  \n",
    "    count += 1  \n",
    "    if count >= 10:  \n",
    "        break  \n",
    "  \n",
    "# Load the CSV file containing records for all years  \n",
    "df = pd.read_csv(csv_path)  \n",
    "print(\"\\nLoaded CSV with\", len(df), \"records.\")  \n",
    "  \n",
    "# Update cost values for records with Year == 2016 using the lookup dictionary  \n",
    "update_count = 0  \n",
    "df_2016 = df[df['Year'] == 2016]  \n",
    "print(\"Found\", len(df_2016), \"records for Year 2016.\")  \n",
    "for idx, row in df_2016.iterrows():  \n",
    "    norm_name = normalize_name(row['Name'])  \n",
    "    if norm_name in cost_lookup_2016:  \n",
    "        cost = cost_lookup_2016[norm_name]  \n",
    "        if cost != 'NR':  # Update only if valid cost exists  \n",
    "            df.at[idx, 'Cost'] = cost  \n",
    "            update_count += 1  \n",
    "            print(\"Updated:\", row['Name'], \"with cost:\", cost)  \n",
    "              \n",
    "print(\"\\nTotal incidents updated with actual costs for 2016:\", update_count)  \n",
    "  \n",
    "# After updating, ensure every record has a cost entry: if blank or missing, fill with 'NR'  \n",
    "def format_cost(cost):  \n",
    "    if pd.isna(cost) or str(cost).strip() == '':  \n",
    "        return 'NR'  \n",
    "    cost_str = str(cost).strip()  \n",
    "    if cost_str.upper() == 'NR':  \n",
    "        return 'NR'  \n",
    "    if not cost_str.startswith('$'):  \n",
    "        return '$' + cost_str  \n",
    "    return cost_str  \n",
    "  \n",
    "df['Cost'] = df['Cost'].apply(format_cost)  \n",
    "  \n",
    "# Save the updated CSV file  \n",
    "df.to_csv(output_csv, index=False)  \n",
    "print(\"Updated CSV saved to:\", output_csv)  \n",
    "  \n",
    "# Display sample 2016 records  \n",
    "df_updated = pd.read_csv(output_csv)  \n",
    "df_2016_updated = df_updated[df_updated['Year'] == 2016]  \n",
    "print(\"\\nSample of updated 2016 records:\")  \n",
    "print(df_2016_updated[['Name', 'Cost']].head(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1133b4-e354-4db2-b615-98d7acc8b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cost data from the 2016 annual report...\n",
      "Found cost data in table 1 on page 11\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Extracted 19 incident costs from the 2016 report.\n",
      "Sample of extracted costs from 2016:\n",
      "anderson creek: $1750000\n",
      "pioneer: $90000000\n",
      "range 12: $1700000\n",
      "soberanes: $262500000\n",
      "hot pot: $3402259\n",
      "alatna complex: $1040018\n",
      "virginia mountains complex: $8000200\n",
      "dulbi flats: NR\n",
      "350 complex: $900000\n",
      "henrys creek: $4320000\n",
      "Extracting cost data from the 2020 annual report...\n",
      "Found cost data in table 1 on page 10\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Found cost data in table 1 on page 11\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Extracted 50 incident costs from the 2020 report.\n",
      "Sample of extracted costs from 2020:\n",
      "august complex: $115511218\n",
      "scu lightning complex: $69412351\n",
      "shf elkhorn: NR\n",
      "creek: $193000000\n",
      "lnu lightning complex: $94646381\n",
      "north complex: $112711950\n",
      "pearl hill: $4241353\n",
      "cameron peak: $133300000\n",
      "lionshead: $65440000\n",
      "east troublesome: $15682681\n",
      "Loaded CSV with 470 records.\n",
      "Found 19 records for Year 2016.\n",
      "Updated 2016: Anderson Creek with cost: $1750000\n",
      "Updated 2016: Pioneer with cost: $90000000\n",
      "Updated 2016: Range 12 with cost: $1700000\n",
      "Updated 2016: Soberanes with cost: $262500000\n",
      "Updated 2016: Hot Pot with cost: $3402259\n",
      "Updated 2016: Alatna Complex with cost: $1040018\n",
      "Updated 2016: Virginia Mountains Complex with cost: $8000200\n",
      "Updated 2016: 350 Complex with cost: $900000\n",
      "Updated 2016: Henrys Creek with cost: $4320000\n",
      "Updated 2016: Erskine with cost: $23000000\n",
      "Updated 2016: Chimney with cost: $78300000\n",
      "Updated 2016: Cedar with cost: $14000000\n",
      "Updated 2016: Maple with cost: $6267928\n",
      "Updated 2016: Big Mud with cost: $59268\n",
      "Updated 2016: North with cost: $4800000\n",
      "Updated 2016: Rail with cost: $34900000\n",
      "Updated 2016: Cottonwood CA with cost: $170000\n",
      "Total incidents updated with actual costs for 2016: 17\n",
      "Found 34 records for Year 2020.\n",
      "Updated 2020: August Complex with cost: $115511218\n",
      "Updated 2020: SCU Lightning\n",
      "Complex with cost: $69412351\n",
      "Updated 2020: Creek with cost: $193000000\n",
      "Updated 2020: LNU Lightning\n",
      "Complex with cost: $94646381\n",
      "Updated 2020: North Complex with cost: $112711950\n",
      "Updated 2020: Pearl Hill with cost: $4241353\n",
      "Updated 2020: Cameron Peak with cost: $133300000\n",
      "Updated 2020: Lionshead with cost: $65440000\n",
      "Updated 2020: East Troublesome with cost: $15682681\n",
      "Updated 2020: Beachie Creek with cost: $29838526\n",
      "Updated 2020: Bush with cost: $11642634\n",
      "Updated 2020: Cold Springs with cost: $3917998\n",
      "Updated 2020: Mullen with cost: $42400000\n",
      "Updated 2020: Holiday Farm with cost: $29100932\n",
      "Updated 2020: Slater with cost: $55043900\n",
      "Updated 2020: August Complex\n",
      "West Zone with cost: $115300000\n",
      "Updated 2020: Pine Gulch with cost: $35000000\n",
      "Updated 2020: Riverside with cost: $20482000\n",
      "Updated 2020: Archie Creek with cost: $40000000\n",
      "Updated 2020: Whitney with cost: $3300000\n",
      "Updated 2020: Dolan with cost: $74000000\n",
      "Updated 2020: Bighorn with cost: $44463612\n",
      "Updated 2020: Bobcat with cost: $100000000\n",
      "Updated 2020: Woodhead with cost: $11230000\n",
      "Updated 2020: Badger with cost: $15800000\n",
      "Updated 2020: East Fork with cost: $23523403\n",
      "Updated 2020: CZU August\n",
      "Lightning with cost: $55900466\n",
      "Updated 2020: W-5 Cold Springs with cost: $10300000\n",
      "Updated 2020: July Complex with cost: $35000000\n",
      "Updated 2020: Caldwall with cost: $34500000\n",
      "Total incidents updated with actual costs for 2020: 30\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_2016_2020.csv\n",
      "Sample of updated 2016 records:\n",
      "                           Name        Cost\n",
      "131              Anderson Creek    $1750000\n",
      "132                     Pioneer   $90000000\n",
      "133                    Range 12    $1700000\n",
      "134                   Soberanes  $262500000\n",
      "135                     Hot Pot    $3402259\n",
      "136              Alatna Complex    $1040018\n",
      "137  Virginia Mountains Complex    $8000200\n",
      "138                 Dulbi Flats          NR\n",
      "139                 350 Complex     $900000\n",
      "140                Henrys Creek    $4320000\n",
      "Sample of updated 2020 records:\n",
      "                       Name        Cost\n",
      "201          August Complex  $115511218\n",
      "202  SCU Lightning\\nComplex   $69412351\n",
      "203             SHF Elkhorn          NR\n",
      "204                   Creek  $193000000\n",
      "205  LNU Lightning\\nComplex   $94646381\n",
      "206           North Complex  $112711950\n",
      "207              Pearl Hill    $4241353\n",
      "208            Cameron Peak  $133300000\n",
      "209               Lionshead   $65440000\n",
      "210        East Troublesome   $15682681\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define file paths for the 2016 report and input CSV\n",
    "pdf_path_2016 = \"annual_report_2016_508.pdf\"\n",
    "csv_path = \"combined_incidents_clean_final_updated_2022_with_costs.csv\"  # Use the most recent updated CSV\n",
    "output_csv = \"combined_incidents_clean_final_updated_2016_2020.csv\"\n",
    "\n",
    "# Function to normalize incident names (convert to string, strip, lower, and reduce whitespace)\n",
    "def normalize_name(name):\n",
    "    return re.sub(r'\\s+', ' ', str(name).strip().lower())\n",
    "\n",
    "# Function to clean cost values (remove $ and commas then add $ back; if blank return 'NR')\n",
    "def clean_cost(cost):\n",
    "    if pd.isna(cost) or str(cost).strip() == '':\n",
    "        return 'NR'\n",
    "    cost_str = str(cost).strip()\n",
    "    if cost_str.upper() == 'NR':\n",
    "        return 'NR'\n",
    "    # Remove $ and commas then add $ back\n",
    "    clean_val = cost_str.replace('$', '').replace(',', '')\n",
    "    return '$' + clean_val\n",
    "\n",
    "# Extract the cost table from the 2016 PDF\n",
    "cost_lookup_2016 = {}\n",
    "print(\"Extracting cost data from the 2016 annual report...\")\n",
    "with pdfplumber.open(pdf_path_2016) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        tables = page.extract_tables()\n",
    "        if not tables:\n",
    "            continue     \n",
    "        for table_idx, table in enumerate(tables):\n",
    "            if len(table) < 2:\n",
    "                continue\n",
    "            df_table = pd.DataFrame(table[1:], columns=table[0])\n",
    "            \n",
    "            # Identify columns for incident names and cost by searching header names\n",
    "            cost_col = None\n",
    "            name_col = None\n",
    "            \n",
    "            for col in df_table.columns:\n",
    "                if col and 'cost' in str(col).lower():\n",
    "                    cost_col = col\n",
    "                if col and 'name' in str(col).lower():\n",
    "                    name_col = col\n",
    "            \n",
    "            if cost_col and name_col:\n",
    "                print(f\"Found cost data in table {table_idx+1} on page {i+1}\")\n",
    "                print(f\"Name column: {name_col}, Cost column: {cost_col}\")\n",
    "                \n",
    "                # Build lookup dictionary\n",
    "                for _, row in df_table.iterrows():\n",
    "                    incident_name = row[name_col]\n",
    "                    cost_value = row[cost_col]\n",
    "                    \n",
    "                    if pd.notna(incident_name) and pd.notna(cost_value) and str(incident_name).strip() != '':\n",
    "                        norm_name = normalize_name(incident_name)\n",
    "                        clean_cost_val = clean_cost(cost_value)\n",
    "                        cost_lookup_2016[norm_name] = clean_cost_val\n",
    "\n",
    "print(f\"Extracted {len(cost_lookup_2016)} incident costs from the 2016 report.\")\n",
    "print(\"\\\n",
    "Sample of extracted costs from 2016:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup_2016.items():\n",
    "    print(f\"{name}: {cost}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Now extract from the 2020 PDF\n",
    "pdf_path_2020 = \"annual_report_2020.pdf\"\n",
    "cost_lookup_2020 = {}\n",
    "\n",
    "print(\"\\\n",
    "Extracting cost data from the 2020 annual report...\")\n",
    "with pdfplumber.open(pdf_path_2020) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        tables = page.extract_tables()\n",
    "        if not tables:\n",
    "            continue     \n",
    "        for table_idx, table in enumerate(tables):\n",
    "            if len(table) < 2:\n",
    "                continue\n",
    "            df_table = pd.DataFrame(table[1:], columns=table[0])\n",
    "            \n",
    "            # Identify columns for incident names and cost by searching header names\n",
    "            cost_col = None\n",
    "            name_col = None\n",
    "            \n",
    "            for col in df_table.columns:\n",
    "                if col and 'cost' in str(col).lower():\n",
    "                    cost_col = col\n",
    "                if col and 'name' in str(col).lower():\n",
    "                    name_col = col\n",
    "            \n",
    "            if cost_col and name_col:\n",
    "                print(f\"Found cost data in table {table_idx+1} on page {i+1}\")\n",
    "                print(f\"Name column: {name_col}, Cost column: {cost_col}\")\n",
    "                \n",
    "                # Build lookup dictionary\n",
    "                for _, row in df_table.iterrows():\n",
    "                    incident_name = row[name_col]\n",
    "                    cost_value = row[cost_col]\n",
    "                    \n",
    "                    if pd.notna(incident_name) and pd.notna(cost_value) and str(incident_name).strip() != '':\n",
    "                        norm_name = normalize_name(incident_name)\n",
    "                        clean_cost_val = clean_cost(cost_value)\n",
    "                        cost_lookup_2020[norm_name] = clean_cost_val\n",
    "\n",
    "print(f\"Extracted {len(cost_lookup_2020)} incident costs from the 2020 report.\")\n",
    "print(\"\\\n",
    "Sample of extracted costs from 2020:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup_2020.items():\n",
    "    print(f\"{name}: {cost}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Load the CSV file containing records for all years\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"\\\n",
    "Loaded CSV with {len(df)} records.\")\n",
    "\n",
    "# Update cost values for records with Year == 2016 using the lookup dictionary\n",
    "update_count_2016 = 0\n",
    "df_2016 = df[df['Year'] == 2016]\n",
    "print(f\"Found {len(df_2016)} records for Year 2016.\")\n",
    "for idx, row in df_2016.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup_2016:\n",
    "        cost = cost_lookup_2016[norm_name]\n",
    "        if cost != 'NR':  # Update only if valid cost exists\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count_2016 += 1\n",
    "            print(f\"Updated 2016: {row['Name']} with cost: {cost}\")\n",
    "            \n",
    "print(f\"\\\n",
    "Total incidents updated with actual costs for 2016: {update_count_2016}\")\n",
    "\n",
    "# Update cost values for records with Year == 2020 using the lookup dictionary\n",
    "update_count_2020 = 0\n",
    "df_2020 = df[df['Year'] == 2020]\n",
    "print(f\"Found {len(df_2020)} records for Year 2020.\")\n",
    "for idx, row in df_2020.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup_2020:\n",
    "        cost = cost_lookup_2020[norm_name]\n",
    "        if cost != 'NR':  # Update only if valid cost exists\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count_2020 += 1\n",
    "            print(f\"Updated 2020: {row['Name']} with cost: {cost}\")\n",
    "            \n",
    "print(f\"\\\n",
    "Total incidents updated with actual costs for 2020: {update_count_2020}\")\n",
    "\n",
    "# After updating, ensure every record has a cost entry: if blank or missing, fill with 'NR'\n",
    "def format_cost(cost):\n",
    "    if pd.isna(cost) or str(cost).strip() == '':\n",
    "        return 'NR'\n",
    "    cost_str = str(cost).strip()\n",
    "    if cost_str.upper() == 'NR':\n",
    "        return 'NR'\n",
    "    if not cost_str.startswith('$'):\n",
    "        return '$' + cost_str\n",
    "    return cost_str\n",
    "\n",
    "df['Cost'] = df['Cost'].apply(format_cost)\n",
    "\n",
    "# Save the updated CSV file\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Updated CSV saved to: {output_csv}\")\n",
    "\n",
    "# Display sample of updated records\n",
    "df_updated = pd.read_csv(output_csv)\n",
    "print(\"\\\n",
    "Sample of updated 2016 records:\")\n",
    "df_2016_updated = df_updated[df_updated['Year'] == 2016]\n",
    "print(df_2016_updated[['Name', 'Cost']].head(10))\n",
    "\n",
    "print(\"\\\n",
    "Sample of updated 2020 records:\")\n",
    "df_2020_updated = df_updated[df_updated['Year'] == 2020]\n",
    "print(df_2020_updated[['Name', 'Cost']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15fe2bf0-9de3-4383-9b01-ded8b3cc395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cost data from the 2023 annual report...\n",
      "Extracted 0 incident costs from the 2023 report.\n",
      "Sample of extracted costs from 2023:\n",
      "Extracting cost data from the 2018 annual report...\n",
      "Found cost data in table 1 on page 9 in annual_report_ 2018_508.pdf\n",
      "Found cost data in table 1 on page 10 in annual_report_ 2018_508.pdf\n",
      "Extracted 47 incident costs from the 2018 report.\n",
      "Sample of extracted costs from 2018:\n",
      "mendocino complex: $220000000\n",
      "martin: $10000000\n",
      "rhea: $3800000\n",
      "south sugarloaf: $20000000\n",
      "carr: $162289294\n",
      "klondike: $104500000\n",
      "camp: $120000000\n",
      "goose creek: $7850000\n",
      "spring creek: $35000000\n",
      "pole creek: $29100000\n",
      "Loading CSV from combined_incidents_clean_final_updated_2016_2020.csv\n",
      "Loaded CSV with 470 records.\n",
      "Found 10 records for Year 2023.\n",
      "Found 24 records for Year 2018.\n",
      "Updated 2018: Mendocino Complex with cost: $220000000\n",
      "Updated 2018: Martin with cost: $10000000\n",
      "Updated 2018: Rhea with cost: $3800000\n",
      "Updated 2018: South Sugarloaf with cost: $20000000\n",
      "Updated 2018: Carr with cost: $162289294\n",
      "Updated 2018: Klondike with cost: $104500000\n",
      "Updated 2018: Camp with cost: $120000000\n",
      "Updated 2018: Goose Creek with cost: $7850000\n",
      "Updated 2018: Spring Creek with cost: $35000000\n",
      "Updated 2018: Pole Creek with cost: $29100000\n",
      "Updated 2018: Boxcar with cost: $3500000\n",
      "Updated 2018: Grassy Ridge with cost: $3000000\n",
      "Updated 2018: Woolsey with cost: $56943320\n",
      "Updated 2018: Ferguson with cost: $150000000\n",
      "Updated 2018: County with cost: $47000000\n",
      "Updated 2018: Avian Complex with cost: $10781364\n",
      "Updated 2018: Substation with cost: $4200000\n",
      "Updated 2018: Grass Valley with cost: $875000\n",
      "Updated 2018: Boylston with cost: $995000\n",
      "Updated 2018: Dollar Ridge with cost: $22000000\n",
      "Updated 2018: Zitziana River with cost: $2440527\n",
      "Updated 2018: Sharps with cost: $9500000\n",
      "Updated 2018: Delta with cost: $64429020\n",
      "Total incidents updated with actual costs for 2023: 0\n",
      "Total incidents updated with actual costs for 2018: 23\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_all_years.csv\n",
      "Sample of updated 2023 records:\n",
      "                            Name Cost\n",
      "259          Smith River Complex   NR\n",
      "260                         York   NR\n",
      "261                  Newell Road   NR\n",
      "262                         Pass   NR\n",
      "263             Anderson Complex   NR\n",
      "264                        Delta   NR\n",
      "265          Clear Creek Complex   NR\n",
      "266  2023 SRF Lightning\\nComplex   NR\n",
      "267               Pogo Mine Road   NR\n",
      "268                     Cooksley   NR\n",
      "Sample of updated 2018 records:\n",
      "                  Name        Cost\n",
      "235  Mendocino Complex  $220000000\n",
      "236             Martin   $10000000\n",
      "237               Rhea    $3800000\n",
      "238    South Sugarloaf   $20000000\n",
      "239               Carr  $162289294\n",
      "240           Klondike  $104500000\n",
      "241               Camp  $120000000\n",
      "242        Goose Creek    $7850000\n",
      "243       Spring Creek   $35000000\n",
      "244         Pole Creek   $29100000\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define file paths for the two new reports\n",
    "pdf_path_2023 = \"annual_report_2023_508_0.pdf\"\n",
    "pdf_path_2018 = \"annual_report_ 2018_508.pdf\"  # Note the space in filename\n",
    "\n",
    "# Use the latest CSV that we have been updating\n",
    "csv_path = \"combined_incidents_clean_final_updated_2016_2020.csv\"\n",
    "output_csv = \"combined_incidents_clean_final_updated_all_years.csv\"\n",
    "\n",
    "# Define helper functions\n",
    "\n",
    "def normalize_name(name):\n",
    "    return re.sub(r'\\s+', ' ', str(name).strip().lower())\n",
    "\n",
    "\n",
    "def clean_cost(cost):\n",
    "    if pd.isna(cost) or str(cost).strip() == '':\n",
    "        return 'NR'\n",
    "    cost_str = str(cost).strip()\n",
    "    if cost_str.upper() == 'NR':\n",
    "        return 'NR'\n",
    "    clean_val = cost_str.replace('$', '').replace(',', '')\n",
    "    return '$' + clean_val\n",
    "\n",
    "# Function to extract cost lookup from a given PDF\n",
    "\n",
    "def extract_cost_lookup(pdf_path):\n",
    "    cost_lookup = {}\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()\n",
    "                if not tables:\n",
    "                    continue\n",
    "                for table_idx, table in enumerate(tables):\n",
    "                    # skip if table has less than 2 rows\n",
    "                    if len(table) < 2:\n",
    "                        continue\n",
    "                    df_table = pd.DataFrame(table[1:], columns=table[0])\n",
    "                    cost_col = None\n",
    "                    name_col = None\n",
    "                    for col in df_table.columns:\n",
    "                        if col and 'cost' in str(col).lower():\n",
    "                            cost_col = col\n",
    "                        if col and 'name' in str(col).lower():\n",
    "                            name_col = col\n",
    "                    if cost_col and name_col:\n",
    "                        # Debug print for which table found\n",
    "                        print(\"Found cost data in table \" + str(table_idx+1) + \" on page \" + str(i+1) + \" in \" + pdf_path)\n",
    "                        for _, row in df_table.iterrows():\n",
    "                            incident_name = row[name_col]\n",
    "                            cost_value = row[cost_col]\n",
    "                            if pd.notna(incident_name) and pd.notna(cost_value) and str(incident_name).strip() != \"\":\n",
    "                                norm_name = normalize_name(incident_name)\n",
    "                                clean_cost_val = clean_cost(cost_value)\n",
    "                                cost_lookup[norm_name] = clean_cost_val\n",
    "    except Exception as e:\n",
    "        print(\"Error processing \" + pdf_path + \": \" + str(e))\n",
    "    return cost_lookup\n",
    "\n",
    "# Extract cost data from the 2023 report\n",
    "print(\"\\\n",
    "Extracting cost data from the 2023 annual report...\")\n",
    "cost_lookup_2023 = extract_cost_lookup(pdf_path_2023)\n",
    "print(\"Extracted \" + str(len(cost_lookup_2023)) + \" incident costs from the 2023 report.\")\n",
    "print(\"\\\n",
    "Sample of extracted costs from 2023:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup_2023.items():\n",
    "    print(name + \": \" + cost)\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Extract cost data from the 2018 report\n",
    "print(\"\\\n",
    "Extracting cost data from the 2018 annual report...\")\n",
    "cost_lookup_2018 = extract_cost_lookup(pdf_path_2018)\n",
    "print(\"Extracted \" + str(len(cost_lookup_2018)) + \" incident costs from the 2018 report.\")\n",
    "print(\"\\\n",
    "Sample of extracted costs from 2018:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup_2018.items():\n",
    "    print(name + \": \" + cost)\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Load CSV file containing records for all years\n",
    "print(\"\\\n",
    "Loading CSV from \" + csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Loaded CSV with \" + str(len(df)) + \" records.\")\n",
    "\n",
    "# Update cost values based on lookup for 2023 and 2018\n",
    "update_count_2023 = 0\n",
    "update_count_2018 = 0\n",
    "\n",
    "# Update records for year 2023\n",
    "df_2023 = df[df['Year'] == 2023]\n",
    "print(\"Found \" + str(len(df_2023)) + \" records for Year 2023.\")\n",
    "for idx, row in df_2023.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup_2023:\n",
    "        cost = cost_lookup_2023[norm_name]\n",
    "        if cost != 'NR':\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count_2023 += 1\n",
    "            print(\"Updated 2023: \" + row['Name'] + \" with cost: \" + cost)\n",
    "\n",
    "# Update records for year 2018\n",
    "df_2018 = df[df['Year'] == 2018]\n",
    "print(\"Found \" + str(len(df_2018)) + \" records for Year 2018.\")\n",
    "for idx, row in df_2018.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup_2018:\n",
    "        cost = cost_lookup_2018[norm_name]\n",
    "        if cost != 'NR':\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count_2018 += 1\n",
    "            print(\"Updated 2018: \" + row['Name'] + \" with cost: \" + cost)\n",
    "\n",
    "print(\"\\\n",
    "Total incidents updated with actual costs for 2023: \" + str(update_count_2023))\n",
    "print(\"Total incidents updated with actual costs for 2018: \" + str(update_count_2018))\n",
    "\n",
    "# Ensure every record has a cost entry: fill blank/missing with 'NR'\n",
    "def format_cost(cost):\n",
    "    if pd.isna(cost) or str(cost).strip() == '' :\n",
    "        return 'NR'\n",
    "    cost_str = str(cost).strip()\n",
    "    if cost_str.upper() == 'NR':\n",
    "        return 'NR'\n",
    "    if not cost_str.startswith('$'):\n",
    "        return '$' + cost_str\n",
    "    return cost_str\n",
    "\n",
    "df['Cost'] = df['Cost'].apply(format_cost)\n",
    "\n",
    "# Save the updated CSV\n",
    "\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(\"\\\n",
    "Updated CSV saved to: \" + output_csv)\n",
    "\n",
    "# Display sample of updated 2023 and 2018 records\n",
    "print(\"\\\n",
    "Sample of updated 2023 records:\")\n",
    "df_updated = pd.read_csv(output_csv)\n",
    "df_2023_updated = df_updated[df_updated['Year'] == 2023]\n",
    "print(df_2023_updated[['Name', 'Cost']].head(10))\n",
    "\n",
    "print(\"\\\n",
    "Sample of updated 2018 records:\")\n",
    "df_2018_updated = df_updated[df_updated['Year'] == 2018]\n",
    "print(df_2018_updated[['Name', 'Cost']].head(10))\n",
    "\n",
    "print(\"\\\n",
    "Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24a7f119-1793-4240-a3f9-f5fa60e8bf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cost data from the 2019 annual report...\n",
      "Found cost data in table 1 on page 10 in annual_report_2019_508.pdf\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Found cost data in table 1 on page 11 in annual_report_2019_508.pdf\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Extracted 27 incident costs from the 2019 report.\n",
      "Sample of extracted costs from 2019:\n",
      "old grouch top: $61000\n",
      "frozen calf: $4332806\n",
      "hess creek: $3005369\n",
      "swan lake: $48101094\n",
      "bearnose hill: $2108024\n",
      "woodbury: $20000000\n",
      "sheep: $710000\n",
      "black river: $30000\n",
      "north river: $40000\n",
      "tractor trail 2: $461188\n",
      "Extracting cost data from the 2021 annual report...\n",
      "Found cost data in table 1 on page 11 in annual_report_2021.pdf\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Found cost data in table 1 on page 12 in annual_report_2021.pdf\n",
      "Name column: Name, Cost column: Estimated\n",
      "Cost\n",
      "Extracted 37 incident costs from the 2021 report.\n",
      "Sample of extracted costs from 2021:\n",
      "dixie: $30000000\n",
      "bootleg: $100900000\n",
      "monument: $163739291\n",
      "caldor: $271147512\n",
      "river complex: $95340595\n",
      "telegraph: $36400000\n",
      "richard spring: $6500000\n",
      "antelope: $74950300\n",
      "mcfarland: $43960000\n",
      "four county: NR\n",
      "Extracting cost data from the 2024 annual report...\n",
      "Extracted 0 incident costs from the 2024 report.\n",
      "Sample of extracted costs from 2024:\n",
      "Loading CSV from combined_incidents_clean_final_updated_all_years.csv\n",
      "Loaded CSV with 470 records.\n",
      "Found 26 records for Year 2019.\n",
      "Updated 2019: Old Grouch Top with cost: $61000\n",
      "Updated 2019: Frozen Calf with cost: $4332806\n",
      "Updated 2019: Hess Creek with cost: $3005369\n",
      "Updated 2019: Swan Lake with cost: $48101094\n",
      "Updated 2019: Bearnose Hill with cost: $2108024\n",
      "Updated 2019: Woodbury with cost: $20000000\n",
      "Updated 2019: Sheep with cost: $710000\n",
      "Updated 2019: Black River with cost: $30000\n",
      "Updated 2019: North River with cost: $40000\n",
      "Updated 2019: Tractor Trail 2 with cost: $461188\n",
      "Updated 2019: Hurst Creek with cost: $231175\n",
      "Updated 2019: Kincade with cost: $77144684\n",
      "Updated 2019: Pothole with cost: $600000\n",
      "Updated 2019: Hadweenzic River with cost: $5004308\n",
      "Updated 2019: Walker with cost: $35600000\n",
      "Updated 2019: Wilderness with cost: $60000\n",
      "Updated 2019: Foraker with cost: $203477\n",
      "Updated 2019: Grouse Creek with cost: $2000000\n",
      "Updated 2019: Page Mountain with cost: $394900\n",
      "Updated 2019: Williams Flatt with cost: $19432000\n",
      "Updated 2019: Bergman Creek with cost: $1233004\n",
      "Updated 2019: Cold Creek with cost: $900000\n",
      "Found 24 records for Year 2021.\n",
      "Updated 2021: Dixie with cost: $30000000\n",
      "Updated 2021: Bootleg with cost: $100900000\n",
      "Updated 2021: Monument with cost: $163739291\n",
      "Updated 2021: Caldor with cost: $271147512\n",
      "Updated 2021: River Complex with cost: $95340595\n",
      "Updated 2021: Telegraph with cost: $36400000\n",
      "Updated 2021: Richard Spring with cost: $6500000\n",
      "Updated 2021: Antelope with cost: $74950300\n",
      "Updated 2021: McFarland with cost: $43960000\n",
      "Updated 2021: Snake River\n",
      "Complex with cost: $9498972\n",
      "Updated 2021: Schneider Springs with cost: $53955225\n",
      "Updated 2021: Beckwourth\n",
      "Complex with cost: $542539952\n",
      "Updated 2021: Windy with cost: $78421741\n",
      "Updated 2021: McCash with cost: $53350000\n",
      "Updated 2021: Cougar Peak with cost: $26000000\n",
      "Updated 2021: Johnson with cost: $7500000\n",
      "Updated 2021: Boundary with cost: $12002000\n",
      "Updated 2021: KNP Complex with cost: $170000000\n",
      "Updated 2021: Rafael with cost: $11000000\n",
      "Updated 2021: Mescal with cost: $12075000\n",
      "Updated 2021: Cub Creek 2 with cost: $27788529\n",
      "Updated 2021: Tamarack with cost: $37000000\n",
      "Found 37 records for Year 2024.\n",
      "Total incidents updated with actual costs for 2019: 22\n",
      "Total incidents updated with actual costs for 2021: 22\n",
      "Total incidents updated with actual costs for 2024: 0\n",
      "Updated CSV saved to: combined_incidents_clean_final_updated_2019_2021_2024.csv\n",
      "Sample of updated 2019 records:\n",
      "                Name       Cost\n",
      "376   Old Grouch Top     $61000\n",
      "377      Frozen Calf   $4332806\n",
      "378       Hess Creek   $3005369\n",
      "379        Swan Lake  $48101094\n",
      "380    Bearnose Hill   $2108024\n",
      "381         Woodbury  $20000000\n",
      "382            Sheep    $710000\n",
      "383      Black River     $30000\n",
      "384      North River     $40000\n",
      "385  Tractor Trail 2    $461188\n",
      "Sample of updated 2021 records:\n",
      "               Name        Cost\n",
      "315           Dixie   $30000000\n",
      "316         Bootleg  $100900000\n",
      "317        Monument  $163739291\n",
      "318          Caldor  $271147512\n",
      "319   River Complex   $95340595\n",
      "320       Telegraph   $36400000\n",
      "321  Richard Spring    $6500000\n",
      "322        Antelope   $74950300\n",
      "323       McFarland   $43960000\n",
      "324     Four County          NR\n",
      "Sample of updated 2024 records:\n",
      "                   Name Cost\n",
      "339         Betty's Way   NR\n",
      "340    Smokehouse Creek   NR\n",
      "341             Catesby   NR\n",
      "342            McDonald   NR\n",
      "343            Midnight   NR\n",
      "344  Grapefruit Complex   NR\n",
      "345               Falls   NR\n",
      "346          Cow Valley   NR\n",
      "347           Lone Rock   NR\n",
      "348            Boneyard   NR\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define file paths for the three new reports\n",
    "pdf_path_2019 = \"annual_report_2019_508.pdf\"\n",
    "pdf_path_2021 = \"annual_report_2021.pdf\"\n",
    "pdf_path_2024 = \"annual_report_2024.pdf\"\n",
    "\n",
    "# Use the latest CSV that we have been updating\n",
    "csv_path = \"combined_incidents_clean_final_updated_all_years.csv\"\n",
    "output_csv = \"combined_incidents_clean_final_updated_2019_2021_2024.csv\"\n",
    "\n",
    "# Define helper functions\n",
    "def normalize_name(name):\n",
    "    return re.sub(r'\\s+', ' ', str(name).strip().lower())\n",
    "\n",
    "def clean_cost(cost):\n",
    "    if pd.isna(cost) or str(cost).strip() == '':\n",
    "        return 'NR'\n",
    "    cost_str = str(cost).strip()\n",
    "    if cost_str.upper() == 'NR':\n",
    "        return 'NR'\n",
    "    clean_val = cost_str.replace('$', '').replace(',', '')\n",
    "    return '$' + clean_val\n",
    "\n",
    "# Function to extract cost lookup from a given PDF\n",
    "def extract_cost_lookup(pdf_path):\n",
    "    cost_lookup = {}\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                tables = page.extract_tables()\n",
    "                if not tables:\n",
    "                    continue\n",
    "                for table_idx, table in enumerate(tables):\n",
    "                    # skip if table has less than 2 rows\n",
    "                    if len(table) < 2:\n",
    "                        continue\n",
    "                    df_table = pd.DataFrame(table[1:], columns=table[0])\n",
    "                    # Clean column names\n",
    "                    df_table.columns = [str(c).strip() for c in df_table.columns]\n",
    "                    \n",
    "                    cost_col = None\n",
    "                    name_col = None\n",
    "                    \n",
    "                    # Look for columns with cost and name\n",
    "                    for col in df_table.columns:\n",
    "                        if col and 'cost' in str(col).lower():\n",
    "                            cost_col = col\n",
    "                        if col and 'name' in str(col).lower():\n",
    "                            name_col = col\n",
    "                    \n",
    "                    if cost_col and name_col:\n",
    "                        # Debug print for which table found\n",
    "                        print(f\"Found cost data in table {table_idx+1} on page {i+1} in {pdf_path}\")\n",
    "                        print(f\"Name column: {name_col}, Cost column: {cost_col}\")\n",
    "                        \n",
    "                        # Build lookup dictionary\n",
    "                        for _, row in df_table.iterrows():\n",
    "                            incident_name = row[name_col]\n",
    "                            cost_value = row[cost_col]\n",
    "                            \n",
    "                            if pd.notna(incident_name) and pd.notna(cost_value) and str(incident_name).strip() != \"\":\n",
    "                                norm_name = normalize_name(incident_name)\n",
    "                                clean_cost_val = clean_cost(cost_value)\n",
    "                                cost_lookup[norm_name] = clean_cost_val\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "    return cost_lookup\n",
    "\n",
    "# Extract cost data from the 2019 report\n",
    "print(\"\\\n",
    "Extracting cost data from the 2019 annual report...\")\n",
    "cost_lookup_2019 = extract_cost_lookup(pdf_path_2019)\n",
    "print(f\"Extracted {len(cost_lookup_2019)} incident costs from the 2019 report.\")\n",
    "print(\"\\\n",
    "Sample of extracted costs from 2019:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup_2019.items():\n",
    "    print(f\"{name}: {cost}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Extract cost data from the 2021 report\n",
    "print(\"\\\n",
    "Extracting cost data from the 2021 annual report...\")\n",
    "cost_lookup_2021 = extract_cost_lookup(pdf_path_2021)\n",
    "print(f\"Extracted {len(cost_lookup_2021)} incident costs from the 2021 report.\")\n",
    "print(\"\\\n",
    "Sample of extracted costs from 2021:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup_2021.items():\n",
    "    print(f\"{name}: {cost}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Extract cost data from the 2024 report\n",
    "print(\"\\\n",
    "Extracting cost data from the 2024 annual report...\")\n",
    "cost_lookup_2024 = extract_cost_lookup(pdf_path_2024)\n",
    "print(f\"Extracted {len(cost_lookup_2024)} incident costs from the 2024 report.\")\n",
    "print(\"\\\n",
    "Sample of extracted costs from 2024:\")\n",
    "count = 0\n",
    "for name, cost in cost_lookup_2024.items():\n",
    "    print(f\"{name}: {cost}\")\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break\n",
    "\n",
    "# Load CSV file containing records for all years\n",
    "print(f\"\\\n",
    "Loading CSV from {csv_path}\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded CSV with {len(df)} records.\")\n",
    "\n",
    "# Update cost values based on lookup for 2019, 2021, and 2024\n",
    "update_count_2019 = 0\n",
    "update_count_2021 = 0\n",
    "update_count_2024 = 0\n",
    "\n",
    "# Update records for year 2019\n",
    "df_2019 = df[df['Year'] == 2019]\n",
    "print(f\"Found {len(df_2019)} records for Year 2019.\")\n",
    "for idx, row in df_2019.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup_2019:\n",
    "        cost = cost_lookup_2019[norm_name]\n",
    "        if cost != 'NR':\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count_2019 += 1\n",
    "            print(f\"Updated 2019: {row['Name']} with cost: {cost}\")\n",
    "\n",
    "# Update records for year 2021\n",
    "df_2021 = df[df['Year'] == 2021]\n",
    "print(f\"Found {len(df_2021)} records for Year 2021.\")\n",
    "for idx, row in df_2021.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup_2021:\n",
    "        cost = cost_lookup_2021[norm_name]\n",
    "        if cost != 'NR':\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count_2021 += 1\n",
    "            print(f\"Updated 2021: {row['Name']} with cost: {cost}\")\n",
    "\n",
    "# Update records for year 2024\n",
    "df_2024 = df[df['Year'] == 2024]\n",
    "print(f\"Found {len(df_2024)} records for Year 2024.\")\n",
    "for idx, row in df_2024.iterrows():\n",
    "    norm_name = normalize_name(row['Name'])\n",
    "    if norm_name in cost_lookup_2024:\n",
    "        cost = cost_lookup_2024[norm_name]\n",
    "        if cost != 'NR':\n",
    "            df.at[idx, 'Cost'] = cost\n",
    "            update_count_2024 += 1\n",
    "            print(f\"Updated 2024: {row['Name']} with cost: {cost}\")\n",
    "\n",
    "print(f\"\\\n",
    "Total incidents updated with actual costs for 2019: {update_count_2019}\")\n",
    "print(f\"Total incidents updated with actual costs for 2021: {update_count_2021}\")\n",
    "print(f\"Total incidents updated with actual costs for 2024: {update_count_2024}\")\n",
    "\n",
    "# Ensure every record has a cost entry: fill blank/missing with 'NR'\n",
    "def format_cost(cost):\n",
    "    if pd.isna(cost) or str(cost).strip() == '':\n",
    "        return 'NR'\n",
    "    cost_str = str(cost).strip()\n",
    "    if cost_str.upper() == 'NR':\n",
    "        return 'NR'\n",
    "    if not cost_str.startswith('$'):\n",
    "        return '$' + cost_str\n",
    "    return cost_str\n",
    "\n",
    "df['Cost'] = df['Cost'].apply(format_cost)\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"\\\n",
    "Updated CSV saved to: {output_csv}\")\n",
    "\n",
    "# Display sample of updated records\n",
    "df_updated = pd.read_csv(output_csv)\n",
    "\n",
    "print(\"\\\n",
    "Sample of updated 2019 records:\")\n",
    "df_2019_updated = df_updated[df_updated['Year'] == 2019]\n",
    "if len(df_2019_updated) > 0:\n",
    "    print(df_2019_updated[['Name', 'Cost']].head(10))\n",
    "else:\n",
    "    print(\"No records found for 2019\")\n",
    "\n",
    "print(\"\\\n",
    "Sample of updated 2021 records:\")\n",
    "df_2021_updated = df_updated[df_updated['Year'] == 2021]\n",
    "if len(df_2021_updated) > 0:\n",
    "    print(df_2021_updated[['Name', 'Cost']].head(10))\n",
    "else:\n",
    "    print(\"No records found for 2021\")\n",
    "\n",
    "print(\"\\\n",
    "Sample of updated 2024 records:\")\n",
    "df_2024_updated = df_updated[df_updated['Year'] == 2024]\n",
    "if len(df_2024_updated) > 0:\n",
    "    print(df_2024_updated[['Name', 'Cost']].head(10))\n",
    "else:\n",
    "    print(\"No records found for 2024\")\n",
    "\n",
    "print(\"\\\n",
    "Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b35c6-8e44-4108-8c27-c8c00e54707e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
