{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77cc153-b62e-4e21-b900-84e801342282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 structured incident files in ./structurejson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 34.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No 'significant_incidents' key in structured_incidents_data_2013.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2016.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2017.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2018.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2019.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2020.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2021.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2022.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2023.json\n",
      "Warning: No 'significant_incidents' key in structured_incidents_data_2024.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total incidents collected: 206\n",
      "\n",
      "Incidents per file:\n",
      "structured_incidents_data.json: 23 incidents\n",
      "structured_incidents_data_2009.json: 27 incidents\n",
      "structured_incidents_data_2010.json: 9 incidents\n",
      "structured_incidents_data_2011.json: 41 incidents\n",
      "structured_incidents_data_2012.json: 51 incidents\n",
      "structured_incidents_data_2014.json: 9 incidents\n",
      "structured_incidents_data_2015.json: 46 incidents\n",
      "\n",
      "Saved combined dataset to 'combined_incidents.csv'\n",
      "\n",
      "Verified CSV file exists with 206 rows and 11 columns\n",
      "\n",
      "CSV file columns:\n",
      "['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date', 'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost', 'source_file', 'Last_Report_Date']\n",
      "\n",
      "First few rows of the CSV file:\n",
      "                      Name Inc_Type GACC State  Start_Date  \\\n",
      "0               Glass Fire       WF   SA    TX  2008-02-25   \n",
      "1          Klamath Theater       WF   NO    CA  2008-06-21   \n",
      "2            Basin Complex       WF   SO    CA  2008-06-21   \n",
      "3  Iron & Alps \\nComplexes       WF   NO    CA  2008-06-21   \n",
      "4       Dunn Mtn. \\nAssist       WF   NR    MT  2008-08-21   \n",
      "\n",
      "  Contain_Control_Date  Size_Acres Cause         Cost  \\\n",
      "0           2008-03-02    219556.0     H          NaN   \n",
      "1           2008-09-26    192038.0     L  126086065.0   \n",
      "2           2008-07-29    162818.0     L   78096079.0   \n",
      "3           2008-09-04    105805.0     L   73974917.0   \n",
      "4           2008-09-02    102383.0     L    2900000.0   \n",
      "\n",
      "                      source_file Last_Report_Date  \n",
      "0  structured_incidents_data.json              NaN  \n",
      "1  structured_incidents_data.json              NaN  \n",
      "2  structured_incidents_data.json              NaN  \n",
      "3  structured_incidents_data.json              NaN  \n",
      "4  structured_incidents_data.json              NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "from datetime import datetime  \n",
    "import os  \n",
    "from tqdm import tqdm  \n",
    "  \n",
    "def clean_incident_data(incident):  \n",
    "    \"\"\"Clean and standardize a single incident record with improved date handling\"\"\"  \n",
    "    cleaned = {}  \n",
    "      \n",
    "    # Copy all fields with standardized names  \n",
    "    for key, value in incident.items():  \n",
    "        # Standardize key names (remove dots, and replace spaces with underscores)  \n",
    "        clean_key = key.replace('.', '').replace(' ', '_')  \n",
    "        cleaned[clean_key] = value  \n",
    "      \n",
    "    # Clean date fields - convert to proper datetime objects  \n",
    "    date_fields = ['Start_Date', 'Contain_Control_Date']  \n",
    "    for date_field in date_fields:  \n",
    "        if date_field in cleaned and cleaned[date_field]:  \n",
    "            date_value = cleaned[date_field]  \n",
    "            try:  \n",
    "                # Try different date formats:  \n",
    "                if re.match(r'\\d{1,2}-[A-Za-z]{3}-\\d{2}', date_value):  # e.g., \"10-Jul-09\"  \n",
    "                    date_obj = datetime.strptime(date_value, '%d-%b-%y')  \n",
    "                elif re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_value):  # e.g., \"2/25/2008\"  \n",
    "                    date_obj = datetime.strptime(date_value, '%m/%d/%Y')  \n",
    "                else:  \n",
    "                    # Format unknown; set to None  \n",
    "                    cleaned[date_field] = None  \n",
    "                    continue  \n",
    "                cleaned[date_field] = date_obj  \n",
    "            except Exception as e:  \n",
    "                print(\"Error parsing date \" + date_value + \": \" + str(e))  \n",
    "                cleaned[date_field] = None  \n",
    "  \n",
    "    # Clean numeric fields  \n",
    "    # Size_Acres  \n",
    "    if 'Size_Acres' in cleaned and cleaned['Size_Acres']:  \n",
    "        try:  \n",
    "            size_str = str(cleaned['Size_Acres']).replace(',', '')  \n",
    "            cleaned['Size_Acres'] = float(size_str)  \n",
    "        except:  \n",
    "            cleaned['Size_Acres'] = None  \n",
    "  \n",
    "    # Cost field  \n",
    "    if 'Cost' in cleaned and cleaned['Cost']:  \n",
    "        try:  \n",
    "            # Remove $, commas, spaces and convert to float  \n",
    "            cost_str = str(cleaned['Cost']).replace('$', '').replace(',', '').replace(' ', '')  \n",
    "            if cost_str.lower() not in ['nr', 'n/a', 'unknown']:  \n",
    "                cleaned['Cost'] = float(cost_str)  \n",
    "            else:  \n",
    "                cleaned['Cost'] = None  \n",
    "        except:  \n",
    "            cleaned['Cost'] = None  \n",
    "  \n",
    "    return cleaned  \n",
    "  \n",
    "# Set the path to the folder containing structured incident files.  \n",
    "folder_path = './structurejson'  \n",
    "structured_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.startswith('structured_incidents_data')]  \n",
    "  \n",
    "print(\"Found \" + str(len(structured_files)) + \" structured incident files in \" + folder_path)  \n",
    "  \n",
    "all_incidents = []  \n",
    "file_counts = {}  \n",
    "  \n",
    "for file_path in tqdm(structured_files):  \n",
    "    file_name = os.path.basename(file_path)  \n",
    "    try:  \n",
    "        with open(file_path, 'r') as f:  \n",
    "            data = json.load(f)  \n",
    "          \n",
    "        if 'significant_incidents' in data:  \n",
    "            incidents = data['significant_incidents']  \n",
    "            cleaned_incidents = [clean_incident_data(incident) for incident in incidents]  \n",
    "              \n",
    "            # Add source file info  \n",
    "            for incident in cleaned_incidents:  \n",
    "                incident['source_file'] = file_name  \n",
    "            all_incidents.extend(cleaned_incidents)  \n",
    "            file_counts[file_name] = len(cleaned_incidents)  \n",
    "        else:  \n",
    "            print(\"Warning: No 'significant_incidents' key in \" + file_name)  \n",
    "    except Exception as e:  \n",
    "        print(\"Error processing \" + file_name + \": \" + str(e))  \n",
    "  \n",
    "# Convert to DataFrame  \n",
    "incidents_df = pd.DataFrame(all_incidents)  \n",
    "  \n",
    "# Display summary  \n",
    "print(\"\\nTotal incidents collected:\", len(incidents_df))  \n",
    "print(\"\\nIncidents per file:\")  \n",
    "for file_name, count in file_counts.items():  \n",
    "    print(file_name + \": \" + str(count) + \" incidents\")  \n",
    "  \n",
    "# Save the combined dataset  \n",
    "incidents_df.to_csv('combined_incidents.csv', index=False)  \n",
    "print(\"\\nSaved combined dataset to 'combined_incidents.csv'\")  \n",
    "  \n",
    "# Verify the CSV file was created and check its contents  \n",
    "if os.path.exists('combined_incidents.csv'):  \n",
    "    df_from_csv = pd.read_csv('combined_incidents.csv')  \n",
    "    print(\"\\nVerified CSV file exists with \" + str(len(df_from_csv)) + \" rows and \" + str(len(df_from_csv.columns)) + \" columns\")  \n",
    "    print(\"\\nCSV file columns:\")  \n",
    "    print(df_from_csv.columns.tolist())  \n",
    "    print(\"\\nFirst few rows of the CSV file:\")  \n",
    "    print(df_from_csv.head())  \n",
    "else:  \n",
    "    print(\"\\nError: CSV file was not created\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f94363-31d2-4db8-8184-6e1fece45de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for names with asterisks or unusual patterns:\n",
      "Row 38: Rex Creek *\n",
      "Row 46: Little Black One **\n",
      "Row 47: Crazy Mountain\n",
      "Complex **\n",
      "Row 48: Minto Flats South *\n",
      "Row 49: Railbelt Complex *\n",
      "After filling missing name:\n",
      "Name          Unknown Incident\n",
      "State                       TX\n",
      "GACC                        SA\n",
      "Start_Date          2008-03-14\n",
      "Size_Acres             67500.0\n",
      "Name: 12, dtype: object\n",
      "Remaining missing names: 0\n",
      "Saved updated dataframe to 'combined_incidents_cleaned.csv'\n",
      "Sample of cleaned data:\n",
      "            Name State GACC  Start_Date  Size_Acres Cause\n",
      "30          Cato    NM   SW  2009-06-10     55080.0     L\n",
      "65       Wildcat    TX   SA  2011-04-11    159308.0     L\n",
      "66   Las Conchas    NM   SW  2011-06-26    156593.0     H\n",
      "39  Wood River 1    AK   AK  2009-07-12    125382.0     L\n",
      "8        Complex    CA   NO  2008-06-21     82186.0     L\n"
     ]
    }
   ],
   "source": [
    "# Load the combined_incidents.csv file\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('combined_incidents.csv')\n",
    "\n",
    "# Check for rows with names containing asterisks or unusual patterns\n",
    "print(\"Checking for names with asterisks or unusual patterns:\")\n",
    "for idx, name in enumerate(df['Name']):\n",
    "    if isinstance(name, str) and ('*' in name or re.match(r'^\\d+$', name)):\n",
    "        print(f\"Row {idx}: {name}\")\n",
    "\n",
    "# Fill the missing name with a placeholder\n",
    "df['Name'] = df['Name'].fillna('Unknown Incident')\n",
    "\n",
    "# Check if the placeholder was applied correctly\n",
    "print(\"\\\n",
    "After filling missing name:\")\n",
    "print(df.iloc[12][['Name', 'State', 'GACC', 'Start_Date', 'Size_Acres']])\n",
    "\n",
    "# Check if there are any remaining missing names\n",
    "print(f\"\\\n",
    "Remaining missing names: {df['Name'].isnull().sum()}\")\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv('combined_incidents_cleaned.csv', index=False)\n",
    "print(\"\\\n",
    "Saved updated dataframe to 'combined_incidents_cleaned.csv'\")\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "print(\"\\\n",
    "Sample of cleaned data:\")\n",
    "print(df.sample(5)[['Name', 'State', 'GACC', 'Start_Date', 'Size_Acres', 'Cause']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b46dba-852f-4d59-bdea-130427c45d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of cleaned names:\n",
      "24            Bluff Creek\n",
      "49       Railbelt Complex\n",
      "164            North Star\n",
      "126    Powell SBW Complex\n",
      "168      Okanogan Complex\n",
      "137                Seeley\n",
      "96           Matador West\n",
      "121              Wellnitz\n",
      "80                Prairie\n",
      "158          July Complex\n",
      "Name: Name, dtype: object\n",
      "Saved updated dataframe to combined_incidents_cleaned_no_special_names.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV file\n",
    "df = pd.read_csv('combined_incidents_cleaned.csv')\n",
    "\n",
    "# Define a function to clean the name field by removing asterisks and trailing numbers\n",
    "import re\n",
    "\n",
    "def clean_name(name):\n",
    "    if pd.isnull(name):\n",
    "        return name\n",
    "    # Remove asterisks\n",
    "    name_cleaned = re.sub(r'\\*+', '', name)\n",
    "    # Remove trailing numbers (e.g., ' Wood River 1' becomes ' Wood River')\n",
    "    name_cleaned = re.sub(r'\\s+\\d+$', '', name_cleaned)\n",
    "    # Strip extra whitespace\n",
    "    return name_cleaned.strip()\n",
    "\n",
    "# Apply cleaning function to Name column\n",
    "df['Name'] = df['Name'].apply(clean_name)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "df.to_csv('combined_incidents_cleaned_no_special_names.csv', index=False)\n",
    "\n",
    "# Show a sample of the cleaned names\n",
    "print('Sample of cleaned names:')\n",
    "print(df['Name'].sample(10))\n",
    "\n",
    "print('\\\n",
    "Saved updated dataframe to combined_incidents_cleaned_no_special_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f38cf4a7-940e-4a9d-8475-eed1320670d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for structured_incidents_data files in structurejson...\n",
      "Found 13 structured_incidents_data files from 2012 to 2024.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 97.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing structurejson\\structured_incidents_data_2012.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2012.json.bak\n",
      "Made 102 date format changes in structurejson\\structured_incidents_data_2012.json\n",
      "Processing structurejson\\structured_incidents_data_2013.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2013.json.bak\n",
      "Made 20 date format changes in structurejson\\structured_incidents_data_2013.json\n",
      "Processing structurejson\\structured_incidents_data_2014.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2014.json.bak\n",
      "Made 9 date format changes in structurejson\\structured_incidents_data_2014.json\n",
      "Processing structurejson\\structured_incidents_data_2015.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2015.json.bak\n",
      "Made 46 date format changes in structurejson\\structured_incidents_data_2015.json\n",
      "Processing structurejson\\structured_incidents_data_2016.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2016.json.bak\n",
      "Made 19 date format changes in structurejson\\structured_incidents_data_2016.json\n",
      "Processing structurejson\\structured_incidents_data_2017.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2017.json.bak\n",
      "Made 30 date format changes in structurejson\\structured_incidents_data_2017.json\n",
      "Processing structurejson\\structured_incidents_data_2018.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2018.json.bak\n",
      "Made 24 date format changes in structurejson\\structured_incidents_data_2018.json\n",
      "Processing structurejson\\structured_incidents_data_2019.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2019.json.bak\n",
      "Made 26 date format changes in structurejson\\structured_incidents_data_2019.json\n",
      "Processing structurejson\\structured_incidents_data_2020.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2020.json.bak\n",
      "Made 34 date format changes in structurejson\\structured_incidents_data_2020.json\n",
      "Processing structurejson\\structured_incidents_data_2021.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2021.json.bak\n",
      "Made 24 date format changes in structurejson\\structured_incidents_data_2021.json\n",
      "Processing structurejson\\structured_incidents_data_2022.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2022.json.bak\n",
      "Made 40 date format changes in structurejson\\structured_incidents_data_2022.json\n",
      "Processing structurejson\\structured_incidents_data_2023.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2023.json.bak\n",
      "Made 10 date format changes in structurejson\\structured_incidents_data_2023.json\n",
      "Processing structurejson\\structured_incidents_data_2024.json...\n",
      "Created backup: structurejson\\structured_incidents_data_2024.json.bak\n",
      "Made 0 date format changes in structurejson\\structured_incidents_data_2024.json\n",
      "Verifying changes in structurejson\\structured_incidents_data_2012.json:\n",
      "Sample records:\n",
      "Key: significant_incidents\n",
      "  Record 1 Start_Date: 08-Jul-12\n",
      "  Record 1 Name: Long Draw\n",
      "  Record 2 Start_Date: 05-Aug-12\n",
      "  Record 2 Name: Holloway\n",
      "  Record 3 Start_Date: 30-Jul-12\n",
      "  Record 3 Name: Mustang Complex\n",
      "Date format conversion completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if the directory exists\n",
    "structurejson_dir = 'structurejson'\n",
    "if not os.path.exists(structurejson_dir):\n",
    "    print(f\"Directory '{structurejson_dir}' does not exist.\")\n",
    "    # Check if it exists in the current directory structure\n",
    "    if os.path.exists('Wildfire/Fire report/structurejson'):\n",
    "        structurejson_dir = 'Wildfire/Fire report/structurejson'\n",
    "        print(f\"Using directory '{structurejson_dir}' instead.\")\n",
    "    else:\n",
    "        print(\"Could not find the structurejson directory.\")\n",
    "        exit()\n",
    "\n",
    "# List all structured_incidents_data files from 2012 to 2024\n",
    "print(f\"Looking for structured_incidents_data files in {structurejson_dir}...\")\n",
    "structured_files = []\n",
    "for year in range(2012, 2025):\n",
    "    file_name = f\"structured_incidents_data_{year}.json\"\n",
    "    file_path = os.path.join(structurejson_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        structured_files.append((file_path, year))\n",
    "\n",
    "print(f\"Found {len(structured_files)} structured_incidents_data files from 2012 to 2024.\")\n",
    "\n",
    "# Function to convert date format from \"9/4\" to \"04-Sep-12\" format\n",
    "def convert_date_format(date_str, year):\n",
    "    if not date_str or pd.isna(date_str) or (isinstance(date_str, str) and date_str.strip() == \"\"):\n",
    "        return None\n",
    "    \n",
    "    # If it already has the desired format (DD-MMM-YY), return as is\n",
    "    if isinstance(date_str, str) and re.match(r'\\d{1,2}-[A-Za-z]{3}-\\d{2}', date_str):\n",
    "        return date_str\n",
    "    \n",
    "    try:\n",
    "        # Handle M/D format (e.g., \"9/4\")\n",
    "        if isinstance(date_str, str) and re.match(r'\\d{1,2}/\\d{1,2}', date_str):\n",
    "            month, day = map(int, date_str.split('/'))\n",
    "            # Create a datetime object\n",
    "            date_obj = datetime(year, month, day)\n",
    "            # Format as \"DD-MMM-YY\"\n",
    "            return date_obj.strftime(\"%d-%b-%y\")\n",
    "        \n",
    "        # Handle other formats if needed\n",
    "        # For now, return as is if format is not recognized\n",
    "        return date_str\n",
    "    except:\n",
    "        # If conversion fails, return the original string\n",
    "        return date_str\n",
    "\n",
    "# Process each file\n",
    "for file_path, year in tqdm(structured_files):\n",
    "    print(f\"\\\n",
    "Processing {file_path}...\")\n",
    "    \n",
    "    # Create a backup if it doesn't exist\n",
    "    backup_path = file_path + '.bak'\n",
    "    if not os.path.exists(backup_path):\n",
    "        import shutil\n",
    "        shutil.copy2(file_path, backup_path)\n",
    "        print(f\"Created backup: {backup_path}\")\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Track changes\n",
    "    changes_made = 0\n",
    "    \n",
    "    # Process the data based on its structure\n",
    "    if isinstance(data, list):\n",
    "        # If data is a list of records\n",
    "        for record in data:\n",
    "            if isinstance(record, dict):\n",
    "                # Update Start_Date\n",
    "                if 'Start_Date' in record:\n",
    "                    old_date = record['Start_Date']\n",
    "                    new_date = convert_date_format(old_date, year)\n",
    "                    if old_date != new_date:\n",
    "                        record['Start_Date'] = new_date\n",
    "                        changes_made += 1\n",
    "                \n",
    "                # Update Control_Date or Contain_Control_Date\n",
    "                for date_field in ['Control_Date', 'Contain_Control_Date']:\n",
    "                    if date_field in record:\n",
    "                        old_date = record[date_field]\n",
    "                        new_date = convert_date_format(old_date, year)\n",
    "                        if old_date != new_date:\n",
    "                            record[date_field] = new_date\n",
    "                            changes_made += 1\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        # If data is a dictionary with nested records\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, list):\n",
    "                for record in value:\n",
    "                    if isinstance(record, dict):\n",
    "                        # Update Start_Date\n",
    "                        if 'Start_Date' in record:\n",
    "                            old_date = record['Start_Date']\n",
    "                            new_date = convert_date_format(old_date, year)\n",
    "                            if old_date != new_date:\n",
    "                                record['Start_Date'] = new_date\n",
    "                                changes_made += 1\n",
    "                        \n",
    "                        # Update Control_Date or Contain_Control_Date\n",
    "                        for date_field in ['Control_Date', 'Contain_Control_Date']:\n",
    "                            if date_field in record:\n",
    "                                old_date = record[date_field]\n",
    "                                new_date = convert_date_format(old_date, year)\n",
    "                                if old_date != new_date:\n",
    "                                    record[date_field] = new_date\n",
    "                                    changes_made += 1\n",
    "    \n",
    "    # Save the updated data\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Made {changes_made} date format changes in {file_path}\")\n",
    "\n",
    "# Verify changes in a sample file\n",
    "if structured_files:\n",
    "    sample_file_path, sample_year = structured_files[0]\n",
    "    print(f\"\\\n",
    "Verifying changes in {sample_file_path}:\")\n",
    "    \n",
    "    with open(sample_file_path, 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    # Display sample records\n",
    "    if isinstance(sample_data, list) and sample_data:\n",
    "        print(\"\\\n",
    "Sample records:\")\n",
    "        for i, record in enumerate(sample_data[:3]):  # Show first 3 records\n",
    "            if 'Start_Date' in record:\n",
    "                print(f\"Record {i+1} Start_Date: {record['Start_Date']}\")\n",
    "            if 'Name' in record:\n",
    "                print(f\"Record {i+1} Name: {record['Name']}\")\n",
    "    \n",
    "    elif isinstance(sample_data, dict):\n",
    "        print(\"\\\n",
    "Sample records:\")\n",
    "        for key, value in list(sample_data.items())[:1]:  # Show first key\n",
    "            print(f\"Key: {key}\")\n",
    "            if isinstance(value, list) and value:\n",
    "                for i, record in enumerate(value[:3]):  # Show first 3 records\n",
    "                    if isinstance(record, dict):\n",
    "                        if 'Start_Date' in record:\n",
    "                            print(f\"  Record {i+1} Start_Date: {record['Start_Date']}\")\n",
    "                        if 'Name' in record:\n",
    "                            print(f\"  Record {i+1} Name: {record['Name']}\")\n",
    "\n",
    "print(\"\\\n",
    "Date format conversion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04a16bd7-4afc-4160-b07e-652f07cb1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████████████████████████| 17/17 [00:00<00:00, 2840.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total incidents collected: 470\n",
      "Converting to DataFrame...\n",
      "Columns in the DataFrame: ['Name', 'GACC', 'State', 'Start_Date', 'Last_Report_Date', 'Size_Acres', 'Cause', 'Cost', 'Inc_Type', 'Contain_Control_Date', 'Year', 'Estimated_Cost', 'Contain_or_Last_Report_Date', 'Start Date', 'Last Report Date', 'Size In Acres']\n",
      "Sample of the combined data:\n",
      "                 Name GACC State Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "0     Buzzard Complex   NW    OR  14-Jul-14             9/11    395,747     L   \n",
      "1     Carlton Complex   NW    WA  14-Jul-14             8/28    256,108     L   \n",
      "2         Funny River   AK    AK  19-May-14             8/14    195,858     H   \n",
      "3  Happy Camp Complex   NO    CA  14-Aug-14             12/4    134,056     L   \n",
      "4                King   NO    CA  13-Sep-14             10/9     97,717     H   \n",
      "\n",
      "           Cost Inc_Type Contain_Control_Date    Year Estimated_Cost  \\\n",
      "0   $11,062,411     None                 None  2014.0            NaN   \n",
      "1   $68,800,000     None                 None  2014.0            NaN   \n",
      "2   $11,496,627     None                 None  2014.0            NaN   \n",
      "3   $88,214,725     None                 None  2014.0            NaN   \n",
      "4  $119,000,000     None                 None  2014.0            NaN   \n",
      "\n",
      "  Contain_or_Last_Report_Date Start Date Last Report Date Size In Acres  \n",
      "0                         NaN        NaN              NaN           NaN  \n",
      "1                         NaN        NaN              NaN           NaN  \n",
      "2                         NaN        NaN              NaN           NaN  \n",
      "3                         NaN        NaN              NaN           NaN  \n",
      "4                         NaN        NaN              NaN           NaN  \n",
      "Saving the CSV to Wildfire\\Fire report\\combined_incidents.csv ...\n",
      "CSV file created: Wildfire\\Fire report\\combined_incidents.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the directory where the JSON files are located (just /structurejson)\n",
    "target_dir = \"structurejson\"\n",
    "\n",
    "# List of file names provided (only the structurejson files)\n",
    "file_names = [\n",
    "    \"structured_incidents_data_2014.json\",\n",
    "    \"structured_incidents_data.json\",\n",
    "    \"structured_incidents_data_2017.json\",\n",
    "    \"structured_incidents_data_2013.json\",\n",
    "    \"structured_incidents_data_2022.json\",\n",
    "    \"structured_incidents_data_2010.json\",\n",
    "    \"structured_incidents_data_2016.json\",\n",
    "    \"structured_incidents_data_2012.json\",\n",
    "    \"structured_incidents_data_2020.json\",\n",
    "    \"structured_incidents_data_2018.json\",\n",
    "    \"structured_incidents_data_2023.json\",\n",
    "    \"structured_incidents_data_2015.json\",\n",
    "    \"structured_incidents_data_2021.json\",\n",
    "    \"structured_incidents_data_2024.json\",\n",
    "    \"structured_incidents_data_2019.json\",\n",
    "    \"structured_incidents_data_2011.json\",\n",
    "    \"structured_incidents_data_2009.json\"\n",
    "]\n",
    "\n",
    "# Remove duplicates if any\n",
    "file_names = list(dict.fromkeys(file_names))\n",
    "\n",
    "# Prepare a list to hold all incidents\n",
    "all_incidents = []\n",
    "\n",
    "# Regex to extract a 4-digit year from file names if available\n",
    "year_pattern = re.compile(r'structured_incidents_data_(\\d{4})\\.json')\n",
    "\n",
    "# Process each file from the /structurejson folder\n",
    "for file in tqdm(file_names, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(target_dir, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"File not found:\", file_path)\n",
    "        continue\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading\", file_path, \":\", e)\n",
    "        continue\n",
    "\n",
    "    # Determine year from filename if available\n",
    "    m = year_pattern.search(file)\n",
    "    if m:\n",
    "        year = int(m.group(1))\n",
    "    else:\n",
    "        year = None\n",
    "\n",
    "    # Extract incident records\n",
    "    incidents = []\n",
    "    if isinstance(data, list):\n",
    "        incidents = data\n",
    "    elif isinstance(data, dict):\n",
    "        # Extract list from any key that is a list\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, list):\n",
    "                incidents = value\n",
    "                break\n",
    "\n",
    "    # Add year information to each incident (if available)\n",
    "    for incident in incidents:\n",
    "        if isinstance(incident, dict) and year is not None:\n",
    "            incident[\"Year\"] = year\n",
    "\n",
    "    all_incidents.extend(incidents)\n",
    "\n",
    "print(\"\\\n",
    "Total incidents collected:\", len(all_incidents))\n",
    "\n",
    "# Convert the combined list of incidents into a DataFrame\n",
    "print(\"\\\n",
    "Converting to DataFrame...\")\n",
    "df = pd.DataFrame(all_incidents)\n",
    "\n",
    "print(\"\\\n",
    "Columns in the DataFrame:\", df.columns.tolist())\n",
    "print(\"\\\n",
    "Sample of the combined data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Define output directory as 'Wildfire/Fire report'\n",
    "output_dir = os.path.join(\"Wildfire\", \"Fire report\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_csv = os.path.join(output_dir, \"combined_incidents.csv\")\n",
    "print(\"\\\n",
    "Saving the CSV to\", output_csv, \"...\")\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(\"CSV file created:\", output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcbbcd-2fb0-47a0-8df0-ea438834b3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
