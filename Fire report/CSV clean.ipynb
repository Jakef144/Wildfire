{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6092bb6a-3496-4cb6-8dac-d3c7ffe844da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample values from Last_Report_Date column:\n",
      "['9/11', '8/28', '8/14', '12/4', '10/9', '6/26', '8/22', '9/25', '9/25', '3/24']\n",
      "Unique formats in Last_Report_Date column:\n",
      "['9/11' '8/28' '8/14' '12/4' '10/9' '6/26' '8/22' '9/25' '3/24' '3/13'\n",
      " 'Active\\ninto\\n2018' '8/11' '7/24' '10/26' '10/17' '7/25' '6/29' '3/10'\n",
      " '10/5' '10/10']\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the original Last_Report_Date column to understand the format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reload the file\n",
    "df = pd.read_csv('combined_incidents.csv', encoding='utf-8')\n",
    "\n",
    "# Look at some examples of Last_Report_Date\n",
    "print(\"Sample values from Last_Report_Date column:\")\n",
    "print(df['Last_Report_Date'].dropna().head(10).tolist())\n",
    "\n",
    "# Check if there are any patterns in the date format\n",
    "print(\"\\\n",
    "Unique formats in Last_Report_Date column:\")\n",
    "formats = df['Last_Report_Date'].dropna().unique()\n",
    "print(formats[:20])  # Show first 20 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930bc182-eb33-40c1-a61c-31bc4b4b0bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Last_Report_Date values:\n",
      "0    9/11\n",
      "1    8/28\n",
      "2    8/14\n",
      "3    12/4\n",
      "4    10/9\n",
      "5    6/26\n",
      "6    8/22\n",
      "7    9/25\n",
      "8    9/25\n",
      "9     NaN\n",
      "Name: Last_Report_Date, dtype: object\n",
      "Cleaned date columns (first 10 rows):\n",
      "Start_Date_Clean:\n",
      "0    14/07/2014\n",
      "1    14/07/2014\n",
      "2    19/05/2014\n",
      "3    14/08/2014\n",
      "4    13/09/2014\n",
      "5    19/04/2014\n",
      "6    02/08/2014\n",
      "7    03/08/2014\n",
      "8    12/07/2014\n",
      "9     2/25/2008\n",
      "Name: Start_Date_Clean, dtype: object\n",
      "Last_Report_Date_Clean:\n",
      "0    11/09/2014\n",
      "1    28/08/2014\n",
      "2    14/08/2014\n",
      "3    04/12/2014\n",
      "4    09/10/2014\n",
      "5    26/06/2014\n",
      "6    22/08/2014\n",
      "7    25/09/2014\n",
      "8    25/09/2014\n",
      "9           NaN\n",
      "Name: Last_Report_Date_Clean, dtype: object\n",
      "Contain_Control_Date_Clean:\n",
      "0           NaN\n",
      "1           NaN\n",
      "2           NaN\n",
      "3           NaN\n",
      "4           NaN\n",
      "5           NaN\n",
      "6           NaN\n",
      "7           NaN\n",
      "8           NaN\n",
      "9    03/02/2008\n",
      "Name: Contain_Control_Date_Clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's check the first row where we have a Last_Report_Date but Last_Report_Date_Clean is null\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Reload the file\n",
    "df = pd.read_csv('combined_incidents.csv', encoding='utf-8')\n",
    "\n",
    "# Create a clean dataframe with the original columns\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Let's look at the first few rows to understand the issue\n",
    "print(\"Original Last_Report_Date values:\")\n",
    "print(df['Last_Report_Date'].head(10))\n",
    "\n",
    "# The issue is that dates like '9/11' don't have a year component\n",
    "# We need to add the year from the 'Year' column\n",
    "\n",
    "# Function to parse and standardize dates\n",
    "def parse_date(date_str, year=None):\n",
    "    if pd.isnull(date_str) or date_str == 'None':\n",
    "        return np.nan\n",
    "    \n",
    "    # Handle special cases\n",
    "    if 'Active' in str(date_str):\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        # Try to parse with various formats\n",
    "        date_str = str(date_str).strip()\n",
    "        \n",
    "        # Check if it's just month/day format (e.g., '9/11')\n",
    "        if '/' in date_str and len(date_str.split('/')) == 2:\n",
    "            month, day = date_str.split('/')\n",
    "            if year is not None:\n",
    "                # Create a date with the provided year\n",
    "                return f\"{day.zfill(2)}/{month.zfill(2)}/{int(year)}\"\n",
    "        \n",
    "        # Try to parse with datetime to standardize\n",
    "        try:\n",
    "            # Try different formats\n",
    "            for fmt in ['%d-%b-%y', '%d/%m/%Y', '%d/%m/%y', '%d-%m-%Y', '%Y-%m-%d']:\n",
    "                try:\n",
    "                    dt = datetime.strptime(date_str, fmt)\n",
    "                    return f\"{dt.day:02d}/{dt.month:02d}/{dt.year}\"\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If we couldn't parse it, return as is\n",
    "        return date_str\n",
    "    except:\n",
    "        return date_str\n",
    "\n",
    "# Apply the function to Start_Date and Last_Report_Date\n",
    "df_clean['Start_Date_Clean'] = df.apply(\n",
    "    lambda row: parse_date(row['Start_Date'], row['Year']), axis=1\n",
    ")\n",
    "\n",
    "df_clean['Last_Report_Date_Clean'] = df.apply(\n",
    "    lambda row: parse_date(row['Last_Report_Date'], row['Year']), axis=1\n",
    ")\n",
    "\n",
    "# For Contain_Control_Date, also use the Year column\n",
    "df_clean['Contain_Control_Date_Clean'] = df.apply(\n",
    "    lambda row: parse_date(row['Contain_Control_Date'], row['Year']), axis=1\n",
    ")\n",
    "\n",
    "# Clean the Size_Acres column\n",
    "def clean_size(x):\n",
    "    try:\n",
    "        if pd.isnull(x):\n",
    "            return np.nan\n",
    "        # Remove commas\n",
    "        x = str(x).replace(',', '')\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_clean['Size_Acres_Clean'] = df['Size_Acres'].apply(clean_size)\n",
    "\n",
    "# Clean the Cost columns\n",
    "def clean_cost(x):\n",
    "    try:\n",
    "        if pd.isnull(x) or x in ['NR', 'None']:\n",
    "            return np.nan\n",
    "        # Remove $ and commas\n",
    "        x = str(x).replace('$', '').replace(',', '')\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_clean['Cost_Clean'] = df['Cost'].apply(clean_cost)\n",
    "df_clean['Estimated_Cost_Clean'] = df['Estimated_Cost'].apply(clean_cost)\n",
    "\n",
    "# Show the results\n",
    "print(\"\\\n",
    "Cleaned date columns (first 10 rows):\")\n",
    "print(\"Start_Date_Clean:\")\n",
    "print(df_clean['Start_Date_Clean'].head(10))\n",
    "print(\"\\\n",
    "Last_Report_Date_Clean:\")\n",
    "print(df_clean['Last_Report_Date_Clean'].head(10))\n",
    "print(\"\\\n",
    "Contain_Control_Date_Clean:\")\n",
    "print(df_clean['Contain_Control_Date_Clean'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a9d245-1354-429f-9835-af3dadf6c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified original combined_incidents.csv with dates in mm/dd/yyyy format (first 10 rows):\n",
      "                 Name GACC State  Start_Date Last_Report_Date Size_Acres  \\\n",
      "0     Buzzard Complex   NW    OR  07/14/2014       09/11/2014    395,747   \n",
      "1     Carlton Complex   NW    WA  07/14/2014       08/28/2014    256,108   \n",
      "2         Funny River   AK    AK  05/19/2014       08/14/2014    195,858   \n",
      "3  Happy Camp Complex   NO    CA  08/14/2014       12/04/2014    134,056   \n",
      "4                King   NO    CA  09/13/2014       10/09/2014     97,717   \n",
      "5               Skunk   SW    AZ  04/19/2014       06/26/2014     73,622   \n",
      "6          Big Cougar   NR    ID  08/02/2014       08/22/2014     65,227   \n",
      "7        July Complex   NO    CA  08/03/2014       09/25/2014     50,042   \n",
      "8       Shaniko Butte   NW    OR  07/12/2014       09/25/2014     42,044   \n",
      "9          Glass Fire   SA    TX  02/25/2008              NaN    219,556   \n",
      "\n",
      "  Cause          Cost Inc_Type Contain_Control_Date    Year Estimated_Cost  \\\n",
      "0     L   $11,062,411      NaN                  NaN  2014.0            NaN   \n",
      "1     L   $68,800,000      NaN                  NaN  2014.0            NaN   \n",
      "2     H   $11,496,627      NaN                  NaN  2014.0            NaN   \n",
      "3     L   $88,214,725      NaN                  NaN  2014.0            NaN   \n",
      "4     H  $119,000,000      NaN                  NaN  2014.0            NaN   \n",
      "5     L    $1,800,000      NaN                  NaN  2014.0            NaN   \n",
      "6     L    $4,500,000      NaN                  NaN  2014.0            NaN   \n",
      "7     L   $50,295,981      NaN                  NaN  2014.0            NaN   \n",
      "8     L    $5,200,000      NaN                  NaN  2014.0            NaN   \n",
      "9     H           NR        WF           03/02/2008     NaN            NaN   \n",
      "\n",
      "  Contain_or_Last_Report_Date Start Date Last Report Date Size In Acres  \n",
      "0                         NaN        NaN              NaN           NaN  \n",
      "1                         NaN        NaN              NaN           NaN  \n",
      "2                         NaN        NaN              NaN           NaN  \n",
      "3                         NaN        NaN              NaN           NaN  \n",
      "4                         NaN        NaN              NaN           NaN  \n",
      "5                         NaN        NaN              NaN           NaN  \n",
      "6                         NaN        NaN              NaN           NaN  \n",
      "7                         NaN        NaN              NaN           NaN  \n",
      "8                         NaN        NaN              NaN           NaN  \n",
      "9                         NaN        NaN              NaN           NaN  \n",
      "Modified file saved as combined_incidents_mmdd_format.csv\n"
     ]
    }
   ],
   "source": [
    "# Reload the original CSV and convert the dates to mm/dd/yyyy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Reload the original file\n",
    "df_original = pd.read_csv('combined_incidents.csv', encoding='utf-8')\n",
    "\n",
    "# Function to parse and standardize dates in mm/dd/yyyy format\n",
    "\n",
    "def parse_date_mmddyyyy(date_str, year=None):\n",
    "    if pd.isnull(date_str) or date_str == 'None':\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Handle special case for 'Active' present content\n",
    "    if 'Active' in date_str:\n",
    "        if year is not None:\n",
    "            return f\"12/31/{int(year)}\"  # placeholder end of year\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        # Check if it's just month/day format (e.g., '9/11')\n",
    "        if '/' in date_str and len(date_str.split('/')) == 2:\n",
    "            month, day = date_str.split('/')\n",
    "            if year is not None:\n",
    "                return f\"{month.zfill(2)}/{day.zfill(2)}/{int(year)}\"\n",
    "            else:\n",
    "                return f\"{month.zfill(2)}/{day.zfill(2)}/1900\"\n",
    "        \n",
    "        # Handle dash-separated format with a month abbreviation, e.g., \"14-Jul-14\"\n",
    "        if '-' in date_str:\n",
    "            try:\n",
    "                dt = datetime.strptime(date_str, '%d-%b-%y')\n",
    "                return f\"{dt.month:02d}/{dt.day:02d}/{dt.year}\"\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Try parsing with common formats\n",
    "        for fmt in ['%m/%d/%Y','%m/%d/%y', '%d/%m/%Y', '%d/%m/%y','%m-%d-%Y','%Y-%m-%d']:\n",
    "            try:\n",
    "                dt = datetime.strptime(date_str, fmt)\n",
    "                return f\"{dt.month:02d}/{dt.day:02d}/{dt.year}\"\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # If not parsed, try to manually combine with year if available\n",
    "        if year is not None and '/' in date_str:\n",
    "            parts = date_str.split('/')\n",
    "            if len(parts)==2:\n",
    "                month, day = parts\n",
    "                return f\"{month.zfill(2)}/{day.zfill(2)}/{int(year)}\"\n",
    "        \n",
    "        return date_str\n",
    "    except Exception as e:\n",
    "        return date_str\n",
    "\n",
    "# Create a new dataframe to store the date converted results, starting from original\n",
    "# Keeping the original columns\n",
    "df_mmdd = df_original.copy()\n",
    "\n",
    "# Convert date columns using the new function with mm/dd/yyyy formatting\n",
    "\n",
    "# For Start_Date\n",
    "if 'Start_Date' in df_mmdd.columns:\n",
    "    df_mmdd['Start_Date'] = df_mmdd.apply(lambda row: parse_date_mmddyyyy(row['Start_Date'], row['Year'] if 'Year' in row else None), axis=1)\n",
    "\n",
    "# For Last_Report_Date\n",
    "if 'Last_Report_Date' in df_mmdd.columns:\n",
    "    df_mmdd['Last_Report_Date'] = df_mmdd.apply(lambda row: parse_date_mmddyyyy(row['Last_Report_Date'], row['Year'] if 'Year' in row else None), axis=1)\n",
    "\n",
    "# For 'Contain_Control_Date' if present\n",
    "if 'Contain_Control_Date' in df_mmdd.columns:\n",
    "    df_mmdd['Contain_Control_Date'] = df_mmdd.apply(lambda row: parse_date_mmddyyyy(row['Contain_Control_Date'], row['Year'] if 'Year' in row else None), axis=1)\n",
    "\n",
    "# Show a preview of the modified dataframe\n",
    "print('Modified original combined_incidents.csv with dates in mm/dd/yyyy format (first 10 rows):')\n",
    "print(df_mmdd.head(10))\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "filename = 'combined_incidents_mmdd_format.csv'\n",
    "df_mmdd.to_csv(filename, index=False)\n",
    "print('\\\n",
    "Modified file saved as ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e16035-a9c0-4006-a490-5e431af29ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 2024 rows (first 10):\n",
      "                   Name GACC State  Start_Date Last_Report_Date Size_Acres  \\\n",
      "339         Betty's Way   RM    NE  02/26/2024       03/11/2024    69810.0   \n",
      "340    Smokehouse Creek   SA    TX  02/26/2024       03/17/2024  1054153.0   \n",
      "341             Catesby   SA    OK  02/27/2024       03/15/2024    89688.0   \n",
      "342            McDonald   AK    AK  06/08/2024       07/20/2024   152227.0   \n",
      "343            Midnight   AK    AK  06/19/2024       07/09/2024    52550.0   \n",
      "344  Grapefruit Complex   AK    AK  06/28/2024       07/13/2024    89011.0   \n",
      "345               Falls   NW    OR  07/10/2024       08/21/2024   151689.0   \n",
      "346          Cow Valley   NW    OR  07/11/2024       08/09/2024   133490.0   \n",
      "347           Lone Rock   NW    OR  07/13/2024       08/31/2024   137222.0   \n",
      "348            Boneyard   NW    OR  07/17/2024       07/25/2024    49716.0   \n",
      "\n",
      "    Cause Cost Inc_Type Contain_Control_Date    Year Estimated_Cost  \\\n",
      "339     U  NaN      NaN                  NaN  2024.0            NaN   \n",
      "340     H  NaN      NaN                  NaN  2024.0            NaN   \n",
      "341     H  NaN      NaN                  NaN  2024.0            NaN   \n",
      "342     L  NaN      NaN                  NaN  2024.0            NaN   \n",
      "343     L  NaN      NaN                  NaN  2024.0            NaN   \n",
      "344     U  NaN      NaN                  NaN  2024.0            NaN   \n",
      "345     H  NaN      NaN                  NaN  2024.0            NaN   \n",
      "346     U  NaN      NaN                  NaN  2024.0            NaN   \n",
      "347     U  NaN      NaN                  NaN  2024.0            NaN   \n",
      "348     L  NaN      NaN                  NaN  2024.0            NaN   \n",
      "\n",
      "    Contain_or_Last_Report_Date Start Date Last Report Date Size In Acres  \n",
      "339                         NaN       2/26             3/11        69,810  \n",
      "340                         NaN       2/26             3/17     1,054,153  \n",
      "341                         NaN       2/27             3/15        89,688  \n",
      "342                         NaN        6/8             7/20       152,227  \n",
      "343                         NaN       6/19              7/9        52,550  \n",
      "344                         NaN       6/28             7/13        89,011  \n",
      "345                         NaN       7/10             8/21       151,689  \n",
      "346                         NaN       7/11              8/9       133,490  \n",
      "347                         NaN       7/13             8/31       137,222  \n",
      "348                         NaN       7/17             7/25        49,716  \n",
      "Updated data saved to 'combined_incidents_fixed.csv'\n",
      "Clean version with essential columns saved to 'combined_incidents_clean.csv'\n",
      "Clean version with essential columns (first 10 rows):\n",
      "                 Name GACC State  Start_Date Last_Report_Date Size_Acres  \\\n",
      "0     Buzzard Complex   NW    OR  07/14/2014       09/11/2014    395,747   \n",
      "1     Carlton Complex   NW    WA  07/14/2014       08/28/2014    256,108   \n",
      "2         Funny River   AK    AK  05/19/2014       08/14/2014    195,858   \n",
      "3  Happy Camp Complex   NO    CA  08/14/2014       12/04/2014    134,056   \n",
      "4                King   NO    CA  09/13/2014       10/09/2014     97,717   \n",
      "5               Skunk   SW    AZ  04/19/2014       06/26/2014     73,622   \n",
      "6          Big Cougar   NR    ID  08/02/2014       08/22/2014     65,227   \n",
      "7        July Complex   NO    CA  08/03/2014       09/25/2014     50,042   \n",
      "8       Shaniko Butte   NW    OR  07/12/2014       09/25/2014     42,044   \n",
      "9          Glass Fire   SA    TX  02/25/2008              NaN    219,556   \n",
      "\n",
      "  Cause          Cost Inc_Type Contain_Control_Date    Year  \n",
      "0     L   $11,062,411      NaN                  NaN  2014.0  \n",
      "1     L   $68,800,000      NaN                  NaN  2014.0  \n",
      "2     H   $11,496,627      NaN                  NaN  2014.0  \n",
      "3     L   $88,214,725      NaN                  NaN  2014.0  \n",
      "4     H  $119,000,000      NaN                  NaN  2014.0  \n",
      "5     L    $1,800,000      NaN                  NaN  2014.0  \n",
      "6     L    $4,500,000      NaN                  NaN  2014.0  \n",
      "7     L   $50,295,981      NaN                  NaN  2014.0  \n",
      "8     L    $5,200,000      NaN                  NaN  2014.0  \n",
      "9     H           NR        WF           03/02/2008     NaN  \n"
     ]
    }
   ],
   "source": [
    "# Let's fix the issue with the 2024 rows by moving the data from the extra columns to the standard columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv('combined_incidents_mmdd_format.csv')\n",
    "\n",
    "# Function to convert date format for 2024 rows\n",
    "def convert_date_format(date_str, year):\n",
    "    if pd.isnull(date_str) or date_str == 'None':\n",
    "        return np.nan\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Check if it's just month/day format (e.g., '2/26')\n",
    "    if '/' in date_str and len(date_str.split('/')) == 2:\n",
    "        month, day = date_str.split('/')\n",
    "        return f\"{month.zfill(2)}/{day.zfill(2)}/{int(year)}\"\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "# Function to clean size acres\n",
    "def clean_size_acres(size_str):\n",
    "    if pd.isnull(size_str) or size_str == 'None':\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove commas and convert to float\n",
    "    try:\n",
    "        return float(str(size_str).replace(',', ''))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Process 2024 rows\n",
    "mask_2024 = df['Year'] == 2024\n",
    "\n",
    "# Move data from extra columns to standard columns for 2024 rows\n",
    "df.loc[mask_2024, 'Start_Date'] = df.loc[mask_2024].apply(\n",
    "    lambda row: convert_date_format(row['Start Date'], row['Year']), axis=1\n",
    ")\n",
    "\n",
    "df.loc[mask_2024, 'Last_Report_Date'] = df.loc[mask_2024].apply(\n",
    "    lambda row: convert_date_format(row['Last Report Date'], row['Year']), axis=1\n",
    ")\n",
    "\n",
    "df.loc[mask_2024, 'Size_Acres'] = df.loc[mask_2024]['Size In Acres'].apply(clean_size_acres)\n",
    "\n",
    "# Show the updated 2024 rows\n",
    "print(\"Updated 2024 rows (first 10):\")\n",
    "print(df.loc[mask_2024].head(10))\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv('combined_incidents_fixed.csv', index=False)\n",
    "print(\"\\\n",
    "Updated data saved to 'combined_incidents_fixed.csv'\")\n",
    "\n",
    "# Let's also create a clean version with only the essential columns\n",
    "df_clean = df[['Name', 'GACC', 'State', 'Start_Date', 'Last_Report_Date', 'Size_Acres', 'Cause', 'Cost', 'Inc_Type', 'Contain_Control_Date', 'Year']]\n",
    "\n",
    "# Save the clean version\n",
    "df_clean.to_csv('combined_incidents_clean.csv', index=False)\n",
    "print(\"\\\n",
    "Clean version with essential columns saved to 'combined_incidents_clean.csv'\")\n",
    "\n",
    "# Show the clean version\n",
    "print(\"\\\n",
    "Clean version with essential columns (first 10 rows):\")\n",
    "print(df_clean.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c5da15-a78f-4b32-94c2-aeec7c2d3478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean.csv\n",
      "Error loading incidents_2012_updated.json: File incidents_2012_updated.json does not exist\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File incidents_2012_updated.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError loading incidents_2012_updated.json:\u001b[39m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Ensure Last_Report_Date column exists in df_clean; if not, create it\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLast_Report_Date\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df_clean\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load the updated incidents_2012 JSON file\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     df_2012 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincidents_2012_updated.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded incidents_2012_updated.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m JsonReader(\n\u001b[0;32m    792\u001b[0m     path_or_buf,\n\u001b[0;32m    793\u001b[0m     orient\u001b[38;5;241m=\u001b[39morient,\n\u001b[0;32m    794\u001b[0m     typ\u001b[38;5;241m=\u001b[39mtyp,\n\u001b[0;32m    795\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    796\u001b[0m     convert_axes\u001b[38;5;241m=\u001b[39mconvert_axes,\n\u001b[0;32m    797\u001b[0m     convert_dates\u001b[38;5;241m=\u001b[39mconvert_dates,\n\u001b[0;32m    798\u001b[0m     keep_default_dates\u001b[38;5;241m=\u001b[39mkeep_default_dates,\n\u001b[0;32m    799\u001b[0m     precise_float\u001b[38;5;241m=\u001b[39mprecise_float,\n\u001b[0;32m    800\u001b[0m     date_unit\u001b[38;5;241m=\u001b[39mdate_unit,\n\u001b[0;32m    801\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    802\u001b[0m     lines\u001b[38;5;241m=\u001b[39mlines,\n\u001b[0;32m    803\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    804\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    805\u001b[0m     nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[0;32m    806\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    807\u001b[0m     encoding_errors\u001b[38;5;241m=\u001b[39mencoding_errors,\n\u001b[0;32m    808\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    809\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    810\u001b[0m )\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    959\u001b[0m ):\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    968\u001b[0m     )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File incidents_2012_updated.json does not exist"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e204dd-a107-4d6f-b38b-321d20058f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
