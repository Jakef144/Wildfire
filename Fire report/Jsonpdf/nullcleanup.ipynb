{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16be0745-e5d5-4e82-b03d-b56441e30a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean_final_updated.csv with 470 rows\n",
      "Number of 2021 incidents in CSV: 24\n",
      "Extracted 24 date values from incidents_2021.json\n",
      "Sample of extracted dates:\n",
      "dixie: 10/23\n",
      "bootleg: 8/13\n",
      "monument: 10/25\n",
      "caldor: 10/20\n",
      "river complex: 10/24\n",
      "Updated 24 rows with dates from incidents_2021.json\n",
      "Remaining null values in Last_Report_Date: 0\n",
      "Saved updated CSV to combined_incidents_clean_final_updated_2021.csv\n",
      "Sample of updated 2021 incidents:\n",
      "              Name    Year  Start_Date Last_Report_Date\n",
      "315          Dixie  2021.0  07/13/2021       10/23/2021\n",
      "316        Bootleg  2021.0  07/06/2021       08/13/2021\n",
      "317       Monument  2021.0  07/31/2021       10/25/2021\n",
      "318         Caldor  2021.0  08/14/2021       10/20/2021\n",
      "319  River Complex  2021.0  07/30/2021       10/24/2021\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Let's extract the date information from incidents_2021.json and update our CSV\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the JSON file\n",
    "json_file = 'incidents_2021.json'\n",
    "with open(json_file, 'r') as f:\n",
    "    incidents_2021 = json.load(f)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'combined_incidents_clean_final_updated.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Loaded {csv_file} with {len(df)} rows\")\n",
    "\n",
    "# Check how many 2021 incidents are in our CSV\n",
    "incidents_2021_count = len(df[df['Year'] == 2021])\n",
    "print(f\"Number of 2021 incidents in CSV: {incidents_2021_count}\")\n",
    "\n",
    "# Create a dictionary to map incident names to their dates from the JSON\n",
    "name_to_date = {}\n",
    "for incident in incidents_2021:\n",
    "    # The field name has spaces in it\n",
    "    name = incident.get('Name', '').strip()\n",
    "    date_field = 'Contain or Last_Report Date'  # This is the actual field name in the JSON\n",
    "    date_value = incident.get(date_field, '').strip()\n",
    "    \n",
    "    if name and date_value:\n",
    "        name_to_date[name.lower()] = date_value\n",
    "\n",
    "print(f\"Extracted {len(name_to_date)} date values from {json_file}\")\n",
    "print(\"Sample of extracted dates:\")\n",
    "sample_items = list(name_to_date.items())[:5]\n",
    "for name, date in sample_items:\n",
    "    print(f\"{name}: {date}\")\n",
    "\n",
    "# Function to format date with year\n",
    "def format_date_with_year(date_str, year):\n",
    "    if not date_str or pd.isna(date_str):\n",
    "        return None\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # Handle format like \"10/23\" by adding the year\n",
    "    if '/' in date_str and len(date_str) <= 5:\n",
    "        date_str = f\"{date_str}/{year}\"\n",
    "    \n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "        return date_obj.strftime('%m/%d/%Y')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Try other formats\n",
    "            formats = ['%m/%d/%y', '%d-%b-%y', '%d-%b-%Y']\n",
    "            for fmt in formats:\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, fmt)\n",
    "                    return date_obj.strftime('%m/%d/%Y')\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # If all formats fail, try pandas to_datetime\n",
    "            date_obj = pd.to_datetime(date_str)\n",
    "            return date_obj.strftime('%m/%d/%Y')\n",
    "        except:\n",
    "            print(f\"Could not parse date: {date_str}\")\n",
    "            return None\n",
    "\n",
    "# Update the Last_Report_Date column for 2021 incidents\n",
    "updated_count = 0\n",
    "for idx, row in df[df['Year'] == 2021].iterrows():\n",
    "    name = row['Name'].lower().strip() if isinstance(row['Name'], str) else ''\n",
    "    \n",
    "    if name in name_to_date:\n",
    "        date_value = name_to_date[name]\n",
    "        formatted_date = format_date_with_year(date_value, 2021)\n",
    "        \n",
    "        if formatted_date and (pd.isna(row['Last_Report_Date']) or row['Last_Report_Date'] == ''):\n",
    "            df.at[idx, 'Last_Report_Date'] = formatted_date\n",
    "            updated_count += 1\n",
    "\n",
    "print(f\"Updated {updated_count} rows with dates from {json_file}\")\n",
    "\n",
    "# Count remaining null values in Last_Report_Date\n",
    "null_last_report = df['Last_Report_Date'].isna().sum()\n",
    "print(f\"Remaining null values in Last_Report_Date: {null_last_report}\")\n",
    "\n",
    "# Save the updated CSV\n",
    "output_file = 'combined_incidents_clean_final_updated_2021.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved updated CSV to {output_file}\")\n",
    "\n",
    "# Show a sample of the updated 2021 incidents\n",
    "print(\"Sample of updated 2021 incidents:\")\n",
    "sample = df[df['Year'] == 2021].head(5)\n",
    "print(sample[['Name', 'Year', 'Start_Date', 'Last_Report_Date']])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b88ed14-92dd-4cbd-acd8-63fb4de22c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean_final_updated_2021.csv with 470 rows\n",
      "Updated Last_Report_Date for 10 rows\n",
      "Saved updated CSV to combined_incidents_clean_final_updated_2021_v2.csv\n",
      "Sample rows after converting Last_Report_Date:\n",
      "                 Name  Start_Date Last_Report_Date\n",
      "0     Buzzard Complex  07/14/2014       09/11/2014\n",
      "1     Carlton Complex  07/14/2014       08/28/2014\n",
      "2         Funny River  05/19/2014       08/14/2014\n",
      "3  Happy Camp Complex  08/14/2014       12/04/2014\n",
      "4                King  09/13/2014       10/09/2014\n",
      "5               Skunk  04/19/2014       06/26/2014\n",
      "6          Big Cougar  08/02/2014       08/22/2014\n",
      "7        July Complex  08/03/2014       09/25/2014\n",
      "8       Shaniko Butte  07/12/2014       09/25/2014\n",
      "9          Glass Fire  02/25/2008       03/02/2008\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Convert Last_Report_Date into mm/dd/year format\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the updated CSV file\n",
    "csv_file = 'combined_incidents_clean_final_updated_2021.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Loaded {csv_file} with {len(df)} rows\")\n",
    "\n",
    "# Define a function to convert a Last_Report_Date value to mm/dd/yyyy\n",
    "# If the value is just a month name, assume it's the first day of that month and year from Start_Date\n",
    "\n",
    "def convert_last_report_date(lr_date, start_date):\n",
    "    # Strip and check value\n",
    "    if pd.isna(lr_date) or lr_date == '':\n",
    "        return None\n",
    "    lr_date_str = str(lr_date).strip()\n",
    "    \n",
    "    # Try known date formats\n",
    "    for fmt in ['%m/%d/%Y', '%m/%d/%y', '%b %d, %Y', '%B %d, %Y']:\n",
    "        try:\n",
    "            dt = datetime.strptime(lr_date_str, fmt)\n",
    "            return dt.strftime('%m/%d/%Y')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Check if the string is just a month name or month abbreviation (e.g., 'August' or 'Aug')\n",
    "    try:\n",
    "        # Try to parse with full month name\n",
    "        dt = datetime.strptime(lr_date_str, '%B')\n",
    "    except:\n",
    "        try:\n",
    "            dt = datetime.strptime(lr_date_str, '%b')\n",
    "        except Exception as e:\n",
    "            # If it fails, then return the original value\n",
    "            return lr_date_str\n",
    "    \n",
    "    # If we got the month, then use the year from start_date\n",
    "    if pd.isna(start_date) or start_date == '':\n",
    "        year = datetime.now().year\n",
    "    else:\n",
    "        # Expect start_date to be in mm/dd/yyyy or similar format.\n",
    "        try:\n",
    "            dt_start = datetime.strptime(start_date, '%m/%d/%Y')\n",
    "            year = dt_start.year\n",
    "        except Exception as e:\n",
    "            year = datetime.now().year\n",
    "    \n",
    "    # Use the month from dt, day = 1, and the year from start_date\n",
    "    new_date = datetime(year, dt.month, 1)\n",
    "    return new_date.strftime('%m/%d/%Y')\n",
    "\n",
    "# Let's apply this function on our dataframe and update Last_Report_Date\n",
    "\n",
    "updated_dates = 0\n",
    "for idx, row in df.iterrows():\n",
    "    original_lr = row['Last_Report_Date']\n",
    "    start_date = row['Start_Date']\n",
    "    new_lr = convert_last_report_date(original_lr, start_date)\n",
    "    # if conversion returns a different value, update\n",
    "    if new_lr and new_lr != original_lr:\n",
    "        df.at[idx, 'Last_Report_Date'] = new_lr\n",
    "        updated_dates += 1\n",
    "\n",
    "print(f\"Updated Last_Report_Date for {updated_dates} rows\")\n",
    "\n",
    "# Save the updated CSV\n",
    "output_file = 'combined_incidents_clean_final_updated_2021_v2.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved updated CSV to {output_file}\")\n",
    "\n",
    "# Show a sample of the updated data\n",
    "print(\"Sample rows after converting Last_Report_Date:\")\n",
    "print(df[['Name', 'Start_Date', 'Last_Report_Date']].head(10))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7a5cb7-a0fa-44e4-961c-e663cba709c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean_final_updated_2021_v2.csv with 470 rows\n",
      "Found 2 unique date formats:\n",
      "- dd-MMM-yy\n",
      "- mm/dd/yyyy\n",
      "Found 10 non-standard date formats (showing up to 10):\n",
      "Row 122: Long Butte - 3-Sep-10\n",
      "Row 123: Toklat - 4-Jun-10\n",
      "Row 124: Jefferson - 17-Jul-10\n",
      "Row 125: Turquoise Lake - 7-Sep-10\n",
      "Row 126: Big Mountain - 2-Sep-10\n",
      "Row 127: Pat Creek - 30-Dec-10\n",
      "Row 128: Lone Mountain - 22-Dec-10\n",
      "Row 129: Twitchell Canyon - 18-Nov-10\n",
      "Row 130: Bull Creek - 25-Jun-10\n",
      "Row 402: Wallow - 8-Jul-11\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Check the current date formats in the Last_Report_Date column\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'combined_incidents_clean_final_updated_2021_v2.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Loaded {csv_file} with {len(df)} rows\")\n",
    "\n",
    "# Check unique formats in Last_Report_Date\n",
    "unique_formats = set()\n",
    "for date_str in df['Last_Report_Date'].dropna().unique():\n",
    "    # Try to identify the format\n",
    "    if re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', str(date_str)):\n",
    "        unique_formats.add('mm/dd/yyyy')\n",
    "    elif re.match(r'\\d{1,2}/\\d{1,2}/\\d{2}', str(date_str)):\n",
    "        unique_formats.add('mm/dd/yy')\n",
    "    elif re.match(r'\\d{1,2}-[A-Za-z]{3}-\\d{2}', str(date_str)):\n",
    "        unique_formats.add('dd-MMM-yy')\n",
    "    elif re.match(r'\\d{1,2}-[A-Za-z]{3}-\\d{4}', str(date_str)):\n",
    "        unique_formats.add('dd-MMM-yyyy')\n",
    "    elif re.match(r'[A-Za-z]{3,9} \\d{1,2}, \\d{4}', str(date_str)):\n",
    "        unique_formats.add('Month dd, yyyy')\n",
    "    else:\n",
    "        unique_formats.add(f'Other: {date_str}')\n",
    "\n",
    "print(f\"Found {len(unique_formats)} unique date formats:\")\n",
    "for fmt in unique_formats:\n",
    "    print(f\"- {fmt}\")\n",
    "\n",
    "# Sample of dates that might not be in mm/dd/yyyy format\n",
    "non_standard_dates = []\n",
    "for idx, row in df.iterrows():\n",
    "    date_str = str(row['Last_Report_Date'])\n",
    "    if not pd.isna(row['Last_Report_Date']) and not re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', date_str):\n",
    "        non_standard_dates.append((idx, row['Name'], date_str))\n",
    "        if len(non_standard_dates) >= 10:\n",
    "            break\n",
    "\n",
    "print(f\"\\\n",
    "Found {len(non_standard_dates)} non-standard date formats (showing up to 10):\")\n",
    "for idx, name, date_str in non_standard_dates:\n",
    "    print(f\"Row {idx}: {name} - {date_str}\")\n",
    "\n",
    "print(\"\\\n",
    "Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88e87ee-77f4-49b3-b07e-c47f7ab61e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean_final_updated_2021_v2.csv with 470 rows\n",
      "Sample of converted Last_Report_Date values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakef\\AppData\\Local\\Temp\\ipykernel_19764\\1822490875.py:15: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(date_str, infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\jakef\\AppData\\Local\\Temp\\ipykernel_19764\\1822490875.py:29: FutureWarning: Series.bool is now deprecated and will be removed in future version of pandas\n",
      "  sample_non_standard = df.loc[df['Last_Report_Date'].apply(lambda x: not pd.isna(x) and not pd.Series(x).str.match(r'\\d{1,2}/\\d{1,2}/\\d{4}').bool()), ['Name', 'Last_Report_Date']].head(10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Last_Report_Date]\n",
      "Index: []\n",
      "Saved updated CSV to combined_incidents_clean_final_updated_2021_v3.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Convert all dates in Last_Report_Date column to mm/dd/yyyy format using pandas to_datetime\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'combined_incidents_clean_final_updated_2021_v2.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Loaded {csv_file} with {len(df)} rows\")\n",
    "\n",
    "# Function to convert a date string to mm/dd/yyyy using pd.to_datetime\n",
    "\n",
    "def convert_to_mmddyyyy(date_str):\n",
    "    try:\n",
    "        # Some dates are in non-standard format, let pandas infer\n",
    "        dt = pd.to_datetime(date_str, infer_datetime_format=True, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            return date_str\n",
    "        return dt.strftime('%m/%d/%Y')\n",
    "    except Exception as e:\n",
    "        return date_str\n",
    "\n",
    "# Apply conversion to Last_Report_Date column\n",
    "# We'll update the column using the conversion function for every non-null entry\n",
    "\n",
    "df['Last_Report_Date'] = df['Last_Report_Date'].apply(lambda x: convert_to_mmddyyyy(x) if pd.notna(x) and x != '' else x)\n",
    "\n",
    "# Verify conversion by printing some rows with non-standard values\n",
    "print(\"Sample of converted Last_Report_Date values:\")\n",
    "sample_non_standard = df.loc[df['Last_Report_Date'].apply(lambda x: not pd.isna(x) and not pd.Series(x).str.match(r'\\d{1,2}/\\d{1,2}/\\d{4}').bool()), ['Name', 'Last_Report_Date']].head(10)\n",
    "print(sample_non_standard.to_string(index=False))\n",
    "\n",
    "# Save the updated CSV\n",
    "output_file = 'combined_incidents_clean_final_updated_2021_v3.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\\n",
    "Saved updated CSV to {output_file}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc5bcfc-41aa-4994-be63-d1af3317856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cost information for 27 incidents in incidents_2017.json\n",
      "Sample of cost information:\n",
      "  NW_Oklahoma_Complex: 3,200,000\n",
      "  Thomas: 123,836,000\n",
      "  Lodgepole_Complex: 9,800,000\n",
      "  Roosters_Comb: 4,000,000\n",
      "  Chetco_Bar: 72,000,000\n",
      "Updated cost information for 7 incidents in the CSV\n",
      "Number of 2017 incidents with null Cost after update: 23\n",
      "Sample of updated 2017 incidents from CSV:\n",
      "                     Name    Year         Cost\n",
      "32   NW Oklahoma\\nComplex  2017.0          NaN\n",
      "33               Perryton  2017.0          NaN\n",
      "34                 Thomas  2017.0  123,836,000\n",
      "35     Lodgepole\\nComplex  2017.0          NaN\n",
      "36          Roosters Comb  2017.0          NaN\n",
      "37             Chetco Bar  2017.0          NaN\n",
      "38             Rice Ridge  2017.0          NaN\n",
      "39  Four Seasons\\nComplex  2017.0          NaN\n",
      "40              West Mims  2017.0          NaN\n",
      "41            Lefors East  2017.0          NaN\n",
      "Saved updated CSV to combined_incidents_clean_final_updated_2021_v5.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Update the CSV with cost information from incidents_2017.json\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the incidents_2017.json file\n",
    "json_file = 'incidents_2017.json'\n",
    "with open(json_file, 'r') as f:\n",
    "    incidents_2017 = json.load(f)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'combined_incidents_clean_final_updated_2021_v4.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create a dictionary mapping incident names to their estimated costs\n",
    "cost_dict = {}\n",
    "for incident in incidents_2017:\n",
    "    name = incident.get('Name', '').strip()\n",
    "    # Handle the case where name might have newlines or other whitespace\n",
    "    name = ' '.join(name.split())\n",
    "    # Replace spaces with underscores to match CSV format\n",
    "    name = name.replace(' ', '_')\n",
    "    \n",
    "    estimated_cost = incident.get('Estimated Cost', '')\n",
    "    if estimated_cost and estimated_cost != 'NR':\n",
    "        cost_dict[name] = estimated_cost\n",
    "\n",
    "print(f\"Found cost information for {len(cost_dict)} incidents in {json_file}\")\n",
    "print(\"Sample of cost information:\")\n",
    "for i, (name, cost) in enumerate(list(cost_dict.items())[:5]):\n",
    "    print(f\"  {name}: {cost}\")\n",
    "\n",
    "# Update the Cost column in the CSV for 2017 incidents\n",
    "updated_count = 0\n",
    "for name, cost in cost_dict.items():\n",
    "    # Find matching rows in the dataframe (2017 incidents with matching name)\n",
    "    mask = (df['Year'] == 2017) & (df['Name'] == name)\n",
    "    if mask.any():\n",
    "        df.loc[mask, 'Cost'] = cost\n",
    "        updated_count += 1\n",
    "    else:\n",
    "        # Try a more flexible match (in case of slight name differences)\n",
    "        for csv_name in df.loc[df['Year'] == 2017, 'Name'].unique():\n",
    "            if name.lower() in csv_name.lower() or csv_name.lower() in name.lower():\n",
    "                mask = (df['Year'] == 2017) & (df['Name'] == csv_name)\n",
    "                df.loc[mask, 'Cost'] = cost\n",
    "                updated_count += 1\n",
    "                print(f\"  Matched {name} to {csv_name}\")\n",
    "                break\n",
    "\n",
    "print(f\"Updated cost information for {updated_count} incidents in the CSV\")\n",
    "\n",
    "# Check how many 2017 incidents still have null Cost\n",
    "df_2017 = df[df['Year'] == 2017]\n",
    "null_cost_count = df_2017['Cost'].isna().sum()\n",
    "print(f\"Number of 2017 incidents with null Cost after update: {null_cost_count}\")\n",
    "\n",
    "# Show the updated 2017 incidents from the CSV\n",
    "print(\"\\\n",
    "Sample of updated 2017 incidents from CSV:\")\n",
    "print(df_2017[['Name', 'Year', 'Cost']].head(10))\n",
    "\n",
    "# Save the updated CSV\n",
    "output_file = 'combined_incidents_clean_final_updated_2021_v5.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\\n",
    "Saved updated CSV to {output_file}\")\n",
    "\n",
    "print(\"\\\n",
    "Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ed3d40d-b78c-4604-99ab-08731ee29943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabula-py\n",
      "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (2.2.2)\n",
      "Requirement already satisfied: numpy>1.24.4 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (1.26.4)\n",
      "Requirement already satisfied: distro in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from tabula-py) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jakef\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
      "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.0 MB 656.4 kB/s eta 0:00:19\n",
      "   - -------------------------------------- 0.3/12.0 MB 2.1 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/12.0 MB 4.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/12.0 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.2/12.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 31.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tabula-py\n",
      "Successfully installed tabula-py-2.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'annual_report_2017_508_0.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9-10\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# pages 9 and 10  \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Use tabula to read the table from the specified pages  \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# This will return a list of DataFrames  \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m tables \u001b[38;5;241m=\u001b[39m tabula\u001b[38;5;241m.\u001b[39mread_pdf(pdf_path, pages\u001b[38;5;241m=\u001b[39mpages, multiple_tables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Let's see how many tables were extracted and show a preview of the first few rows of each  \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tables detected:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tables))  \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tabula\\io.py:394\u001b[0m, in \u001b[0;36mread_pdf\u001b[1;34m(input_path, output_format, encoding, java_options, pandas_options, multiple_tables, user_agent, use_raw_url, pages, guess, area, relative_area, lattice, stream, password, silent, columns, relative_columns, format, batch, output_path, force_subprocess, options)\u001b[0m\n\u001b[0;32m    391\u001b[0m path, temporary \u001b[38;5;241m=\u001b[39m localize_file(input_path, user_agent, use_raw_url\u001b[38;5;241m=\u001b[39muse_raw_url)\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mENOENT, os\u001b[38;5;241m.\u001b[39mstrerror(errno\u001b[38;5;241m.\u001b[39mENOENT), path)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is empty. Check the file, or download it manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'annual_report_2017_508_0.pdf'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea7442-a505-4749-a93d-f4b1b58d2430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69518ad-73f3-44a5-a6ed-42ac3e8eed41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a36d0-6b8e-45e3-9680-f810c28cd25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565de15-e8bf-4b97-86d0-63d6f692094c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d404965-6a19-4b29-864c-1bb11bdd5322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
