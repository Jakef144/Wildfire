{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21796d02-076c-4e91-907c-ec694fc5f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries  \n",
    "import PyPDF2\n",
    "import re  \n",
    "import json  \n",
    "import pandas as pd  \n",
    "from datetime import datetime  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2e8def8-f0b2-4901-b17e-602029568b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Text saved to annual_report_2009_text.txt\n",
      "\n",
      "Processing text to extract incident data...\n",
      "Found incidents section. Length: 32267\n",
      "First 500 characters: large fires were \n",
      "rapidly contained. By the end of February, the Southern Area was very active with numerous large fires mostly in Texas, Oklahoma, and Florida. The Southern Area had 7,424 fires (133 percent  of normal) during January and Februar y, which burned 136,020 acres (118 percent  of \n",
      "normal) for the same time period.  \n",
      " \n",
      "Spring (March â€“  May)  \n",
      " \n",
      "Spring was warmer than normal in Alaska as well as the Southwest and Northeast quarters of \n",
      "the country. After a dry winter, the Southeast ex\n",
      "No table matches found with the specific pattern. Trying alternative pattern.\n",
      "Still no matches. Trying line-by-line approach.\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": [
    "# Convert PDF to text  \n",
    "def extract_text_from_pdf(pdf_path):  \n",
    "    text = \"\"  \n",
    "    with open(pdf_path, 'rb') as file:  \n",
    "        pdf_reader = PyPDF2.PdfReader(file)  \n",
    "        for page_num in range(len(pdf_reader.pages)):  \n",
    "            page = pdf_reader.pages[page_num]  \n",
    "            text += page.extract_text() + \"\\n\\n\"  \n",
    "    return text  \n",
    "  \n",
    "# Save text to file  \n",
    "def save_text_to_file(text, output_path):  \n",
    "    with open(output_path, 'w', encoding='utf-8') as file:  \n",
    "        file.write(text)  \n",
    "    print(f\"Text saved to {output_path}\")  \n",
    "  \n",
    "# Extract text from PDF  \n",
    "pdf_path = 'annual_report_2009_508.pdf'  \n",
    "output_text_path = 'annual_report_2009_text.txt'  \n",
    "  \n",
    "print(\"Extracting text from PDF...\")  \n",
    "extracted_text = extract_text_from_pdf(pdf_path)  \n",
    "save_text_to_file(extracted_text, output_text_path)  \n",
    "  \n",
    "# Step 2: Process the text to extract incident data  \n",
    "print(\"\\nProcessing text to extract incident data...\")  \n",
    "  \n",
    "# Read the text file  \n",
    "with open(output_text_path, 'r', encoding='utf-8') as file:  \n",
    "    text_content = file.read()  \n",
    "  \n",
    "# Look for the significant incidents section  \n",
    "# This pattern might need adjustment based on the actual format  \n",
    "incidents_section_pattern = r\"(?:Significant\\s+Incidents|Large\\s+Fires|Major\\s+Incidents).*?(?=\\n\\n\\w+|\\Z)\"  \n",
    "incidents_section_match = re.search(incidents_section_pattern, text_content, re.DOTALL | re.IGNORECASE)  \n",
    "  \n",
    "if incidents_section_match:  \n",
    "    incidents_section = incidents_section_match.group(0)  \n",
    "    print(\"Found incidents section. Length:\", len(incidents_section))  \n",
    "    print(\"First 500 characters:\", incidents_section[:500])  \n",
    "else:  \n",
    "    print(\"Incidents section not found. Looking for table patterns directly.\")  \n",
    "    incidents_section = text_content  \n",
    "  \n",
    "# Look for table patterns in the incidents section or full text  \n",
    "# This pattern looks for lines that might be table rows with fire data  \n",
    "table_pattern = r\"([A-Za-z\\s\\-\\.&]+)\\s+(\\w+)\\s+(\\w+)\\s+(\\w+)\\s+(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,3}(?:,\\d{3})*)\\s+([A-Z])\"  \n",
    "table_matches = re.findall(table_pattern, incidents_section)  \n",
    "  \n",
    "if not table_matches:  \n",
    "    print(\"No table matches found with the specific pattern. Trying alternative pattern.\")  \n",
    "    # Try an alternative pattern that's more flexible  \n",
    "    table_pattern = r\"([A-Za-z\\s\\-\\.&]+?)\\s+(\\w+)\\s+(\\w+)\\s+(\\w+)\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,3}(?:,\\d{3})*)\\s+([A-Z])\"  \n",
    "    table_matches = re.findall(table_pattern, incidents_section)  \n",
    "  \n",
    "# If still no matches, try a more general approach  \n",
    "if not table_matches:  \n",
    "    print(\"Still no matches. Trying line-by-line approach.\")  \n",
    "    lines = incidents_section.split('\\n')  \n",
    "    table_matches = []  \n",
    "      \n",
    "    # Look for lines that might contain fire data  \n",
    "    for line in lines:  \n",
    "        # This pattern looks for lines with dates and numbers that might be fire data  \n",
    "        if re.search(r'\\d{1,2}/\\d{1,2}/\\d{2,4}.*\\d{1,2}/\\d{1,2}/\\d{2,4}.*\\d{1,3}(?:,\\d{3})*', line):  \n",
    "            # Try to parse the line into components  \n",
    "            parts = re.split(r'\\s{2,}', line.strip())  \n",
    "            if len(parts) >= 8:  # Assuming at least 8 columns  \n",
    "                # Extract the components  \n",
    "                name = parts[0].strip()  \n",
    "                inc_type = parts[1].strip() if len(parts) > 1 else \"\"  \n",
    "                gacc = parts[2].strip() if len(parts) > 2 else \"\"  \n",
    "                state = parts[3].strip() if len(parts) > 3 else \"\"  \n",
    "                start_date = parts[4].strip() if len(parts) > 4 else \"\"  \n",
    "                contain_date = parts[5].strip() if len(parts) > 5 else \"\"  \n",
    "                size_acres = parts[6].strip() if len(parts) > 6 else \"\"  \n",
    "                cause = parts[7].strip() if len(parts) > 7 else \"\"  \n",
    "                  \n",
    "                table_matches.append((name, inc_type, gacc, state, start_date, contain_date, size_acres, cause))  \n",
    "  \n",
    "# Step 3: Structure the extracted data  \n",
    "significant_incidents = []  \n",
    "  \n",
    "if table_matches:  \n",
    "    for match in table_matches:  \n",
    "        name, inc_type, gacc, state, start_date, contain_date, size_acres, cause = match  \n",
    "          \n",
    "        # Clean up the data  \n",
    "        name = name.strip()  \n",
    "        size_acres = size_acres.replace(',', '')  # Remove commas from numbers  \n",
    "          \n",
    "        # Create a dictionary for this incident  \n",
    "        incident = {  \n",
    "            'Name': name,  \n",
    "            'Inc_Type': inc_type,  \n",
    "            'GACC': gacc,  \n",
    "            'State': state,  \n",
    "            'Start_Date': start_date,  \n",
    "            'Contain_Control_Date': contain_date,  \n",
    "            'Size_Acres': size_acres,  \n",
    "            'Cause': cause,  \n",
    "            'Cause_Description': 'Human-caused' if cause == 'H' else 'Lightning' if cause == 'L' else 'Unknown'  \n",
    "        }  \n",
    "          \n",
    "        significant_incidents.append(incident)  \n",
    "      \n",
    "    # Step 4: Create dimension tables for the star schema  \n",
    "    # State dimension  \n",
    "    states = list(set(incident['State'] for incident in significant_incidents))  \n",
    "    state_dim = [{'State_Code': state, 'Region': 'Unknown'} for state in states]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = list(set(incident['GACC'] for incident in significant_incidents))  \n",
    "    gacc_dim = [{'GACC_Code': gacc, 'GACC_Name': 'Unknown'} for gacc in gaccs]  \n",
    "      \n",
    "    # Time dimension  \n",
    "    time_dim = []  \n",
    "    for i, incident in enumerate(significant_incidents):  \n",
    "        date_str = incident.get('Start_Date', 'Unknown')  \n",
    "          \n",
    "        # Try to parse the date  \n",
    "        if date_str != 'Unknown':  \n",
    "            try:  \n",
    "                # Handle different date formats  \n",
    "                if '/' in date_str:  \n",
    "                    # Format: MM/DD/YYYY or MM/DD/YY  \n",
    "                    parts = date_str.split('/')  \n",
    "                    if len(parts[2]) == 2:  # Two-digit year  \n",
    "                        parts[2] = '20' + parts[2]  # Assume 2000s  \n",
    "                    date_obj = datetime(int(parts[2]), int(parts[0]), int(parts[1]))  \n",
    "                    month = date_obj.month  \n",
    "                    year = date_obj.year  \n",
    "                    formatted_date = date_obj.strftime('%Y-%m-%d')  \n",
    "                else:  \n",
    "                    # Try standard datetime parsing  \n",
    "                    date_obj = datetime.strptime(date_str, '%Y-%m-%d')  \n",
    "                    month = date_obj.month  \n",
    "                    year = date_obj.year  \n",
    "                    formatted_date = date_str  \n",
    "            except ValueError:  \n",
    "                month = 0  \n",
    "                year = 0  \n",
    "                formatted_date = date_str  \n",
    "        else:  \n",
    "            month = 0  \n",
    "            year = 0  \n",
    "            formatted_date = date_str  \n",
    "          \n",
    "        time_dim.append({  \n",
    "            'Date_ID': i + 1,  \n",
    "            'Date': formatted_date,  \n",
    "            'Month': month,  \n",
    "            'Year': year  \n",
    "        })  \n",
    "      \n",
    "    # Step 5: Save the structured data to JSON files  \n",
    "    structured_data = {  \n",
    "        'significant_incidents': significant_incidents  \n",
    "    }  \n",
    "      \n",
    "    dimensions = {  \n",
    "        'state_dimension': state_dim,  \n",
    "        'gacc_dimension': gacc_dim,  \n",
    "        'time_dimension': time_dim  \n",
    "    }  \n",
    "      \n",
    "    with open('structured_incidents_data_2009.json', 'w') as f:  \n",
    "        json.dump(structured_data, f, indent=4)  \n",
    "      \n",
    "    with open('dimension_tables_2009.json', 'w') as f:  \n",
    "        json.dump(dimensions, f, indent=4)  \n",
    "      \n",
    "    print(\"Data successfully extracted and saved to JSON files.\")  \n",
    "    print(f\"Found {len(significant_incidents)} significant incidents.\")  \n",
    "    print(f\"Created dimension tables: States ({len(state_dim)}), GACCs ({len(gacc_dim)}), Time ({len(time_dim)}).\")  \n",
    "      \n",
    "    # Display a sample of the extracted data  \n",
    "    print(\"\\nSample of extracted incidents (first 5):\")  \n",
    "    for i, incident in enumerate(significant_incidents[:5]):  \n",
    "        print(f\"{i+1}. {incident['Name']} - {incident['State']} - {incident['Start_Date']} - {incident['Size_Acres']} acres\")  \n",
    "else:  \n",
    "    print(\"No data to save.\")  \n",
    "  \n",
    "# Optional: If you want to see the data in a DataFrame format  \n",
    "if significant_incidents:  \n",
    "    df = pd.DataFrame(significant_incidents)  \n",
    "    print(\"\\nDataFrame view of the first 5 incidents:\")  \n",
    "    print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ba7361d-9d66-4098-be72-925376a04282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found incidents section. Length: 32267\n",
      "First 500 characters: large fires were \n",
      "rapidly contained. By the end of February, the Southern Area was very active with numerous large fires mostly in Texas, Oklahoma, and Florida. The Southern Area had 7,424 fires (133 percent  of normal) during January and Februar y, which burned 136,020 acres (118 percent  of \n",
      "normal) for the same time period.  \n",
      " \n",
      "Spring (March â€“  May)  \n",
      " \n",
      "Spring was warmer than normal in Alaska as well as the Southwest and Northeast quarters of \n",
      "the country. After a dry winter, the Southeast ex\n",
      "No table matches found with the specific pattern. Trying alternative pattern.\n",
      "Still no matches. Trying line-by-line approach.\n",
      "No incident data found. Check the text file format and adjust the extraction patterns.\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": [
    "# Read the text file  \n",
    "with open('annual_report_2009_text.txt', 'r', encoding='utf-8') as file:  \n",
    "    text_content = file.read()  \n",
    "  \n",
    "# Step 2: Extract the significant incidents table  \n",
    "# We'll look for patterns that indicate the start and end of the table  \n",
    "# This might need adjustment based on the actual format in the text file  \n",
    "  \n",
    "# First, let's try to find the section with significant incidents  \n",
    "# Look for a section header like \"Significant Incidents\" or similar  \n",
    "incidents_section_pattern = r\"(?:Significant\\s+Incidents|Large\\s+Fires|Major\\s+Incidents).*?(?=\\n\\n\\w+|\\Z)\"  \n",
    "incidents_section_match = re.search(incidents_section_pattern, text_content, re.DOTALL | re.IGNORECASE)  \n",
    "  \n",
    "if incidents_section_match:  \n",
    "    incidents_section = incidents_section_match.group(0)  \n",
    "    print(\"Found incidents section. Length:\", len(incidents_section))  \n",
    "    print(\"First 500 characters:\", incidents_section[:500])  \n",
    "else:  \n",
    "    print(\"Incidents section not found. Looking for table patterns directly.\")  \n",
    "    incidents_section = text_content  \n",
    "  \n",
    "# Look for table patterns in the incidents section or full text  \n",
    "# This pattern looks for lines that might be table rows with fire data  \n",
    "# Adjust the pattern based on the actual format in your text  \n",
    "table_pattern = r\"([A-Za-z\\s\\-\\.&]+)\\s+(\\w+)\\s+(\\w+)\\s+(\\w+)\\s+(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,2}/\\d{1,2}/\\d{4})\\s+(\\d{1,3}(?:,\\d{3})*)\\s+([A-Z])\"  \n",
    "table_matches = re.findall(table_pattern, incidents_section)  \n",
    "  \n",
    "if not table_matches:  \n",
    "    print(\"No table matches found with the specific pattern. Trying alternative pattern.\")  \n",
    "    # Try an alternative pattern that's more flexible  \n",
    "    table_pattern = r\"([A-Za-z\\s\\-\\.&]+?)\\s+(\\w+)\\s+(\\w+)\\s+(\\w+)\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,3}(?:,\\d{3})*)\\s+([A-Z])\"  \n",
    "    table_matches = re.findall(table_pattern, incidents_section)  \n",
    "  \n",
    "# If still no matches, try a more general approach  \n",
    "if not table_matches:  \n",
    "    print(\"Still no matches. Trying line-by-line approach.\")  \n",
    "    lines = incidents_section.split('\\n')  \n",
    "    table_data = []  \n",
    "      \n",
    "    # Look for lines that might contain fire data  \n",
    "    for line in lines:  \n",
    "        # Skip empty lines and headers  \n",
    "        if not line.strip() or re.match(r'^(Name|Type|GACC|State|Date|Size|Cause)', line, re.IGNORECASE):  \n",
    "            continue  \n",
    "          \n",
    "        # Check if line contains date patterns and numbers that might be acres  \n",
    "        if re.search(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', line) and re.search(r'\\d{1,3}(?:,\\d{3})*', line):  \n",
    "            # Split the line by multiple spaces to separate columns  \n",
    "            columns = re.split(r'\\s{2,}', line.strip())  \n",
    "            if len(columns) >= 7:  # Assuming at least 7 columns for a valid row  \n",
    "                table_data.append(columns)  \n",
    "      \n",
    "    if table_data:  \n",
    "        print(f\"Found {len(table_data)} potential incident rows using line-by-line approach.\")  \n",
    "        # Convert to the format expected by the rest of the code  \n",
    "        table_matches = []  \n",
    "        for row in table_data:  \n",
    "            # Ensure we have enough columns, pad with empty strings if needed  \n",
    "            while len(row) < 8:  \n",
    "                row.append(\"\")  \n",
    "            # Extract the columns we need  \n",
    "            name = row[0]  \n",
    "            inc_type = row[1] if len(row) > 1 else \"\"  \n",
    "            gacc = row[2] if len(row) > 2 else \"\"  \n",
    "            state = row[3] if len(row) > 3 else \"\"  \n",
    "            start_date = row[4] if len(row) > 4 else \"\"  \n",
    "            contain_date = row[5] if len(row) > 5 else \"\"  \n",
    "            size_acres = row[6] if len(row) > 6 else \"\"  \n",
    "            cause = row[7] if len(row) > 7 else \"\"  \n",
    "              \n",
    "            table_matches.append((name, inc_type, gacc, state, start_date, contain_date, size_acres, cause))  \n",
    "  \n",
    "# Step 3: Convert the extracted data to a structured format  \n",
    "significant_incidents = []  \n",
    "  \n",
    "if table_matches:  \n",
    "    print(f\"Found {len(table_matches)} incident rows.\")  \n",
    "    for match in table_matches:  \n",
    "        name, inc_type, gacc, state, start_date, contain_date, size_acres, cause = match  \n",
    "          \n",
    "        # Clean up the data  \n",
    "        name = name.strip()  \n",
    "        inc_type = inc_type.strip()  \n",
    "        gacc = gacc.strip()  \n",
    "        state = state.strip()  \n",
    "        start_date = start_date.strip()  \n",
    "        contain_date = contain_date.strip()  \n",
    "        size_acres = size_acres.strip().replace(',', '')  # Remove commas from numbers  \n",
    "        cause = cause.strip()  \n",
    "          \n",
    "        # Map cause codes to descriptions  \n",
    "        cause_description = {  \n",
    "            'H': 'Human-caused',  \n",
    "            'L': 'Lightning',  \n",
    "            'U': 'Unknown',  \n",
    "            'NR': 'Not Reported'  \n",
    "        }.get(cause, 'Unknown')  \n",
    "          \n",
    "        # Create a dictionary for this incident  \n",
    "        incident = {  \n",
    "            'Name': name,  \n",
    "            'Inc_Type': inc_type,  \n",
    "            'GACC': gacc,  \n",
    "            'State': state,  \n",
    "            'Start_Date': start_date,  \n",
    "            'Contain_Control_Date': contain_date,  \n",
    "            'Size_Acres': size_acres,  \n",
    "            'Cause': cause,  \n",
    "            'Cause_Description': cause_description  \n",
    "        }  \n",
    "          \n",
    "        significant_incidents.append(incident)  \n",
    "else:  \n",
    "    print(\"No incident data found. Check the text file format and adjust the extraction patterns.\")  \n",
    "  \n",
    "# Step 4: Create the star schema structure  \n",
    "# Fact table is already created as significant_incidents  \n",
    "  \n",
    "# Create dimension tables  \n",
    "if significant_incidents:  \n",
    "    # State dimension  \n",
    "    states = list(set(incident['State'] for incident in significant_incidents)) \n",
    "    state_dim = [{'State_Code': state, 'Region': 'Unknown'} for state in states]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = list(set(incident['GACC'] for incident in significant_incidents))  \n",
    "    gacc_dim = [{'GACC_Code': gacc, 'GACC_Name': 'Unknown'} for gacc in gaccs]  \n",
    "      \n",
    "    # Time dimension  \n",
    "    time_dim = []  \n",
    "    for i, incident in enumerate(significant_incidents):  \n",
    "        date_str = incident.get('Start_Date', 'Unknown')  \n",
    "          \n",
    "        # Try to parse the date  \n",
    "        try:  \n",
    "            # First try MM/DD/YYYY format  \n",
    "            date_obj = datetime.strptime(date_str, '%m/%d/%Y')  \n",
    "            month = date_obj.month  \n",
    "            year = date_obj.year  \n",
    "            formatted_date = date_obj.strftime('%Y-%m-%d')  \n",
    "        except ValueError:  \n",
    "            try:  \n",
    "                # Then try MM/DD/YY format  \n",
    "                date_obj = datetime.strptime(date_str, '%m/%d/%y')  \n",
    "                month = date_obj.month  \n",
    "                year = date_obj.year  \n",
    "                formatted_date = date_obj.strftime('%Y-%m-%d')  \n",
    "            except ValueError:  \n",
    "                month = 0  \n",
    "                year = 0  \n",
    "                formatted_date = date_str  \n",
    "          \n",
    "        time_dim.append({  \n",
    "            'Date_ID': i + 1,  \n",
    "            'Date': formatted_date,  \n",
    "            'Month': month,  \n",
    "            'Year': year  \n",
    "        })  \n",
    "      \n",
    "    # Step 5: Save the structured data to JSON files  \n",
    "    structured_data = {  \n",
    "        'significant_incidents': significant_incidents  \n",
    "    }  \n",
    "      \n",
    "    dimensions = {  \n",
    "        'state_dimension': state_dim,  \n",
    "        'gacc_dimension': gacc_dim,  \n",
    "        'time_dimension': time_dim  \n",
    "    }  \n",
    "      \n",
    "    with open('structured_incidents_data_2009.json', 'w') as f:  \n",
    "        json.dump(structured_data, f, indent=4)  \n",
    "      \n",
    "    with open('dimension_tables_2009.json', 'w') as f:  \n",
    "        json.dump(dimensions, f, indent=4)  \n",
    "      \n",
    "    print(\"Data successfully extracted and saved to JSON files.\")  \n",
    "    print(f\"Found {len(significant_incidents)} significant incidents.\")  \n",
    "    print(f\"Created dimension tables: States ({len(state_dim)}), GACCs ({len(gacc_dim)}), Time ({len(time_dim)}).\")  \n",
    "      \n",
    "    # Display a sample of the extracted data  \n",
    "    print(\"\\nSample of extracted incidents (first 5):\")  \n",
    "    for i, incident in enumerate(significant_incidents[:5]):  \n",
    "        print(f\"{i+1}. {incident['Name']} - {incident['State']} - {incident['Start_Date']} - {incident['Size_Acres']} acres\")  \n",
    "else:  \n",
    "    print(\"No data to save.\")  \n",
    "  \n",
    "# Optional: If you want to see the data in a DataFrame format  \n",
    "if significant_incidents:  \n",
    "    df = pd.DataFrame(significant_incidents)  \n",
    "    print(\"\\nDataFrame view of the first 5 incidents:\")  \n",
    "    print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89582fd4-8ef1-4780-812f-c7faf1e48e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\hanna\\AppData\\Local\\Temp\\ipykernel_6248\\971295909.py:34: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  header_pattern = f\"{header}.*?(?=\\n\\n\\w+|\\Z)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches found with specific pattern. Trying alternative pattern...\n",
      "Still no matches. Looking for section headers...\n",
      "Found section: Large Fire Activity\n",
      "No incident data found in the text file.\n",
      "You may need to manually inspect the text file and adjust the pattern matching approach.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries  \n",
    "import re  \n",
    "import json  \n",
    "import pandas as pd  \n",
    "from datetime import datetime  \n",
    "  \n",
    "# Step 1: Read the text file  \n",
    "with open('annual_report_2009_text.txt', 'r', encoding='utf-8') as file:  \n",
    "    text_content = file.read()  \n",
    "  \n",
    "# Step 2: Look for the significant incidents table  \n",
    "# We'll search for patterns that might indicate the table of significant incidents  \n",
    "print(\"Searching for significant incidents table...\")  \n",
    "  \n",
    "# Try to find a table with fire data  \n",
    "# This pattern looks for lines with fire name, type, location, dates, and acreage  \n",
    "table_pattern = r\"([A-Za-z0-9\\s\\-\\.&,()]+?)\\s+(\\w+)\\s+(\\w+)\\s+(\\w+)\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,3}(?:,\\d{3})*)\\s+([A-Z])\"  \n",
    "table_matches = re.findall(table_pattern, text_content)  \n",
    "  \n",
    "# If no matches found, try a more flexible pattern  \n",
    "if not table_matches:  \n",
    "    print(\"No matches found with specific pattern. Trying alternative pattern...\")  \n",
    "    # More flexible pattern  \n",
    "    table_pattern = r\"([A-Za-z0-9\\s\\-\\.&,()]+?)\\s+(\\w+)\\s+(\\w+)\\s+(\\w+)\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,3}(?:,\\d{3})*)\"  \n",
    "    table_matches = re.findall(table_pattern, text_content)  \n",
    "  \n",
    "# If still no matches, try to find the table by looking for a section header  \n",
    "if not table_matches:  \n",
    "    print(\"Still no matches. Looking for section headers...\")  \n",
    "    # Look for section headers that might indicate the start of the table  \n",
    "    section_headers = [\"Significant Incidents\", \"Large Fire Activity\", \"Major Incidents\"]  \n",
    "      \n",
    "    for header in section_headers:  \n",
    "        header_pattern = f\"{header}.*?(?=\\n\\n\\w+|\\Z)\"  \n",
    "        section_match = re.search(header_pattern, text_content, re.DOTALL | re.IGNORECASE)  \n",
    "          \n",
    "        if section_match:  \n",
    "            section_text = section_match.group(0)  \n",
    "            print(f\"Found section: {header}\")  \n",
    "              \n",
    "            # Try to extract table rows from this section  \n",
    "            table_pattern = r\"([A-Za-z0-9\\s\\-\\.&,()]+?)\\s+(\\w+)\\s+(\\w+)\\s+(\\w+)\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,2}/\\d{1,2}/\\d{2,4})\\s+(\\d{1,3}(?:,\\d{3})*)\\s+([A-Z])\"  \n",
    "            table_matches = re.findall(table_pattern, section_text)  \n",
    "              \n",
    "            if table_matches:  \n",
    "                break  \n",
    "  \n",
    "# Step 3: Process the extracted data  \n",
    "if table_matches:  \n",
    "    print(f\"Found {len(table_matches)} potential incident records.\")  \n",
    "      \n",
    "    # Create a list to store the structured incident data  \n",
    "    significant_incidents = []  \n",
    "      \n",
    "    # Process each match  \n",
    "    for match in table_matches:  \n",
    "        # Clean up the fire name (remove leading/trailing spaces, replace multiple spaces with single space)  \n",
    "        name = re.sub(r'\\s+', ' ', match[0].strip())  \n",
    "          \n",
    "        # Create a dictionary for this incident  \n",
    "        incident = {  \n",
    "            'Name': name,  \n",
    "            'Inc_Type': match[1],  \n",
    "            'GACC': match[2],  \n",
    "            'State': match[3],  \n",
    "            'Start_Date': match[4],  \n",
    "            'Contain_Control_Date': match[5],  \n",
    "            'Size_Acres': match[6],  \n",
    "            'Cause': match[7] if len(match) > 7 else \"Unknown\"  \n",
    "        }  \n",
    "          \n",
    "        # Add cause description based on cause code  \n",
    "        if incident['Cause'] == 'H':  \n",
    "            incident['Cause_Description'] = 'Human-caused'  \n",
    "        elif incident['Cause'] == 'L':  \n",
    "            incident['Cause_Description'] = 'Lightning-caused'  \n",
    "        else:  \n",
    "            incident['Cause_Description'] = 'Unknown'  \n",
    "          \n",
    "        significant_incidents.append(incident)  \n",
    "      \n",
    "    # Step 4: Create dimension tables for the star schema  \n",
    "      \n",
    "    # State dimension  \n",
    "    states = list(set(incident['State'] for incident in significant_incidents))  \n",
    "    state_dim = [{'State_Code': state, 'Region': 'Unknown'} for state in states]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = list(set(incident['GACC'] for incident in significant_incidents))  \n",
    "    gacc_dim = [{'GACC_Code': gacc, 'GACC_Name': 'Unknown'} for gacc in gaccs]  \n",
    "      \n",
    "    # Time dimension  \n",
    "    time_dim = []  \n",
    "    for i, incident in enumerate(significant_incidents):  \n",
    "        date_str = incident.get('Start_Date', 'Unknown')  \n",
    "          \n",
    "        # Try to parse the date  \n",
    "        try:  \n",
    "            # Handle different date formats (MM/DD/YYYY or MM/DD/YY)  \n",
    "            if len(date_str.split('/')[2]) == 2:  \n",
    "                # Assume 20xx for two-digit years  \n",
    "                date_str = f\"{date_str.split('/')[0]}/{date_str.split('/')[1]}/20{date_str.split('/')[2]}\"  \n",
    "              \n",
    "            date_obj = datetime.strptime(date_str, '%m/%d/%Y')  \n",
    "            month = date_obj.month  \n",
    "            year = date_obj.year  \n",
    "            formatted_date = date_obj.strftime('%Y-%m-%d')  \n",
    "        except ValueError:  \n",
    "            month = 0  \n",
    "            year = 0  \n",
    "            formatted_date = date_str  \n",
    "          \n",
    "        time_dim.append({  \n",
    "            'Date_ID': i + 1,  \n",
    "            'Date': formatted_date,  \n",
    "            'Month': month,  \n",
    "            'Year': year  \n",
    "        })  \n",
    "      \n",
    "    # Step 5: Save the structured data to JSON files  \n",
    "    structured_data = {  \n",
    "        'significant_incidents': significant_incidents  \n",
    "    }  \n",
    "      \n",
    "    dimensions = {  \n",
    "        'state_dimension': state_dim,  \n",
    "        'gacc_dimension': gacc_dim,  \n",
    "        'time_dimension': time_dim  \n",
    "    }  \n",
    "      \n",
    "    with open('structured_incidents_data_2009.json', 'w') as f:  \n",
    "        json.dump(structured_data, f, indent=4)  \n",
    "      \n",
    "    with open('dimension_tables_2009.json', 'w') as f:  \n",
    "        json.dump(dimensions, f, indent=4)  \n",
    "      \n",
    "    print(\"Data successfully extracted and saved to JSON files:\")  \n",
    "    print(\"- structured_incidents_data_2009.json\")  \n",
    "    print(\"- dimension_tables_2009.json\")  \n",
    "    print(f\"Found {len(significant_incidents)} significant incidents.\")  \n",
    "    print(f\"Created dimension tables: States ({len(state_dim)}), GACCs ({len(gacc_dim)}), Time ({len(time_dim)}).\")  \n",
    "      \n",
    "    # Display a sample of the extracted data  \n",
    "    print(\"\\nSample of extracted incidents (first 5):\")  \n",
    "    for i, incident in enumerate(significant_incidents[:5]):  \n",
    "        print(f\"{i+1}. {incident['Name']} - {incident['State']} - {incident['Start_Date']} - {incident['Size_Acres']} acres\")  \n",
    "      \n",
    "    # Create a DataFrame for easier viewing  \n",
    "    df = pd.DataFrame(significant_incidents)  \n",
    "    print(\"\\nDataFrame view of the first 5 incidents:\")  \n",
    "    print(df.head())  \n",
    "else:  \n",
    "    print(\"No incident data found in the text file.\")  \n",
    "    print(\"You may need to manually inspect the text file and adjust the pattern matching approach.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7180faea-a294-4674-9520-60b12c1dc722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabula-py\n",
      "  Using cached tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\hanna\\anaconda3\\lib\\site-packages (1.25.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\hanna\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>1.24.4 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from tabula-py) (1.26.4)\n",
      "Requirement already satisfied: distro in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from tabula-py) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hanna\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.0 MB 4.7 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/12.0 MB 4.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/12.0 MB 4.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.0 MB 4.7 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.7/12.0 MB 5.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.0/12.0 MB 5.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/12.0 MB 5.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.5/12.0 MB 5.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.8/12.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.0/12.0 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.3/12.0 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.6/12.0 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.8/12.0 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.8/12.0 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.8/12.0 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/12.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.1/12.0 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.2/12.0 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/12.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.8/12.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.1/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.4/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.0 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.6/12.0 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.0/12.0 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.0 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.7/12.0 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.0/12.0 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.4/12.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.0 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.0 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.0 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.0 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: tabula-py\n",
      "Successfully installed tabula-py-2.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tabula-py PyMuPDF pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76cfbfdd-b08d-4ad0-80a3-17d0a44b963a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Text extraction complete. Saved to 'annual_report_2009_extracted.txt'\n",
      "Searching for sections with incident data...\n",
      "Found 2 potential sections with incident data.\n",
      "Section 1: Large Fire Activity (position 9902)\n",
      "Context: the 90th percentile or historic maximums in several locations, \n",
      "especially in Texas. By late April, large fire activity began to taper off in Texas and pick up in \n",
      "Florida and the Eastern, Southwest, ...\n",
      "Section 2: Large Fire Activity (position 16190)\n",
      "Context: Colorado. Much of the West saw frequent mixed wet and dry lightning storms, yet initial attack \n",
      "and large fire activity continued to be below normal for the most part. Nationally, the \n",
      "preparedness le...\n",
      "Found 0 date patterns in the text.\n",
      "Sample dates: []\n",
      "Found 0 potential state abbreviations.\n",
      "Sample states: []\n",
      "Found 23 acreage mentions.\n",
      "Sample acreages: ['40,000', '1,000', '136,020', '897,496', '1,400,185', '109,988', '8', '2,934,455', '90,000', '160,000']\n",
      "Found 0 potential fire incident entries.\n",
      "Sample entries:\n",
      "Found 0 paragraphs mentioning fires with dates.\n",
      "Sample paragraphs:\n",
      "Extracted 0 structured incident records.\n",
      "Sample structured incidents:\n",
      "Data successfully extracted and saved to JSON files:\n",
      "- structured_incidents_data_2009.json\n",
      "- dimension_tables_2009.json\n",
      "Created dimension tables: States (0), GACCs (0), Time (0).\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Extract text from the PDF using PyMuPDF\n",
    "pdf_path = 'annual_report_2009_508.pdf'\n",
    "text_content = \"\"\n",
    "\n",
    "print(\"Extracting text from PDF...\")\n",
    "doc = fitz.open(pdf_path)\n",
    "for page in doc:\n",
    "    text_content += page.get_text()\n",
    "\n",
    "# Save the extracted text to a file for reference\n",
    "with open('annual_report_2009_extracted.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(text_content)\n",
    "\n",
    "print(\"Text extraction complete. Saved to 'annual_report_2009_extracted.txt'\")\n",
    "\n",
    "# Step 2: Look for sections that might contain significant incidents data\n",
    "print(\"\\\n",
    "Searching for sections with incident data...\")\n",
    "\n",
    "# Let's look for section headers that might indicate significant incidents\n",
    "section_headers = [\n",
    "    \"Significant Incidents\", \n",
    "    \"Large Fire Activity\", \n",
    "    \"Major Incidents\",\n",
    "    \"Significant Wildland Fires\"\n",
    "]\n",
    "\n",
    "# Search for these headers in the text\n",
    "found_sections = []\n",
    "for header in section_headers:\n",
    "    matches = re.finditer(r'(?i)' + re.escape(header), text_content)\n",
    "    for match in matches:\n",
    "        start_pos = match.start()\n",
    "        # Get some context (500 characters) around the match\n",
    "        context_start = max(0, start_pos - 100)\n",
    "        context_end = min(len(text_content), start_pos + 500)\n",
    "        context = text_content[context_start:context_end]\n",
    "        found_sections.append((header, start_pos, context))\n",
    "\n",
    "print(f\"Found {len(found_sections)} potential sections with incident data.\")\n",
    "for i, (header, pos, context) in enumerate(found_sections):\n",
    "    print(f\"\\\n",
    "Section {i+1}: {header} (position {pos})\")\n",
    "    print(f\"Context: {context[:200]}...\")  # Show first 200 chars of context\n",
    "\n",
    "# Step 3: Let's look for patterns that might indicate incident data\n",
    "# Common patterns in wildfire incident reports include:\n",
    "# - Fire names followed by acreage\n",
    "# - Dates in MM/DD/YYYY format\n",
    "# - State abbreviations\n",
    "\n",
    "# Look for date patterns\n",
    "date_pattern = r'\\d{1,2}/\\d{1,2}/\\d{4}'\n",
    "dates = re.findall(date_pattern, text_content)\n",
    "print(f\"\\\n",
    "Found {len(dates)} date patterns in the text.\")\n",
    "print(\"Sample dates:\", dates[:10])\n",
    "\n",
    "# Look for state abbreviations (2 uppercase letters)\n",
    "state_pattern = r'\\\b([A-Z]{2})\\\b'\n",
    "states = re.findall(state_pattern, text_content)\n",
    "print(f\"\\\n",
    "Found {len(states)} potential state abbreviations.\")\n",
    "print(\"Sample states:\", states[:20])\n",
    "\n",
    "# Look for acreage patterns (numbers followed by \"acres\")\n",
    "acreage_pattern = r'(\\d{1,3}(?:,\\d{3})*)\\s*acres'\n",
    "acreages = re.findall(acreage_pattern, text_content, re.IGNORECASE)\n",
    "print(f\"\\\n",
    "Found {len(acreages)} acreage mentions.\")\n",
    "print(\"Sample acreages:\", acreages[:10])\n",
    "\n",
    "# Step 4: Let's try to find tables or structured data\n",
    "# Look for patterns that might indicate table rows with fire data\n",
    "# This is a more complex pattern that tries to match fire name, location, date, and acreage\n",
    "fire_pattern = r'([A-Za-z\\s\\-\\.&]+)\\s+(?:Fire|Complex)\\s+(?:in\\s+)?([A-Z]{2})\\s+.*?(\\d{1,2}/\\d{1,2}/\\d{4}).*?(\\d{1,3}(?:,\\d{3})*)\\s*acres'\n",
    "fire_matches = re.findall(fire_pattern, text_content, re.IGNORECASE)\n",
    "\n",
    "print(f\"\\\n",
    "Found {len(fire_matches)} potential fire incident entries.\")\n",
    "print(\"Sample entries:\")\n",
    "for i, match in enumerate(fire_matches[:5]):\n",
    "    print(f\"{i+1}. {match}\")\n",
    "\n",
    "# Let's also try to find paragraphs that mention fires\n",
    "fire_paragraphs = []\n",
    "paragraphs = re.split(r'\\\n",
    "\\s*\\\n",
    "', text_content)\n",
    "for para in paragraphs:\n",
    "    if re.search(r'(?:Fire|Complex|Wildfire)', para, re.IGNORECASE) and re.search(date_pattern, para):\n",
    "        fire_paragraphs.append(para)\n",
    "\n",
    "print(f\"\\\n",
    "Found {len(fire_paragraphs)} paragraphs mentioning fires with dates.\")\n",
    "print(\"Sample paragraphs:\")\n",
    "for i, para in enumerate(fire_paragraphs[:3]):\n",
    "    print(f\"\\\n",
    "Paragraph {i+1}: {para[:200]}...\")\n",
    "\n",
    "# Step 5: Let's try to extract structured data from these paragraphs\n",
    "# We'll look for patterns like:\n",
    "# - Fire name (often ends with \"Fire\" or \"Complex\")\n",
    "# - State abbreviation\n",
    "# - Start date\n",
    "# - Containment date (if available)\n",
    "# - Acreage\n",
    "\n",
    "structured_incidents = []\n",
    "\n",
    "for para in fire_paragraphs:\n",
    "    # Try to extract fire name\n",
    "    name_match = re.search(r'([A-Za-z\\s\\-\\.&]+?)(?:Fire|Complex)', para, re.IGNORECASE)\n",
    "    if name_match:\n",
    "        name = name_match.group(1).strip()\n",
    "    else:\n",
    "        continue  # Skip if no fire name found\n",
    "    \n",
    "    # Try to extract state\n",
    "    state_match = re.search(state_pattern, para)\n",
    "    state = state_match.group(1) if state_match else \"Unknown\"\n",
    "    \n",
    "    # Try to extract dates\n",
    "    dates = re.findall(date_pattern, para)\n",
    "    start_date = dates[0] if dates else \"Unknown\"\n",
    "    contain_date = dates[1] if len(dates) > 1 else \"Unknown\"\n",
    "    \n",
    "    # Try to extract acreage\n",
    "    acreage_match = re.search(acreage_pattern, para, re.IGNORECASE)\n",
    "    acres = acreage_match.group(1).replace(',', '') if acreage_match else \"0\"\n",
    "    \n",
    "    # Try to extract GACC (Geographic Area Coordination Center)\n",
    "    gacc_match = re.search(r'\\\b(EA|SA|SW|RM|GB|NW|NO|SO|NR|AK)\\\b', para)\n",
    "    gacc = gacc_match.group(1) if gacc_match else \"Unknown\"\n",
    "    \n",
    "    # Try to extract cause\n",
    "    cause_match = re.search(r'caused by\\s+([A-Za-z\\s]+)', para, re.IGNORECASE)\n",
    "    cause = cause_match.group(1).strip() if cause_match else \"Unknown\"\n",
    "    \n",
    "    # Add to structured incidents\n",
    "    structured_incidents.append({\n",
    "        'Name': name,\n",
    "        'State': state,\n",
    "        'GACC': gacc,\n",
    "        'Start_Date': start_date,\n",
    "        'Contain_Date': contain_date,\n",
    "        'Size_Acres': acres,\n",
    "        'Cause': cause\n",
    "    })\n",
    "\n",
    "print(f\"\\\n",
    "Extracted {len(structured_incidents)} structured incident records.\")\n",
    "print(\"Sample structured incidents:\")\n",
    "for i, incident in enumerate(structured_incidents[:5]):\n",
    "    print(f\"{i+1}. {incident}\")\n",
    "\n",
    "# Step 6: Create dimension tables for the star schema\n",
    "# State Dimension\n",
    "states = set(incident['State'] for incident in structured_incidents)\n",
    "state_dim = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]\n",
    "\n",
    "# GACC Dimension\n",
    "gaccs = set(incident['GACC'] for incident in structured_incidents)\n",
    "gacc_dim = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]\n",
    "\n",
    "# Time Dimension\n",
    "dates = set(incident['Start_Date'] for incident in structured_incidents if incident['Start_Date'] != \"Unknown\")\n",
    "time_dim = []\n",
    "for i, date_str in enumerate(sorted(dates)):\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "        time_dim.append({\n",
    "            'Date_ID': i+1,\n",
    "            'Date': date_obj.strftime('%Y-%m-%d'),\n",
    "            'Month': date_obj.month,\n",
    "            'Year': date_obj.year\n",
    "        })\n",
    "    except ValueError:\n",
    "        pass  # Skip invalid dates\n",
    "\n",
    "# Step 7: Create the fact table with dimension keys\n",
    "fact_table = []\n",
    "state_id_map = {state['State']: state['State_ID'] for state in state_dim}\n",
    "gacc_id_map = {gacc['GACC']: gacc['GACC_ID'] for gacc in gacc_dim}\n",
    "date_id_map = {time['Date']: time['Date_ID'] for time in time_dim}\n",
    "\n",
    "for incident in structured_incidents:\n",
    "    # Convert Start_Date to the format used in time_dim\n",
    "    start_date_formatted = None\n",
    "    if incident['Start_Date'] != \"Unknown\":\n",
    "        try:\n",
    "            date_obj = datetime.strptime(incident['Start_Date'], '%m/%d/%Y')\n",
    "            start_date_formatted = date_obj.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    fact_table.append({\n",
    "        'Incident_ID': len(fact_table) + 1,\n",
    "        'Name': incident['Name'],\n",
    "        'State_ID': state_id_map.get(incident['State'], None),\n",
    "        'GACC_ID': gacc_id_map.get(incident['GACC'], None),\n",
    "        'Date_ID': date_id_map.get(start_date_formatted, None) if start_date_formatted else None,\n",
    "        'Start_Date': incident['Start_Date'],\n",
    "        'Contain_Date': incident['Contain_Date'],\n",
    "        'Size_Acres': incident['Size_Acres'],\n",
    "        'Cause': incident['Cause']\n",
    "    })\n",
    "\n",
    "# Step 8: Save the structured data to JSON files\n",
    "structured_data = {\n",
    "    'significant_incidents': fact_table\n",
    "}\n",
    "\n",
    "dimensions = {\n",
    "    'state_dimension': state_dim,\n",
    "    'gacc_dimension': gacc_dim,\n",
    "    'time_dimension': time_dim\n",
    "}\n",
    "\n",
    "with open('structured_incidents_data_2009.json', 'w') as f:\n",
    "    json.dump(structured_data, f, indent=4)\n",
    "\n",
    "with open('dimension_tables_2009.json', 'w') as f:\n",
    "    json.dump(dimensions, f, indent=4)\n",
    "\n",
    "print(\"\\\n",
    "Data successfully extracted and saved to JSON files:\")\n",
    "print(\"- structured_incidents_data_2009.json\")\n",
    "print(\"- dimension_tables_2009.json\")\n",
    "print(f\"Created dimension tables: States ({len(state_dim)}), GACCs ({len(gacc_dim)}), Time ({len(time_dim)}).\")\n",
    "\n",
    "# Create a DataFrame for easier viewing\n",
    "if structured_incidents:\n",
    "    df = pd.DataFrame(structured_incidents)\n",
    "    print(\"\\\n",
    "DataFrame view of the first 5 incidents:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdfa271e-7024-4d48-bf54-a04aebad4975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table of Contents:\n",
      "Identifier Legend (Page 3)\n",
      "Preface (Page 4)\n",
      "2009 Fire Season Summary (Page 5)\n",
      "National Fire Activity Synopsis (Page 12)\n",
      "Fires and Complexes Over 40,000 Acres in 2009 (Page 13)\n",
      "Significant Fire Activity (Page 14)\n",
      "Wildfires Reported to NICC (Page 16)\n",
      "National Preparedness Levels (Page 31)\n",
      "  National Preparedness Level Summary (Page 32)\n",
      "Incident Management Team Mobilizations (Page 33)\n",
      "Department of Defense Mobilization (Page 39)\n",
      "Crew Mobilization (Page 39)\n",
      "Engine Mobilization (Page 42)\n",
      "Overhead Mobilization (Page 45)\n",
      "Helicopter Mobilization (Page 47)\n",
      "Fixed Wing Aircraft Mobilization (Page 51)\n",
      "Large Transportation Aircraft (Page 55)\n",
      "Light Cargo and Passenger Flights by Requesting Agency and Geographic Area (Page 57)\n",
      "Equipment Services Mobilization (Page 58)\n",
      "Radio and Weather Equipment Mobilization (Page 60)\n",
      "Average Worst Summary (Page 62)\n",
      "NICC Benchmarks (Page 63)\n",
      "Acronyms and Terminology (Page 64)\n",
      "National Report of Wildland Fires and Acres Burned by State (Page 65)\n",
      "Searching for tables in the document...\n",
      "Found potential tables on 0 pages.\n",
      "Examining specific pages for incident data...\n",
      "Page 11 might contain incident data:\n",
      " \n",
      " 10\n",
      "Military and International Resource Mobilizations \n",
      " \n",
      "There were no military activations in support of wildland fires in 2009. In February the United \n",
      "States sent three Burned Area Emergency Rehabilitation (BAER) teams, a 20 person crew and \n",
      "15 fire specialists (a total of 73 personnel) to Aust...\n",
      "Page 12 might contain incident data:\n",
      " \n",
      " 11\n",
      "National Fire Activity Synopsis \n",
      " \n",
      "The 2009 fire season was slightly above normal for number of reported wildfires. There were \n",
      "291 more fires reported than the average for the past ten years. There were 78,792 wildfires \n",
      "reported (compared to 78,949 wildfires reported in 2008). This represent...\n",
      "Page 13 might contain incident data:\n",
      " \n",
      " 12\n",
      "Significant Fire Activity \n",
      "Fires and Complexes Over 40,000 Acres in 2009 \n",
      "Information derived from ICS-209 reports. \n",
      " \n",
      "Name \n",
      "Inc. \n",
      "Type \n",
      "GACC \n",
      "State \n",
      "Start Date \n",
      "Contain or \n",
      "Control Date \n",
      "Size \n",
      "(Acres) \n",
      "Cause \n",
      "Estimated \n",
      "Cost \n",
      "Chester \n",
      "WF \n",
      "SA \n",
      "OK \n",
      "10-Jul-09 \n",
      "17-Jul-09 \n",
      "41,497 \n",
      "U \n",
      "$250,000 \n",
      "Blu...\n",
      "Page 14 might contain incident data:\n",
      " \n",
      " 13\n",
      "Significant Fire Activity \n",
      "There were 1,101 large or significant wildfires reported to NICC during 2009 (from ICS-209 \n",
      "reports submitted in the FAMWEB reporting system). The maps below depict the locations of \n",
      "these fires. \n",
      " \n",
      " \n",
      "...\n",
      "Page 15 might contain incident data:\n",
      " \n",
      " 14\n",
      "Significant Fire Activity \n",
      "Significant fires are defined in the National Mobilization Guide as fires that are a minimum of 100 \n",
      "acres in timber fuel types and 300 acres in grass and brush fuel types, or are managed by a \n",
      "Type 1, 2, WFMT or NIMO incident management team.  \n",
      " \n",
      "Percent of Reported...\n",
      "Page 16 might contain incident data:\n",
      " \n",
      " 15\n",
      "Wildfires Reported to NICC \n",
      "There were 78,792 wildfires reported, which burned 5,921,786 acres in 2009. These figures are \n",
      "below both the five-year and ten-year averages for both wildfires and acres. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "Page 17 might contain incident data:\n",
      " \n",
      " 16\n",
      "Wildfire Acres Reported to NICC \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "Page 18 might contain incident data:\n",
      " \n",
      " 17\n",
      "Wildfire Activity Levels by Geographic Area \n",
      "Percent of Geographic Area wildfire activity in 2009 compared to the previous 10 years.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "Page 19 might contain incident data:\n",
      " \n",
      " 18\n",
      "Alaska  Wildfire Activity \n",
      "In 2009 Alaska burned 50 percent of all acres in the U.S. Over the past 10 years Alaska has \n",
      "burned an average of 34 percent of total acres annually. The chart below compares annual acres \n",
      "burned between Alaska and continental U.S. (includes Hawaiâ€™i). \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "Page 20 might contain incident data:\n",
      " \n",
      " 19\n",
      "Wildfires by Agency\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "ST / OT â€“ States and other non-federal \n",
      "...\n",
      "Page 21 might contain incident data:\n",
      " \n",
      " 20\n",
      "Wildfire Acres by Agency \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "ST / OT â€“ States and other non-federal  \n",
      "...\n",
      "Attempting to extract tables directly...\n",
      "Found 1 tables on page 13\n",
      "Table 1 on page 13:\n",
      "                Name Inc.\\nType GACC State Start Date  \\\n",
      "0            Chester         WF   SA    OK  10-Jul-09   \n",
      "1        Bluff Creek         WF   AK    AK  26-Jul-09   \n",
      "2           Big Pole         WF   EB    UT   6-Aug-09   \n",
      "3  Dry Creek Complex         WF   NW    WA  20-Aug-09   \n",
      "4         Bear Creek         WF   AK    AK  17-Jun-09   \n",
      "\n",
      "  Contain or\\nControl Date Size\\n(Acres) Cause Estimated\\nCost  \n",
      "0                17-Jul-09        41,497     U        $250,000  \n",
      "1                10-Nov-09        41,756     L              NR  \n",
      "2                 9-Nov-09        44,345     L              NR  \n",
      "3                24-Aug-09        48,902     L        $870,000  \n",
      "4                25-Sep-09        50,897     L              NR  \n",
      "This table likely contains incident data!\n",
      "Saved to extracted_table_page_13_table_1.csv\n",
      "Searching for specific sections with incident data...\n",
      "Analysis complete. Check the extracted tables and sections for incident data.\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the PDF structure more closely\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "pdf_path = 'annual_report_2009_508.pdf'\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# Let's look at the table of contents or bookmarks to find relevant sections\n",
    "toc = doc.get_toc()\n",
    "print(\"Table of Contents:\")\n",
    "for item in toc:\n",
    "    level, title, page = item\n",
    "    print(f\"{'  ' * (level-1)}{title} (Page {page})\")\n",
    "\n",
    "# Let's also check for any tables in the document\n",
    "print(\"\\\n",
    "Searching for tables in the document...\")\n",
    "tables_found = []\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    # Check if this page might contain a table\n",
    "    text = page.get_text()\n",
    "    \n",
    "    # Look for patterns that might indicate tables\n",
    "    # Tables often have multiple lines with similar structure\n",
    "    lines = text.split(' ')\n",
    "    \n",
    "    # Count lines with similar patterns (e.g., lines with multiple spaces between words)\n",
    "    pattern_count = defaultdict(int)\n",
    "    for line in lines:\n",
    "        # Create a simplified pattern of the line (spaces vs non-spaces)\n",
    "        pattern = re.sub(r'\\S+', 'W', line)  # Replace word characters with 'W'\n",
    "        pattern = re.sub(r'\\s+', ' ', pattern)  # Normalize spaces\n",
    "        pattern_count[pattern] += 1\n",
    "    \n",
    "    # If we have multiple lines with the same pattern, it might be a table\n",
    "    potential_tables = [pattern for pattern, count in pattern_count.items() if count >= 3 and 'W W' in pattern]\n",
    "    \n",
    "    if potential_tables:\n",
    "        tables_found.append((page_num + 1, len(potential_tables), text[:200]))\n",
    "\n",
    "print(f\"Found potential tables on {len(tables_found)} pages.\")\n",
    "for page_num, num_tables, sample_text in tables_found[:10]:  # Show first 10\n",
    "    print(f\"\\\n",
    "Page {page_num}: {num_tables} potential tables\")\n",
    "    print(f\"Sample text: {sample_text}...\")\n",
    "\n",
    "# Let's look at specific pages that might contain significant incident data\n",
    "# Based on the TOC or common report structure, we might want to check specific pages\n",
    "# For example, pages with \"Significant Incidents\" or \"Large Fires\" in the title\n",
    "\n",
    "# Let's check a few specific pages that might contain incident data\n",
    "# We'll look at pages 10-20 as an example (adjust based on TOC findings)\n",
    "target_pages = range(10, 21)\n",
    "print(\"\\\n",
    "Examining specific pages for incident data...\")\n",
    "\n",
    "for page_num in target_pages:\n",
    "    if page_num < len(doc):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Check if this page might contain incident data\n",
    "        if re.search(r'(Fire|Incident|Wildfire|Acres|Significant)', text, re.IGNORECASE):\n",
    "            print(f\"\\\n",
    "Page {page_num+1} might contain incident data:\")\n",
    "            print(text[:300] + \"...\")  # Print first 300 chars\n",
    "\n",
    "# Let's also try to extract any tables directly using PyMuPDF's table extraction\n",
    "print(\"\\\n",
    "Attempting to extract tables directly...\")\n",
    "\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]\n",
    "    \n",
    "    # Extract tables from the page\n",
    "    tables = page.find_tables()\n",
    "    \n",
    "    if tables and tables.tables:\n",
    "        print(f\"\\\n",
    "Found {len(tables.tables)} tables on page {page_num+1}\")\n",
    "        \n",
    "        for i, table in enumerate(tables.tables):\n",
    "            df = table.to_pandas()\n",
    "            print(f\"Table {i+1} on page {page_num+1}:\")\n",
    "            print(df.head())\n",
    "            \n",
    "            # Check if this table might contain incident data\n",
    "            # Look for columns that might indicate fire incidents\n",
    "            columns = [str(col).lower() for col in df.columns]\n",
    "            if any(keyword in ' '.join(columns) for keyword in ['fire', 'incident', 'acre', 'date']):\n",
    "                print(f\"This table likely contains incident data!\")\n",
    "                \n",
    "                # Save this table to a CSV file\n",
    "                csv_filename = f\"extracted_table_page_{page_num+1}_table_{i+1}.csv\"\n",
    "                df.to_csv(csv_filename, index=False)\n",
    "                print(f\"Saved to {csv_filename}\")\n",
    "\n",
    "# Let's also try to find specific sections in the text that might contain incident data\n",
    "print(\"\\\n",
    "Searching for specific sections with incident data...\")\n",
    "\n",
    "# Common section titles for incident data\n",
    "section_titles = [\n",
    "    \"Significant Wildland Fires\",\n",
    "    \"Large Fire Activity\",\n",
    "    \"Significant Incidents\",\n",
    "    \"Major Incidents\",\n",
    "    \"Appendix\"  # Appendices often contain tables with incident data\n",
    "]\n",
    "\n",
    "# Search for these section titles in the text\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "    \n",
    "    for title in section_titles:\n",
    "        if title in text:\n",
    "            print(f\"\\\n",
    "Found '{title}' on page {page_num+1}\")\n",
    "            \n",
    "            # Get the position of the title\n",
    "            title_pos = text.find(title)\n",
    "            \n",
    "            # Get the text following the title (up to 500 chars)\n",
    "            section_text = text[title_pos:title_pos+500]\n",
    "            print(f\"Section text: {section_text}...\")\n",
    "\n",
    "# Close the document\n",
    "doc.close()\n",
    "\n",
    "print(\"\\\n",
    "Analysis complete. Check the extracted tables and sections for incident data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4f17c66-fd44-4164-b5f1-e20f09be4fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2009 CSV data with 27 incidents.\n",
      "Structured JSON data for the 2009 incidents:\n",
      "{\n",
      "    \"significant_incidents\": [\n",
      "        {\n",
      "            \"Name\": \"Chester\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"SA\",\n",
      "            \"State\": \"OK\",\n",
      "            \"Start_Date\": \"10-Jul-09\",\n",
      "            \"Contain_Control_Date\": \"17-Jul-09\",\n",
      "            \"Size_Acres\": \"41,497\",\n",
      "            \"Cause\": \"U\",\n",
      "            \"Cost\": \"$250,000\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Bluff Creek\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"26-Jul-09\",\n",
      "            \"Contain_Control_Date\": \"10-Nov-09\",\n",
      "            \"Size_Acres\": \"41,756\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Big Pole\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"EB\",\n",
      "            \"State\": \"UT\",\n",
      "            \"Start_Date\": \"6-Aug-09\",\n",
      "            \"Contain_Control_Date\": \"9-Nov-09\",\n",
      "            \"Size_Acres\": \"44,345\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Dry Creek Complex\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"NW\",\n",
      "            \"State\": \"WA\",\n",
      "            \"Start_Date\": \"20-Aug-09\",\n",
      "            \"Contain_Control_Date\": \"24-Aug-09\",\n",
      "            \"Size_Acres\": \"48,902\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"$870,000\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Bear Creek\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"17-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"25-Sep-09\",\n",
      "            \"Size_Acres\": \"50,897\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Putnam\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"SA\",\n",
      "            \"State\": \"OK\",\n",
      "            \"Start_Date\": \"5-Mar-09\",\n",
      "            \"Contain_Control_Date\": \"11-Mar-09\",\n",
      "            \"Size_Acres\": \"52,790\",\n",
      "            \"Cause\": \"N\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"East Slide Rock Ridge\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"WB\",\n",
      "            \"State\": \"NV\",\n",
      "            \"Start_Date\": \"10-Aug-08\",\n",
      "            \"Contain_Control_Date\": \"7-Apr-09\",\n",
      "            \"Size_Acres\": \"54,549\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Cato\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"SW\",\n",
      "            \"State\": \"NM\",\n",
      "            \"Start_Date\": \"10-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"13-Jun-09\",\n",
      "            \"Size_Acres\": \"55,080\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"$460,000\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Chakina\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"2-Jul-09\",\n",
      "            \"Contain_Control_Date\": \"25-Aug-09\",\n",
      "            \"Size_Acres\": \"56,413\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"$1,933,616\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Loco/Healdton\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"SA\",\n",
      "            \"State\": \"OK\",\n",
      "            \"Start_Date\": \"9-Apr-09\",\n",
      "            \"Contain_Control_Date\": \"15-Apr-09\",\n",
      "            \"Size_Acres\": \"56,688\",\n",
      "            \"Cause\": \"U\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Rock Slough\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"30-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"10-Nov-09\",\n",
      "            \"Size_Acres\": \"62,313\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Sheenjek\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"2-Jul-09\",\n",
      "            \"Contain_Control_Date\": \"1-Oct-09\",\n",
      "            \"Size_Acres\": \"62,658\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"$245,000\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Stevens Creek #1\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"18-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"10-Nov-09\",\n",
      "            \"Size_Acres\": \"85,909\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"La Brea\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"SO\",\n",
      "            \"State\": \"CA\",\n",
      "            \"Start_Date\": \"8-Aug-09\",\n",
      "            \"Contain_Control_Date\": \"5-Oct-09\",\n",
      "            \"Size_Acres\": \"89,489\",\n",
      "            \"Cause\": \"H\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Pasco\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"SW\",\n",
      "            \"State\": \"NM\",\n",
      "            \"Start_Date\": \"10-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"23-Jun-09\",\n",
      "            \"Size_Acres\": \"93,029\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"$450,000\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Rex Creek *\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"2-Aug-09\",\n",
      "            \"Contain_Control_Date\": \"September\",\n",
      "            \"Size_Acres\": \"101,150\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Wood River 1\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"12-Jul-09\",\n",
      "            \"Contain_Control_Date\": \"September\",\n",
      "            \"Size_Acres\": \"125,382\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Nowitna\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"13-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"August\",\n",
      "            \"Size_Acres\": \"126,582\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Zitziana\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"17-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"August\",\n",
      "            \"Size_Acres\": \"141,125\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Station\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"SO\",\n",
      "            \"State\": \"CA\",\n",
      "            \"Start_Date\": \"26-Aug-09\",\n",
      "            \"Contain_Control_Date\": \"22-Nov-09\",\n",
      "            \"Size_Acres\": \"160,577\",\n",
      "            \"Cause\": \"H\",\n",
      "            \"Cost\": \"$95,510,000\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Tonclonukna Creek\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"23-May-09\",\n",
      "            \"Contain_Control_Date\": \"July\",\n",
      "            \"Size_Acres\": \"164,318\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"$2,969,556\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Titna River\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"11-Jul-09\",\n",
      "            \"Contain_Control_Date\": \"August\",\n",
      "            \"Size_Acres\": \"164,542\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Big Creek\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"18-Jul-09\",\n",
      "            \"Contain_Control_Date\": \"18-Aug-09\",\n",
      "            \"Size_Acres\": \"169,639\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"$820,825\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Little Black One **\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"20-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"August\",\n",
      "            \"Size_Acres\": \"349,450\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Crazy Mountain\\nComplex **\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"20-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"August\",\n",
      "            \"Size_Acres\": \"447,420\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Minto Flats South *\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"21-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"July\",\n",
      "            \"Size_Acres\": \"517,078\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        },\n",
      "        {\n",
      "            \"Name\": \"Railbelt Complex *\",\n",
      "            \"Inc_Type\": \"WF\",\n",
      "            \"GACC\": \"AK\",\n",
      "            \"State\": \"AK\",\n",
      "            \"Start_Date\": \"21-Jun-09\",\n",
      "            \"Contain_Control_Date\": \"August\",\n",
      "            \"Size_Acres\": \"636,224\",\n",
      "            \"Cause\": \"L\",\n",
      "            \"Cost\": \"NR\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Saved the structured data to structured_incidents_data_2009.json\n"
     ]
    }
   ],
   "source": [
    "import json  \n",
    "import pandas as pd  \n",
    "  \n",
    "# Step 1: Read the 2009 CSV file  \n",
    "df_2009 = pd.read_csv('extracted_table_page_13_table_1.csv')  \n",
    "print(\"Loaded 2009 CSV data with\", len(df_2009), \"incidents.\")  \n",
    "  \n",
    "# Step 2: Rename the columns to match the 2008 JSON structure  \n",
    "column_mapping = {  \n",
    "    'Name': 'Name',  \n",
    "    'Inc.\\nType': 'Inc_Type',  \n",
    "    'GACC': 'GACC',  \n",
    "    'State': 'State',  \n",
    "    'Start Date': 'Start_Date',  \n",
    "    'Contain or\\nControl Date': 'Contain_Control_Date',  \n",
    "    'Size\\n(Acres)': 'Size_Acres',  \n",
    "    'Cause': 'Cause',  \n",
    "    'Estimated\\nCost': 'Cost'  \n",
    "}  \n",
    "  \n",
    "df_2009_cleaned = df_2009.rename(columns=column_mapping)  \n",
    "  \n",
    "# Step 3: Convert the DataFrame into a list of incident dictionaries  \n",
    "incidents_2009 = df_2009_cleaned.to_dict(orient='records')  \n",
    "  \n",
    "# Create new structured JSON data  \n",
    "structured_data_2009 = {  \n",
    "    \"significant_incidents\": incidents_2009  \n",
    "}  \n",
    "  \n",
    "# Step 4: Output the JSON into the notebook  \n",
    "json_output = json.dumps(structured_data_2009, indent=4)  \n",
    "print(\"Structured JSON data for the 2009 incidents:\")  \n",
    "print(json_output)  \n",
    "  \n",
    "# Step 5: Save the JSON data into a new file  \n",
    "with open('structured_incidents_data_2009.json', 'w') as f:  \n",
    "    json.dump(structured_data_2009, f, indent=4)  \n",
    "  \n",
    "print(\"Saved the structured data to structured_incidents_data_2009.json\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c09e2-556b-4e0f-ad13-50708f44cf20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
