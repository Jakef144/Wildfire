{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa73c642-c345-4cfe-98f7-af07a6200749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2013 PDF...\n",
      "Found 2 tables on pages: [12, 15]\n",
      "\n",
      "Table 1 on page 12 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start\\nDate', 'Last\\nReport\\nDate', 'Size In\\nAcres', 'Cause*', 'Estimated\\nCost']\n",
      "\n",
      "Significant incidents table found:\n",
      "           Name GACC State Start\\nDate Last\\nReport\\nDate Size In\\nAcres  \\\n",
      "0           Rim   SO    CA        8/17              10/24        257,314   \n",
      "1    Lime Hills   AK    AK        5/31               8/29        201,808   \n",
      "2   Moore Creek   AK    AK         6/2               8/29        157,747   \n",
      "3  Pony Complex   EB    ID         8/9               8/19        149,384   \n",
      "4        Silver   SW    NM         6/7               9/10        138,546   \n",
      "\n",
      "  Cause* Estimated\\nCost  \n",
      "0      U    $127,350,000  \n",
      "1      L      $2,883,457  \n",
      "2      L        $371,499  \n",
      "3      L      $4,000,000  \n",
      "4      L     $14,300,000  \n",
      "\n",
      "Saved raw extracted data to incidents_2013.json\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start_Date': 'Start_Date', 'Size In_Acres': 'Size_Acres', 'Cause*': 'Cause', 'Estimated_Cost': 'Cost'}\n",
      "Added missing column: Inc_Type\n",
      "Added missing column: Contain_Control_Date\n",
      "\n",
      "Saved structured data to structured_incidents_data_2013.json\n",
      "Sample structured incident:\n",
      "{\n",
      "  \"Name\": \"Rim\",\n",
      "  \"GACC\": \"SO\",\n",
      "  \"State\": \"CA\",\n",
      "  \"Start_Date\": \"8/17\",\n",
      "  \"Last_Report_Date\": \"10/24\",\n",
      "  \"Size_Acres\": \"257,314\",\n",
      "  \"Cause\": \"U\",\n",
      "  \"Cost\": \"$127,350,000\",\n",
      "  \"Inc_Type\": null,\n",
      "  \"Contain_Control_Date\": null\n",
      "}\n",
      "\n",
      "Created dimension tables and saved to dimension_tables_2013.json\n",
      "- State dimension: 8 states\n",
      "- GACC dimension: 7 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 18 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the 2013 PDF  \n",
    "pdf_path = 'Annual_Report_2013_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Define expected page range (similar to previous years)  \n",
    "start_page = 7   # 0-indexed (page 8)  \n",
    "end_page = 15    # 0-indexed (page 16)  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2013 PDF...\")  \n",
    "  \n",
    "# Function to extract tables from a page using PyMuPDF's table extraction  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "# Extract tables from the given pages  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # 1-indexed page numbers  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the significant incidents table by inspecting the column headers  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    # Use the found table for processing  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table found:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean up column names by stripping whitespace and replacing newline characters with underscores  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw table as JSON (call this incidents_2013.json)  \n",
    "    raw_data = incidents_table.to_dict(orient='records')  \n",
    "    raw_filename = 'incidents_2013.json'  \n",
    "    with open(raw_filename, 'w') as f:  \n",
    "        json.dump(raw_data, f, indent=4)  \n",
    "    print(\"\\nSaved raw extracted data to \" + raw_filename)  \n",
    "      \n",
    "    # Define a mapping to standardize the column names (adjust based on actual column names)  \n",
    "    column_mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        col_lower = col.lower()  \n",
    "        if 'name' in col_lower:  \n",
    "            column_mapping[col] = 'Name'  \n",
    "        elif 'type' in col_lower:  \n",
    "            column_mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in col_lower:  \n",
    "            column_mapping[col] = 'GACC'  \n",
    "        elif 'state' in col_lower:  \n",
    "            column_mapping[col] = 'State'  \n",
    "        elif 'start' in col_lower and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Start_Date'  \n",
    "        elif ('contain' in col_lower or 'control' in col_lower) and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Contain_Control_Date'  \n",
    "        elif 'size' in col_lower or 'acres' in col_lower:  \n",
    "            column_mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in col_lower:  \n",
    "            column_mapping[col] = 'Cause'  \n",
    "        elif 'cost' in col_lower:  \n",
    "            column_mapping[col] = 'Cost'  \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(column_mapping)  \n",
    "      \n",
    "    # Apply the mapping to create the structured incidents data  \n",
    "    structured_table = incidents_table.rename(columns=column_mapping)  \n",
    "      \n",
    "    # For any required key columns not present post-mapping, add them as None  \n",
    "    required_columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date', 'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']  \n",
    "    for col in required_columns:  \n",
    "        if col not in structured_table.columns:  \n",
    "            structured_table[col] = None  \n",
    "            print(\"Added missing column: \" + col)  \n",
    "      \n",
    "    structured_data = structured_table.to_dict(orient='records')  \n",
    "    structured_filename = 'structured_incidents_data_2013.json'  \n",
    "    with open(structured_filename, 'w') as f:  \n",
    "        json.dump(structured_data, f, indent=4)  \n",
    "    print(\"\\nSaved structured data to \" + structured_filename)  \n",
    "    print(\"Sample structured incident:\")  \n",
    "    print(json.dumps(structured_data[0], indent=2))  \n",
    "      \n",
    "    # Create dimension tables  \n",
    "    # State dimension  \n",
    "    states = structured_table['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = structured_table['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension  \n",
    "    inc_types = structured_table['Inc_Type'].dropna().unique()  \n",
    "    inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = structured_table['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension (using Start_Date values)  \n",
    "    dates = structured_table['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2013.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "    print(\"\\nCreated dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "      \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table in the given page range.\")  \n",
    "  \n",
    "print(\"\\nProcess completed.\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
