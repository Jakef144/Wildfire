{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778c07a4-fda6-4b64-a514-1323f34690bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16...\n",
      "Found 2 tables on pages: [9, 11]\n",
      "Table 1 on page 9 has shape (9, 9):\n",
      "         Name Inc.\\nType GACC State Start Date Contain or\\nControl\\nDate  \\\n",
      "0  Long Butte         WF   EB    ID  21-Aug-10                  3-Sep-10   \n",
      "1      Toklat         WF   AK    AK  16-May-10                  4-Jun-10   \n",
      "\n",
      "  Size\\n(Acres) Cause Estimated\\nCost  \n",
      "0       306,113     L     $ 4,225,000  \n",
      "1       171,727     L      $2,109,186  \n",
      "Table 2 on page 11 has shape (1, 11):\n",
      "    AK  NW  NO  SO  NR  EB  WB  SW  RM   EA   SA\n",
      "0  11%  4%  2%  4%  4%  9%  1%  7%  6%  11%  42%\n",
      "Table 1 on page 9 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'Inc.\\nType', 'GACC', 'State', 'Start Date', 'Contain or\\nControl\\nDate', 'Size\\n(Acres)', 'Cause', 'Estimated\\nCost']\n",
      "Significant incidents table found:\n",
      "             Name Inc.\\nType GACC State Start Date Contain or\\nControl\\nDate  \\\n",
      "0      Long Butte         WF   EB    ID  21-Aug-10                  3-Sep-10   \n",
      "1          Toklat         WF   AK    AK  16-May-10                  4-Jun-10   \n",
      "2       Jefferson         WF   EB    ID  13-Jul-10                 17-Jul-10   \n",
      "3  Turquoise Lake         WF   AK    AK  18-May-10                  7-Sep-10   \n",
      "4    Big Mountain         WF   AK    AK   2-Jun-10                  2-Sep-10   \n",
      "\n",
      "  Size\\n(Acres) Cause Estimated\\nCost  \n",
      "0       306,113     L     $ 4,225,000  \n",
      "1       171,727     L      $2,109,186  \n",
      "2       109,436     H       $ 700,819  \n",
      "3        91,885     H        $317,515  \n",
      "4        83,746     L              NR  \n",
      "Saved raw table to significant_incidents_2010.csv\n",
      "Actual columns in the 2010 table:\n",
      "['_N_a_m_e_', '_I_n_c_._\\n_T_y_p_e_', '_G_A_C_C_', '_S_t_a_t_e_', '_S_t_a_r_t_ _D_a_t_e_', '_C_o_n_t_a_i_n_ _o_r_\\n_C_o_n_t_r_o_l_\\n_D_a_t_e_', '_S_i_z_e_\\n_(_A_c_r_e_s_)_', '_C_a_u_s_e_', '_E_s_t_i_m_a_t_e_d_\\n_C_o_s_t_']\n",
      "Proposed column mapping:\n",
      "{}\n",
      "Could not create a complete column mapping. Manual mapping needed.\n",
      "Detailed view of the table:\n",
      "        _N_a_m_e_ _I_n_c_._\\n_T_y_p_e_ _G_A_C_C_ _S_t_a_t_e_  \\\n",
      "0      Long Butte                   WF        EB          ID   \n",
      "1          Toklat                   WF        AK          AK   \n",
      "2       Jefferson                   WF        EB          ID   \n",
      "3  Turquoise Lake                   WF        AK          AK   \n",
      "4    Big Mountain                   WF        AK          AK   \n",
      "\n",
      "  _S_t_a_r_t_ _D_a_t_e_ _C_o_n_t_a_i_n_ _o_r_\\n_C_o_n_t_r_o_l_\\n_D_a_t_e_  \\\n",
      "0             21-Aug-10                                          3-Sep-10   \n",
      "1             16-May-10                                          4-Jun-10   \n",
      "2             13-Jul-10                                         17-Jul-10   \n",
      "3             18-May-10                                          7-Sep-10   \n",
      "4              2-Jun-10                                          2-Sep-10   \n",
      "\n",
      "  _S_i_z_e_\\n_(_A_c_r_e_s_)_ _C_a_u_s_e_ _E_s_t_i_m_a_t_e_d_\\n_C_o_s_t_  \n",
      "0                    306,113           L                    $ 4,225,000  \n",
      "1                    171,727           L                     $2,109,186  \n",
      "2                    109,436           H                      $ 700,819  \n",
      "3                     91,885           H                       $317,515  \n",
      "4                     83,746           L                             NR  \n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the PDF to find the significant incidents table\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load the PDF\n",
    "pdf_path = 'annual_report_2010_508.pdf'\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# Based on the TOC, the significant fire activity is on page 8\n",
    "# Let's check a few pages around that to find the table\n",
    "start_page = 7  # 0-indexed, so page 8 is index 7\n",
    "end_page = 15   # Check a few pages after\n",
    "\n",
    "print(f\"Searching for significant incidents table in pages {start_page+1} to {end_page+1}...\")\n",
    "\n",
    "# Function to extract tables from a page\n",
    "def extract_tables_from_page(page):\n",
    "    tables = page.find_tables()\n",
    "    if tables and tables.tables:\n",
    "        return [table.to_pandas() for table in tables.tables]\n",
    "    return []\n",
    "\n",
    "# Search for tables in the specified page range\n",
    "all_tables = []\n",
    "table_pages = []\n",
    "\n",
    "for page_num in range(start_page, end_page + 1):\n",
    "    page = doc[page_num]\n",
    "    tables = extract_tables_from_page(page)\n",
    "    if tables:\n",
    "        all_tables.extend(tables)\n",
    "        table_pages.append(page_num + 1)  # Convert to 1-indexed for reporting\n",
    "\n",
    "print(f\"Found {len(all_tables)} tables on pages: {table_pages}\")\n",
    "\n",
    "# Let's examine each table to find the one with significant incidents\n",
    "for i, table in enumerate(all_tables):\n",
    "    print(f\"\\\n",
    "Table {i+1} on page {table_pages[i]} has shape {table.shape}:\")\n",
    "    print(table.head(2))  # Show first 2 rows to identify the table\n",
    "\n",
    "# Based on the output, let's identify which table contains the significant incidents\n",
    "# Let's assume it's the table with columns like Name, Type, GACC, State, etc.\n",
    "# We'll examine each table more carefully\n",
    "\n",
    "significant_table_index = None\n",
    "for i, table in enumerate(all_tables):\n",
    "    # Check if this looks like the significant incidents table\n",
    "    columns = table.columns.tolist()\n",
    "    column_str = ', '.join(columns).lower()\n",
    "    \n",
    "    # Look for keywords that might indicate this is the incidents table\n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):\n",
    "        significant_table_index = i\n",
    "        print(f\"\\\n",
    "Table {i+1} on page {table_pages[i]} appears to be the significant incidents table.\")\n",
    "        print(\"Columns:\", columns)\n",
    "        break\n",
    "\n",
    "# If we found the table, let's process it\n",
    "if significant_table_index is not None:\n",
    "    incidents_table = all_tables[significant_table_index]\n",
    "    print(\"\\\n",
    "Significant incidents table found:\")\n",
    "    print(incidents_table.head())\n",
    "    \n",
    "    # Clean up column names\n",
    "    incidents_table.columns = [col.strip().replace('\\\n",
    "', '_') for col in incidents_table.columns]\n",
    "    \n",
    "    # Save the raw table to CSV\n",
    "    csv_filename = 'significant_incidents_2010.csv'\n",
    "    incidents_table.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\\n",
    "Saved raw table to {csv_filename}\")\n",
    "    \n",
    "    # Create the structured JSON format (similar to 2008/2009)\n",
    "    # First, map column names to match the 2008/2009 format\n",
    "    column_mapping = {}\n",
    "    \n",
    "    # We need to inspect the actual columns to create the mapping\n",
    "    print(\"\\\n",
    "Actual columns in the 2010 table:\")\n",
    "    print(incidents_table.columns.tolist())\n",
    "    \n",
    "    # Let's try to automatically map columns based on keywords\n",
    "    for col in incidents_table.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'name' in col_lower:\n",
    "            column_mapping[col] = 'Name'\n",
    "        elif 'type' in col_lower:\n",
    "            column_mapping[col] = 'Inc_Type'\n",
    "        elif 'gacc' in col_lower:\n",
    "            column_mapping[col] = 'GACC'\n",
    "        elif 'state' in col_lower:\n",
    "            column_mapping[col] = 'State'\n",
    "        elif 'start' in col_lower and 'date' in col_lower:\n",
    "            column_mapping[col] = 'Start_Date'\n",
    "        elif ('contain' in col_lower or 'control' in col_lower) and 'date' in col_lower:\n",
    "            column_mapping[col] = 'Contain_Control_Date'\n",
    "        elif 'size' in col_lower or 'acres' in col_lower:\n",
    "            column_mapping[col] = 'Size_Acres'\n",
    "        elif 'cause' in col_lower:\n",
    "            column_mapping[col] = 'Cause'\n",
    "        elif 'cost' in col_lower:\n",
    "            column_mapping[col] = 'Cost'\n",
    "    \n",
    "    print(\"\\\n",
    "Proposed column mapping:\")\n",
    "    print(column_mapping)\n",
    "    \n",
    "    # Apply the mapping if it's complete\n",
    "    if len(column_mapping) >= 5:  # Assuming we need at least 5 key columns\n",
    "        incidents_table_mapped = incidents_table.rename(columns=column_mapping)\n",
    "        \n",
    "        # For any missing columns in our mapping, add them with None values\n",
    "        required_columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date', \n",
    "                           'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col not in incidents_table_mapped.columns:\n",
    "                incidents_table_mapped[col] = None\n",
    "                print(f\"Added missing column: {col}\")\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        incidents_2010 = incidents_table_mapped.to_dict(orient='records')\n",
    "        \n",
    "        # Create the structured data\n",
    "        structured_data_2010 = {\n",
    "            \"significant_incidents\": incidents_2010\n",
    "        }\n",
    "        \n",
    "        # Save to JSON\n",
    "        json_filename = 'structured_incidents_data_2010.json'\n",
    "        with open(json_filename, 'w') as f:\n",
    "            json.dump(structured_data_2010, f, indent=4)\n",
    "        \n",
    "        print(f\"\\\n",
    "Saved structured data to {json_filename}\")\n",
    "        print(\"Sample incident:\")\n",
    "        print(json.dumps(incidents_2010[0], indent=2))\n",
    "        \n",
    "        # Also create the untouched JSON\n",
    "        untouched_json = incidents_table.to_dict(orient='records')\n",
    "        untouched_filename = 'untouched_incidents_2010.json'\n",
    "        with open(untouched_filename, 'w') as f:\n",
    "            json.dump(untouched_json, f, indent=4)\n",
    "        \n",
    "        print(f\"\\\n",
    "Saved untouched data to {untouched_filename}\")\n",
    "        \n",
    "        # Create dimension tables\n",
    "        # State dimension\n",
    "        states = incidents_table_mapped['State'].dropna().unique()\n",
    "        state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]\n",
    "        \n",
    "        # GACC dimension\n",
    "        gaccs = incidents_table_mapped['GACC'].dropna().unique()\n",
    "        gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]\n",
    "        \n",
    "        # Incident Type dimension\n",
    "        inc_types = incidents_table_mapped['Inc_Type'].dropna().unique()\n",
    "        inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]\n",
    "        \n",
    "        # Cause dimension\n",
    "        causes = incidents_table_mapped['Cause'].dropna().unique()\n",
    "        cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]\n",
    "        \n",
    "        # Time dimension (simplified)\n",
    "        dates = incidents_table_mapped['Start_Date'].dropna().unique()\n",
    "        time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]\n",
    "        \n",
    "        # Create the dimension tables JSON\n",
    "        dimension_tables = {\n",
    "            'state_dimension': state_dimension,\n",
    "            'gacc_dimension': gacc_dimension,\n",
    "            'inc_type_dimension': inc_type_dimension,\n",
    "            'cause_dimension': cause_dimension,\n",
    "            'time_dimension': time_dimension\n",
    "        }\n",
    "        \n",
    "        # Save the dimension tables\n",
    "        dimension_filename = 'dimension_tables_2010.json'\n",
    "        with open(dimension_filename, 'w') as f:\n",
    "            json.dump(dimension_tables, f, indent=4)\n",
    "        \n",
    "        print(f\"\\\n",
    "Created {dimension_filename} with the following dimensions:\")\n",
    "        print(f\"- State dimension: {len(state_dimension)} states\")\n",
    "        print(f\"- GACC dimension: {len(gacc_dimension)} GACCs\")\n",
    "        print(f\"- Incident Type dimension: {len(inc_type_dimension)} types\")\n",
    "        print(f\"- Cause dimension: {len(cause_dimension)} causes\")\n",
    "        print(f\"- Time dimension: {len(time_dimension)} dates\")\n",
    "    else:\n",
    "        print(\"\\\n",
    "Could not create a complete column mapping. Manual mapping needed.\")\n",
    "        \n",
    "        # Let's create a more detailed view of the table to help with manual mapping\n",
    "        print(\"\\\n",
    "Detailed view of the table:\")\n",
    "        print(incidents_table.head(5))\n",
    "else:\n",
    "    print(\"\\\n",
    "Could not identify the significant incidents table.\")\n",
    "    \n",
    "    # Let's try a different approach - look for text that might indicate the table\n",
    "    for page_num in range(start_page, end_page + 1):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        if \"significant\" in text.lower() and (\"fire\" in text.lower() or \"incident\" in text.lower()):\n",
    "            print(f\"\\\n",
    "Page {page_num+1} contains text about significant fires/incidents.\")\n",
    "            print(\"First 500 characters of the page:\")\n",
    "            print(text[:500])\n",
    "\n",
    "print(\"\\\n",
    "Process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4da45ea-6f81-496a-bb35-5a76137bc658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16...\n",
      "Found 2 tables on pages: [9, 11]\n",
      "\n",
      "Table 1 on page 9 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'Inc.\\nType', 'GACC', 'State', 'Start Date', 'Contain or\\nControl\\nDate', 'Size\\n(Acres)', 'Cause', 'Estimated\\nCost']\n",
      "\n",
      "Significant incidents table found:\n",
      "             Name Inc.\\nType GACC State Start Date Contain or\\nControl\\nDate  \\\n",
      "0      Long Butte         WF   EB    ID  21-Aug-10                  3-Sep-10   \n",
      "1          Toklat         WF   AK    AK  16-May-10                  4-Jun-10   \n",
      "2       Jefferson         WF   EB    ID  13-Jul-10                 17-Jul-10   \n",
      "3  Turquoise Lake         WF   AK    AK  18-May-10                  7-Sep-10   \n",
      "4    Big Mountain         WF   AK    AK   2-Jun-10                  2-Sep-10   \n",
      "\n",
      "  Size\\n(Acres) Cause Estimated\\nCost  \n",
      "0       306,113     L     $ 4,225,000  \n",
      "1       171,727     L      $2,109,186  \n",
      "2       109,436     H       $ 700,819  \n",
      "3        91,885     H        $317,515  \n",
      "4        83,746     L              NR  \n",
      "\n",
      "Saved raw table to significant_incidents_2010.csv\n",
      "\n",
      "Actual columns in the 2010 table:\n",
      "['Name', 'Inc._Type', 'GACC', 'State', 'Start Date', 'Contain or_Control_Date', 'Size_(Acres)', 'Cause', 'Estimated_Cost']\n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'Inc._Type': 'Inc_Type', 'GACC': 'GACC', 'State': 'State', 'Start Date': 'Start_Date', 'Contain or_Control_Date': 'Contain_Control_Date', 'Size_(Acres)': 'Size_Acres', 'Cause': 'Cause', 'Estimated_Cost': 'Cost'}\n",
      "\n",
      "Saved structured data to structured_incidents_data_2010.json\n",
      "Sample incident:\n",
      "{\n",
      "  \"Name\": \"Long Butte\",\n",
      "  \"Inc_Type\": \"WF\",\n",
      "  \"GACC\": \"EB\",\n",
      "  \"State\": \"ID\",\n",
      "  \"Start_Date\": \"21-Aug-10\",\n",
      "  \"Contain_Control_Date\": \"3-Sep-10\",\n",
      "  \"Size_Acres\": \"306,113\",\n",
      "  \"Cause\": \"L\",\n",
      "  \"Cost\": \"$ 4,225,000\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF  \n",
    "pdf_path = 'annual_report_2010_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# Based on the TOC, we expect the significant incidents table on page 8 or nearby.  \n",
    "# We'll check pages 8 to 16 (0-indexed: pages 7 to 15)  \n",
    "start_page = 7  # page 8 (0-indexed)  \n",
    "end_page = 15   # page 16  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \"...\")  \n",
    "  \n",
    "# Function to extract tables from a page  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "# Search for tables in the specified page range  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "for page_num in range(start_page, end_page + 1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num + 1)  # 1-indexed  \n",
    "  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Examine tables to find the one with significant incidents  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "      \n",
    "    # Look for keywords like name, state, fire, or acres to decide if it is the incidents table  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table found:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean up column names  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Save the raw table as CSV for reference  \n",
    "    csv_filename = 'significant_incidents_2010.csv'  \n",
    "    incidents_table.to_csv(csv_filename, index=False)  \n",
    "    print(\"\\nSaved raw table to \" + csv_filename)  \n",
    "      \n",
    "    # Map columns to match the expected structure (similar to 2008/2009)  \n",
    "    column_mapping = {}  \n",
    "    print(\"\\nActual columns in the 2010 table:\")  \n",
    "    print(incidents_table.columns.tolist())  \n",
    "      \n",
    "    # Map based on keywords  \n",
    "    for col in incidents_table.columns:  \n",
    "        col_lower = col.lower()  \n",
    "        if 'name' in col_lower:  \n",
    "            column_mapping[col] = 'Name'  \n",
    "        elif 'type' in col_lower:  \n",
    "            column_mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in col_lower:  \n",
    "            column_mapping[col] = 'GACC'  \n",
    "        elif 'state' in col_lower:  \n",
    "            column_mapping[col] = 'State'  \n",
    "        elif 'start' in col_lower and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Start_Date'  \n",
    "        elif ('contain' in col_lower or 'control' in col_lower) and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Contain_Control_Date'  \n",
    "        elif 'size' in col_lower or 'acres' in col_lower:  \n",
    "            column_mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in col_lower:  \n",
    "            column_mapping[col] = 'Cause'  \n",
    "        elif 'cost' in col_lower:  \n",
    "            column_mapping[col] = 'Cost'  \n",
    "      \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(column_mapping)  \n",
    "      \n",
    "    # Apply mapping if sufficient key columns exist  \n",
    "    if len(column_mapping) >= 5:  \n",
    "        incidents_table_mapped = incidents_table.rename(columns=column_mapping)  \n",
    "          \n",
    "        # Ensure required columns exist; add missing columns as None  \n",
    "        required_columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date',   \n",
    "                           'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']  \n",
    "        for col in required_columns:  \n",
    "            if col not in incidents_table_mapped.columns:  \n",
    "                incidents_table_mapped[col] = None  \n",
    "                print(\"Added missing column: \" + col)  \n",
    "          \n",
    "        # Convert the mapped DataFrame to a list of dictionaries  \n",
    "        incidents_2010 = incidents_table_mapped.to_dict(orient='records')  \n",
    "          \n",
    "        # Create the structured data JSON  \n",
    "        structured_data_2010 = {  \n",
    "            \"significant_incidents\": incidents_2010  \n",
    "        }  \n",
    "          \n",
    "        # Save the structured JSON  \n",
    "        json_filename = 'structured_incidents_data_2010.json'  \n",
    "        with open(json_filename, 'w') as f:  \n",
    "            json.dump(structured_data_2010, f, indent=4)  \n",
    "              \n",
    "        print(\"\\nSaved structured data to \" + json_filename)  \n",
    "        print(\"Sample incident:\")  \n",
    "        print(json.dumps(incidents_2010[0], indent=2))  \n",
    "    else:  \n",
    "        print(\"\\nCould not create a complete column mapping. Manual mapping needed.\")  \n",
    "else:  \n",
    "    print(\"\\nCould not identify the significant incidents table.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ffe96e-d19a-4567-88e1-1cc12c87227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved untouched data to untouched_incidents_2010.json\n"
     ]
    }
   ],
   "source": [
    "# Continuing from the previous code, if we have the raw incidents_table:  \n",
    "if 'incidents_table' in locals():  \n",
    "    # Convert the raw table (untouched) directly to JSON  \n",
    "    untouched_json = incidents_table.to_dict(orient='records')  \n",
    "    untouched_filename = 'untouched_incidents_2010.json'  \n",
    "    with open(untouched_filename, 'w') as f:  \n",
    "        json.dump(untouched_json, f, indent=4)  \n",
    "      \n",
    "    print(\"\\nSaved untouched data to \" + untouched_filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4523c8cf-08b8-4298-a53a-be41257d0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created dimension_tables_2010.json with the following dimensions:\n",
      "- State dimension: 3 states\n",
      "- GACC dimension: 2 GACCs\n",
      "- Incident Type dimension: 1 types\n",
      "- Cause dimension: 2 causes\n",
      "- Time dimension: 9 dates\n"
     ]
    }
   ],
   "source": [
    "# Once we have the mapped table (incidents_table_mapped), we can create dimension tables.  \n",
    "if 'incidents_table_mapped' in locals():  \n",
    "    # State dimension  \n",
    "    states = incidents_table_mapped['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = incidents_table_mapped['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension  \n",
    "    inc_types = incidents_table_mapped['Inc_Type'].dropna().unique()  \n",
    "    inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = incidents_table_mapped['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension (using Start_Date as-is)  \n",
    "    dates = incidents_table_mapped['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    # Structure the dimension tables together  \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    # Save the dimension tables to a JSON file  \n",
    "    dimension_filename = 'dimension_tables_2010.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "          \n",
    "    print(\"\\nCreated \" + dimension_filename + \" with the following dimensions:\")  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
