{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "febc3636-8ab2-48e1-9f4e-95501215bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for significant incidents table in pages 8 to 16 of the 2011 PDF...\n",
      "Found 2 tables on pages: [12, 14]\n",
      "\n",
      "Table 1 on page 12 appears to be the significant incidents table.\n",
      "Columns: ['Name', 'GACC', 'State', 'Start Date', 'Contain or\\nControl\\nDate', 'Size\\n(Acres)', 'Cause', 'Estimated\\nCost']\n",
      "\n",
      "Significant incidents table found:\n",
      "            Name GACC State Start Date Contain or\\nControl\\nDate  \\\n",
      "0         Wallow   SW    AZ  29-May-11                  8-Jul-11   \n",
      "1     Rock House   SA    TX   9-Apr-11                 12-May-11   \n",
      "2  Honey Prairie   SA    GA  30-Apr-11                 28-Dec-11   \n",
      "3    Horseshoe 2   SW    AZ   8-May-11                 20-Jul-11   \n",
      "4    Deaton Cole   SA    TX  25-Apr-11                 11-May-11   \n",
      "\n",
      "  Size\\n(Acres) Cause Estimated\\nCost  \n",
      "0       538,049     U    $109,000,000  \n",
      "1       314,444     H      $8,399,072  \n",
      "2       309,200     L     $53,420,000  \n",
      "3       222,954     H     $52,000,000  \n",
      "4       175,000     U              NR  \n",
      "\n",
      "Proposed column mapping:\n",
      "{'Name': 'Name', 'GACC': 'GACC', 'State': 'State', 'Start Date': 'Start_Date', 'Contain or_Control_Date': 'Contain_Control_Date', 'Size_(Acres)': 'Size_Acres', 'Cause': 'Cause', 'Estimated_Cost': 'Cost'}\n",
      "Added missing column: Inc_Type\n",
      "\n",
      "Saved structured JSON data to structured_incidents_data_2011.json\n",
      "Sample incident (structured):\n",
      "{\n",
      "  \"Name\": \"Wallow\",\n",
      "  \"GACC\": \"SW\",\n",
      "  \"State\": \"AZ\",\n",
      "  \"Start_Date\": \"29-May-11\",\n",
      "  \"Contain_Control_Date\": \"8-Jul-11\",\n",
      "  \"Size_Acres\": \"538,049\",\n",
      "  \"Cause\": \"U\",\n",
      "  \"Cost\": \"$109,000,000\",\n",
      "  \"Inc_Type\": null\n",
      "}\n",
      "\n",
      "Saved untouched JSON data to untouched_incidents_2011.json\n",
      "\n",
      "Created dimension tables and saved to dimension_tables_2011.json\n",
      "- State dimension: 14 states\n",
      "- GACC dimension: 8 GACCs\n",
      "- Incident Type dimension: 0 types\n",
      "- Cause dimension: 3 causes\n",
      "- Time dimension: 30 dates\n",
      "\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF  \n",
    "import pandas as pd  \n",
    "import json  \n",
    "import re  \n",
    "  \n",
    "# Load the PDF for 2011 annual report  \n",
    "pdf_path = 'annual_report_2011_508.pdf'  \n",
    "doc = fitz.open(pdf_path)  \n",
    "  \n",
    "# We use the table of contents (if available) or the expected page range.  \n",
    "# Assuming the significant incidents table is near page 9 (as in previous reports)  \n",
    "start_page = 7  # 0-indexed, page 8  \n",
    "end_page = 15   # 0-indexed, page 16  \n",
    "  \n",
    "print(\"Searching for significant incidents table in pages \" + str(start_page+1) + \" to \" + str(end_page+1) + \" of the 2011 PDF...\")  \n",
    "  \n",
    "def extract_tables_from_page(page):  \n",
    "    tables = page.find_tables()  \n",
    "    if tables and tables.tables:  \n",
    "        return [table.to_pandas() for table in tables.tables]  \n",
    "    return []  \n",
    "  \n",
    "all_tables = []  \n",
    "table_pages = []  \n",
    "  \n",
    "for page_num in range(start_page, end_page+1):  \n",
    "    page = doc[page_num]  \n",
    "    tables = extract_tables_from_page(page)  \n",
    "    if tables:  \n",
    "        all_tables.extend(tables)  \n",
    "        table_pages.append(page_num+1)  # 1-indexed  \n",
    "  \n",
    "print(\"Found \" + str(len(all_tables)) + \" tables on pages: \" + str(table_pages))  \n",
    "  \n",
    "# Identify the table likely containing the significant incidents data:  \n",
    "significant_table_index = None  \n",
    "for i, table in enumerate(all_tables):  \n",
    "    columns = table.columns.tolist()  \n",
    "    column_str = ', '.join(columns).lower()  \n",
    "    if ('name' in column_str and 'state' in column_str) or ('fire' in column_str and 'acres' in column_str):  \n",
    "        significant_table_index = i  \n",
    "        print(\"\\nTable \" + str(i+1) + \" on page \" + str(table_pages[i]) + \" appears to be the significant incidents table.\")  \n",
    "        print(\"Columns:\", columns)  \n",
    "        break  \n",
    "  \n",
    "if significant_table_index is not None:  \n",
    "    incidents_table = all_tables[significant_table_index]  \n",
    "    print(\"\\nSignificant incidents table found:\")  \n",
    "    print(incidents_table.head())  \n",
    "      \n",
    "    # Clean column names (replace newlines with underscores, strip spaces)  \n",
    "    incidents_table.columns = [col.strip().replace('\\n', '_') for col in incidents_table.columns]  \n",
    "      \n",
    "    # Create the structured JSON (mapped to common format)  \n",
    "    # We manually map columns names here; adjust as needed based on actual column names  \n",
    "    column_mapping = {}  \n",
    "    for col in incidents_table.columns:  \n",
    "        col_lower = col.lower()  \n",
    "        if 'name' in col_lower:  \n",
    "            column_mapping[col] = 'Name'  \n",
    "        elif 'type' in col_lower:  \n",
    "            column_mapping[col] = 'Inc_Type'  \n",
    "        elif 'gacc' in col_lower:  \n",
    "            column_mapping[col] = 'GACC'  \n",
    "        elif 'state' in col_lower:  \n",
    "            column_mapping[col] = 'State'  \n",
    "        elif 'start' in col_lower and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Start_Date'  \n",
    "        elif ('contain' in col_lower or 'control' in col_lower) and 'date' in col_lower:  \n",
    "            column_mapping[col] = 'Contain_Control_Date'  \n",
    "        elif 'size' in col_lower or 'acres' in col_lower:  \n",
    "            column_mapping[col] = 'Size_Acres'  \n",
    "        elif 'cause' in col_lower:  \n",
    "            column_mapping[col] = 'Cause'  \n",
    "        elif 'cost' in col_lower:  \n",
    "            column_mapping[col] = 'Cost'  \n",
    "      \n",
    "    print(\"\\nProposed column mapping:\")  \n",
    "    print(column_mapping)  \n",
    "      \n",
    "    # Rename columns using the mapping  \n",
    "    incidents_table_mapped = incidents_table.rename(columns=column_mapping)  \n",
    "      \n",
    "    # Ensure that all required columns exist, add missing ones as None  \n",
    "    required_columns = ['Name', 'Inc_Type', 'GACC', 'State', 'Start_Date',   \n",
    "                        'Contain_Control_Date', 'Size_Acres', 'Cause', 'Cost']  \n",
    "    for col in required_columns:  \n",
    "        if col not in incidents_table_mapped.columns:  \n",
    "            incidents_table_mapped[col] = None  \n",
    "            print(\"Added missing column:\", col)  \n",
    "      \n",
    "    # Create structured JSON  \n",
    "    incidents_2011 = incidents_table_mapped.to_dict(orient='records')  \n",
    "    structured_data_2011 = {\"significant_incidents\": incidents_2011}  \n",
    "      \n",
    "    structured_filename = 'structured_incidents_data_2011.json'  \n",
    "    with open(structured_filename, 'w') as f:  \n",
    "        json.dump(structured_data_2011, f, indent=4)  \n",
    "    print(\"\\nSaved structured JSON data to \" + structured_filename)  \n",
    "    print(\"Sample incident (structured):\")  \n",
    "    print(json.dumps(incidents_2011[0], indent=2))  \n",
    "      \n",
    "    # Create an untouched JSON file (direct output from table extraction)  \n",
    "    untouched_json = incidents_table.to_dict(orient='records')  \n",
    "    untouched_filename = 'untouched_incidents_2011.json'  \n",
    "    with open(untouched_filename, 'w') as f:  \n",
    "        json.dump(untouched_json, f, indent=4)  \n",
    "    print(\"\\nSaved untouched JSON data to \" + untouched_filename)  \n",
    "      \n",
    "    # Create dimension tables  \n",
    "    # State dimension  \n",
    "    states = incidents_table_mapped['State'].dropna().unique()  \n",
    "    state_dimension = [{'State_ID': i+1, 'State': state} for i, state in enumerate(sorted(states))]  \n",
    "      \n",
    "    # GACC dimension  \n",
    "    gaccs = incidents_table_mapped['GACC'].dropna().unique()  \n",
    "    gacc_dimension = [{'GACC_ID': i+1, 'GACC': gacc} for i, gacc in enumerate(sorted(gaccs))]  \n",
    "      \n",
    "    # Incident Type dimension  \n",
    "    inc_types = incidents_table_mapped['Inc_Type'].dropna().unique()  \n",
    "    inc_type_dimension = [{'Inc_Type_ID': i+1, 'Inc_Type': inc_type} for i, inc_type in enumerate(sorted(inc_types))]  \n",
    "      \n",
    "    # Cause dimension  \n",
    "    causes = incidents_table_mapped['Cause'].dropna().unique()  \n",
    "    cause_dimension = [{'Cause_ID': i+1, 'Cause': cause} for i, cause in enumerate(sorted(causes))]  \n",
    "      \n",
    "    # Time dimension using Start_Date values  \n",
    "    dates = incidents_table_mapped['Start_Date'].dropna().unique()  \n",
    "    time_dimension = [{'Date_ID': i+1, 'Date': date} for i, date in enumerate(sorted(dates))]  \n",
    "      \n",
    "    dimension_tables = {  \n",
    "        'state_dimension': state_dimension,  \n",
    "        'gacc_dimension': gacc_dimension,  \n",
    "        'inc_type_dimension': inc_type_dimension,  \n",
    "        'cause_dimension': cause_dimension,  \n",
    "        'time_dimension': time_dimension  \n",
    "    }  \n",
    "      \n",
    "    dimension_filename = 'dimension_tables_2011.json'  \n",
    "    with open(dimension_filename, 'w') as f:  \n",
    "        json.dump(dimension_tables, f, indent=4)  \n",
    "      \n",
    "    print(\"\\nCreated dimension tables and saved to \" + dimension_filename)  \n",
    "    print(\"- State dimension: \" + str(len(state_dimension)) + \" states\")  \n",
    "    print(\"- GACC dimension: \" + str(len(gacc_dimension)) + \" GACCs\")  \n",
    "    print(\"- Incident Type dimension: \" + str(len(inc_type_dimension)) + \" types\")  \n",
    "    print(\"- Cause dimension: \" + str(len(cause_dimension)) + \" causes\")  \n",
    "    print(\"- Time dimension: \" + str(len(time_dimension)) + \" dates\")  \n",
    "      \n",
    "else:  \n",
    "    print(\"\\nCould not identify a significant incidents table in the given page range.\")  \n",
    "      \n",
    "print(\"\\nProcess completed.\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
