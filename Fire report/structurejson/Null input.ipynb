{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167f1dc5-2bc2-487a-b532-243b9a78e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean.csv\n",
      "Loaded structured_incidents_2012.json\n",
      "Missing required columns in incidents_2012_updated.json\n",
      "Updated 2012 rows from combined_incidents_clean_updated.csv (first 10 rows):\n",
      "                      Name   GACC  State  Start_Date Last_Report_Date  \\\n",
      "150              Long Draw     NW     OR  07/08/2012              NaN   \n",
      "151               Holloway  NW/WB  OR/NV  08/05/2012              NaN   \n",
      "152        Mustang Complex     EB     ID  07/30/2012              NaN   \n",
      "153                   Rush     NO     CA  08/12/2012              NaN   \n",
      "154       Whitewater-Baldy     SW     NM  05/16/2012              NaN   \n",
      "155              Ash Creek     NR     MT  06/25/2012              NaN   \n",
      "156            Kinyon Road     EB     ID  07/07/2012              NaN   \n",
      "157               Halstead     EB     ID  07/27/2012              NaN   \n",
      "158  Rosebud Creek Complex     NR     MT  08/01/2012              NaN   \n",
      "159       Miller Homestead     NW     OR  07/08/2012              NaN   \n",
      "\n",
      "    Size_Acres Cause         Cost Inc_Type Contain_Control_Date    Year  \n",
      "150    557,628     L   $4,360,000      NaN           07/30/2012  2012.0  \n",
      "151    460,850     L   $9,166,719      NaN           08/23/2012  2012.0  \n",
      "152    341,488     L  $38,323,413      NaN           10/18/2012  2012.0  \n",
      "153    315,577     L  $15,170,000      NaN           09/04/2012  2012.0  \n",
      "154    297,845     L  $23,000,000      NaN           07/31/2012  2012.0  \n",
      "155    249,562     L   $7,500,000      NaN           07/11/2012  2012.0  \n",
      "156    210,874     L   $1,625,000      NaN           07/19/2012  2012.0  \n",
      "157    181,948     L  $26,413,932      NaN           10/18/2012  2012.0  \n",
      "158    171,444     L   $9,000,000      NaN           08/16/2012  2012.0  \n",
      "159    160,853     L   $6,000,000      NaN           09/21/2012  2012.0  \n"
     ]
    }
   ],
   "source": [
    "# This code updates the combined_incidents_clean.csv using the updated incidents_2012 data\n",
    "# focusing on 2012 rows and ensuring that the Last_Report_Date field is updated from incidents_2012_updated.json.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the clean CSV file\n",
    "try:\n",
    "    df_clean = pd.read_csv('combined_incidents_clean.csv')\n",
    "    print('Loaded combined_incidents_clean.csv')\n",
    "except Exception as e:\n",
    "    print('Error loading combined_incidents_clean.csv:', e)\n",
    "    raise e\n",
    "\n",
    "# Load the updated incidents_2012 JSON file\n",
    "try:\n",
    "    df_2012 = pd.read_json('structured_incidents_data_2012.json')\n",
    "    print('Loaded structured_incidents_2012.json')\n",
    "except Exception as e:\n",
    "    print('Error loading incidents_2012_updated.json:', e)\n",
    "    raise e\n",
    "\n",
    "# Ensure Last_Report_Date column exists in df_clean; if not, create it\n",
    "if 'Last_Report_Date' not in df_clean.columns:\n",
    "    df_clean['Last_Report_Date'] = np.nan\n",
    "\n",
    "# We'll update rows in df_clean that correspond to year 2012\n",
    "mask_2012 = df_clean['Year'] == 2012\n",
    "\n",
    "# Merge the 2012 records from the clean CSV file with the updated incidents 2012 JSON based on 'Name'\n",
    "# (You might need to adjust if the key is different; here we assume 'Name' is common.)\n",
    "\n",
    "# Select only necessary columns from df_2012: Name and Last_Report_Date\n",
    "if 'Name' not in df_2012.columns or 'Last_Report_Date' not in df_2012.columns:\n",
    "    print('Missing required columns in incidents_2012_updated.json')\n",
    "else:\n",
    "    df_2012_subset = df_2012[['Name', 'Contain/_Control Date']].copy()\n",
    "    # The updated Last_Report_Date is stored in the 'Contain/_Control Date' column from our previous merge for consistency\n",
    "    df_2012_subset = df_2012_subset.rename(columns={'Contain/_Control Date': 'Last_Report_Date_new'})\n",
    "\n",
    "    # Merge with the 2012 subset of the clean csv\n",
    "    df_clean_2012 = df_clean[mask_2012].merge(df_2012_subset, on='Name', how='left')\n",
    "\n",
    "    # Update Last_Report_Date if missing in df_clean or if new value is provided\n",
    "    df_clean_2012['Last_Report_Date'] = df_clean_2012.apply(\n",
    "        lambda row: row['Last_Report_Date_new'] if pd.isna(row['Last_Report_Date']) and pd.notna(row['Last_Report_Date_new']) else row['Last_Report_Date'], axis=1\n",
    "    )\n",
    "\n",
    "    # Remove the temporary column\n",
    "    df_clean_2012.drop(columns=['Last_Report_Date_new'], inplace=True)\n",
    "\n",
    "    # Update the main dataframe with the updated 2012 records\n",
    "    df_clean.update(df_clean_2012)\n",
    "\n",
    "    # Save the updated CSV\n",
    "    df_clean.to_csv('combined_incidents_clean_updated.csv', index=False)\n",
    "    print('\\\n",
    "Updated combined_incidents_clean.csv saved as combined_incidents_clean_updated.csv')\n",
    "\n",
    "# Show the updated 2012 rows\n",
    "print('\\\n",
    "Updated 2012 rows from combined_incidents_clean_updated.csv (first 10 rows):')\n",
    "print(df_clean[mask_2012].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b618246a-c3f5-4cb8-8139-3a9af9d03beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean.csv\n",
      "Before update, number of 2012 rows with missing Last_Report_Date: 51\n",
      "After update, number of 2012 rows with missing Last_Report_Date: 0\n",
      "\n",
      "Updated CSV saved as combined_incidents_clean_updated.csv\n",
      "\n",
      "Sample of updated 2012 rows:\n",
      "                      Name   GACC  State  Start_Date Last_Report_Date  \\\n",
      "150              Long Draw     NW     OR  07/08/2012       07/30/2012   \n",
      "151               Holloway  NW/WB  OR/NV  08/05/2012       08/23/2012   \n",
      "152        Mustang Complex     EB     ID  07/30/2012       10/18/2012   \n",
      "153                   Rush     NO     CA  08/12/2012       09/04/2012   \n",
      "154       Whitewater-Baldy     SW     NM  05/16/2012       07/31/2012   \n",
      "155              Ash Creek     NR     MT  06/25/2012       07/11/2012   \n",
      "156            Kinyon Road     EB     ID  07/07/2012       07/19/2012   \n",
      "157               Halstead     EB     ID  07/27/2012       10/18/2012   \n",
      "158  Rosebud Creek Complex     NR     MT  08/01/2012       08/16/2012   \n",
      "159       Miller Homestead     NW     OR  07/08/2012       09/21/2012   \n",
      "\n",
      "    Size_Acres Cause         Cost Inc_Type Contain_Control_Date    Year  \n",
      "150    557,628     L   $4,360,000      NaN           07/30/2012  2012.0  \n",
      "151    460,850     L   $9,166,719      NaN           08/23/2012  2012.0  \n",
      "152    341,488     L  $38,323,413      NaN           10/18/2012  2012.0  \n",
      "153    315,577     L  $15,170,000      NaN           09/04/2012  2012.0  \n",
      "154    297,845     L  $23,000,000      NaN           07/31/2012  2012.0  \n",
      "155    249,562     L   $7,500,000      NaN           07/11/2012  2012.0  \n",
      "156    210,874     L   $1,625,000      NaN           07/19/2012  2012.0  \n",
      "157    181,948     L  $26,413,932      NaN           10/18/2012  2012.0  \n",
      "158    171,444     L   $9,000,000      NaN           08/16/2012  2012.0  \n",
      "159    160,853     L   $6,000,000      NaN           09/21/2012  2012.0  \n"
     ]
    }
   ],
   "source": [
    "# This code updates the 'Last_Report_Date' column for 2012 rows in combined_incidents_clean.csv   \n",
    "# using the 'Contain_Control_Date' value.  \n",
    "  \n",
    "import pandas as pd  \n",
    "  \n",
    "# Load the clean CSV file  \n",
    "df_clean = pd.read_csv('combined_incidents_clean.csv')  \n",
    "print('Loaded combined_incidents_clean.csv')  \n",
    "  \n",
    "# Filter rows from 2012 where Last_Report_Date is missing  \n",
    "mask_2012 = (df_clean['Year'] == 2012) & (pd.isna(df_clean['Last_Report_Date']))  \n",
    "  \n",
    "print('Before update, number of 2012 rows with missing Last_Report_Date:', df_clean.loc[mask_2012].shape[0])  \n",
    "  \n",
    "# Update the Last_Report_Date for these rows with the value from Contain_Control_Date  \n",
    "df_clean.loc[mask_2012, 'Last_Report_Date'] = df_clean.loc[mask_2012, 'Contain_Control_Date']  \n",
    "  \n",
    "# Check if the update was successful  \n",
    "mask_2012_missing_after = (df_clean['Year'] == 2012) & (pd.isna(df_clean['Last_Report_Date']))  \n",
    "print('After update, number of 2012 rows with missing Last_Report_Date:', df_clean.loc[mask_2012_missing_after].shape[0])  \n",
    "  \n",
    "# Save the updated CSV file  \n",
    "df_clean.to_csv('combined_incidents_clean_updated.csv', index=False)  \n",
    "print('\\nUpdated CSV saved as combined_incidents_clean_updated.csv')  \n",
    "  \n",
    "# Show a sample of the updated rows for 2012  \n",
    "mask_2012_all = (df_clean['Year'] == 2012)  \n",
    "print('\\nSample of updated 2012 rows:')  \n",
    "print(df_clean.loc[mask_2012_all].head(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ee9d1e-edb0-444e-8859-179d2c6792b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean.csv with 470 rows.\n",
      "\\nProcessing 2009 data from structured_incidents_data_2009.json\n",
      "Year 2009: Updated 27 rows with Last_Report_Date\n",
      "\\nProcessing 2010 data from structured_incidents_data_2010.json\n",
      "Year 2010: Updated 9 rows with Last_Report_Date\n",
      "\\nProcessing 2011 data from structured_incidents_data_2011.json\n",
      "Year 2011: Updated 41 rows with Last_Report_Date\n",
      "\\nProcessing 2021 data from structured_incidents_data_2021.json\n",
      "Date column Contain_Control_Date not found in structured_incidents_data_2021.json\n",
      "\\nUpdated CSV saved as combined_incidents_clean_updated_all_years.csv\n",
      "\\nSummary of updates:\n",
      "Year 2009: 27 rows updated.\n",
      "Year 2010: 9 rows updated.\n",
      "Year 2011: 41 rows updated.\n",
      "\\nRemaining rows with missing Last_Report_Date:\n",
      "Year 2009: 0 rows still missing Last_Report_Date\n",
      "Year 2010: 0 rows still missing Last_Report_Date\n",
      "Year 2011: 0 rows still missing Last_Report_Date\n",
      "Year 2021: 24 rows still missing Last_Report_Date\n",
      "\\nSample of updated 2009 rows:\n",
      "            Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "443      Chester   SA    OK  07/10/2009        17-Jul-09     41,497     U   \n",
      "444  Bluff Creek   AK    AK  07/26/2009        10-Nov-09     41,756     L   \n",
      "445     Big Pole   EB    UT  08/06/2009         9-Nov-09     44,345     L   \n",
      "\n",
      "         Cost Inc_Type Contain_Control_Date    Year  \n",
      "443  $250,000       WF           07/17/2009  2009.0  \n",
      "444        NR       WF           11/10/2009  2009.0  \n",
      "445        NR       WF           11/09/2009  2009.0  \n",
      "\\nSample of updated 2010 rows:\n",
      "           Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "122  Long Butte   EB    ID  08/21/2010         3-Sep-10    306,113     L   \n",
      "123      Toklat   AK    AK  05/16/2010         4-Jun-10    171,727     L   \n",
      "124   Jefferson   EB    ID  07/13/2010        17-Jul-10    109,436     H   \n",
      "\n",
      "            Cost Inc_Type Contain_Control_Date    Year  \n",
      "122  $ 4,225,000       WF           09/03/2010  2010.0  \n",
      "123   $2,109,186       WF           06/04/2010  2010.0  \n",
      "124    $ 700,819       WF           07/17/2010  2010.0  \n",
      "\\nSample of updated 2011 rows:\n",
      "              Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "402         Wallow   SW    AZ  05/29/2011         8-Jul-11    538,049     U   \n",
      "403     Rock House   SA    TX  04/09/2011        12-May-11    314,444     H   \n",
      "404  Honey Prairie   SA    GA  04/30/2011        28-Dec-11    309,200     L   \n",
      "\n",
      "             Cost Inc_Type Contain_Control_Date    Year  \n",
      "402  $109,000,000      NaN           07/08/2011  2011.0  \n",
      "403    $8,399,072      NaN           05/12/2011  2011.0  \n",
      "404   $53,420,000      NaN           12/28/2011  2011.0  \n",
      "\\nSample of updated 2021 rows:\n",
      "         Name GACC State  Start_Date Last_Report_Date Size_Acres Cause Cost  \\\n",
      "315     Dixie   NO    CA  07/13/2021              NaN    963,309     U  NaN   \n",
      "316   Bootleg   NW    OR  07/06/2021              NaN    413,717     L  NaN   \n",
      "317  Monument   NO    CA  07/31/2021              NaN    223,124     L  NaN   \n",
      "\n",
      "    Inc_Type Contain_Control_Date    Year  \n",
      "315      NaN                  NaN  2021.0  \n",
      "316      NaN                  NaN  2021.0  \n",
      "317      NaN                  NaN  2021.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import os  \n",
    "  \n",
    "# Function to load and extract date information from a structured JSON file.  \n",
    "def load_and_extract_dates(filepath, year, date_column):  \n",
    "    try:  \n",
    "        df_raw = pd.read_json(filepath)  \n",
    "        if 'significant_incidents' in df_raw.columns:  \n",
    "            df = pd.json_normalize(df_raw['significant_incidents'])  \n",
    "        else:  \n",
    "            df = df_raw  \n",
    "        # Select only the Name and the specified date column, and add the Year column.  \n",
    "        if date_column in df.columns:  \n",
    "            df_dates = df[['Name', date_column]].copy()  \n",
    "            df_dates['Year'] = int(year)  \n",
    "            return df_dates  \n",
    "        else:  \n",
    "            print('Date column ' + date_column + ' not found in ' + filepath)  \n",
    "            return None  \n",
    "    except Exception as e:  \n",
    "        print('Error processing ' + filepath + ': ' + str(e))  \n",
    "        return None  \n",
    "  \n",
    "# Define filepaths and date columns for each year.  \n",
    "# If the JSON filename consistently uses the pattern structured_incidents_data_YEAR.json then...  \n",
    "date_columns = {  \n",
    "    '2009': 'Contain_Control_Date',  \n",
    "    '2010': 'Contain_Control_Date',  \n",
    "    '2011': 'Contain_Control_Date',  \n",
    "    '2021': 'Contain_Control_Date'  \n",
    "}  \n",
    "  \n",
    "# Load the combined_incidents_clean.csv file.  \n",
    "df_clean = pd.read_csv('combined_incidents_clean.csv')  \n",
    "print('Loaded combined_incidents_clean.csv with ' + str(len(df_clean)) + ' rows.')  \n",
    "  \n",
    "# Process each year from the JSON structured files.  \n",
    "updates_by_year = {}  \n",
    "for year, date_column in date_columns.items():  \n",
    "    filepath = f'structured_incidents_data_{year}.json'  \n",
    "    if os.path.exists(filepath):  \n",
    "        print('\\\\nProcessing ' + year + ' data from ' + filepath)  \n",
    "        df_dates = load_and_extract_dates(filepath, year, date_column)  \n",
    "        if df_dates is not None:  \n",
    "            # Filter df_clean for the current year.  \n",
    "            df_clean_year = df_clean[df_clean['Year'] == int(year)].copy()  \n",
    "            # Merge on 'Name' and 'Year'. This will add a column named '<date_column>_y'  \n",
    "            df_merged = df_clean_year.merge(df_dates, on=['Name', 'Year'], how='left')  \n",
    "            # Before update, count missing Last_Report_Date values.  \n",
    "            mask_year = (df_clean['Year'] == int(year)) & (pd.isna(df_clean['Last_Report_Date']))  \n",
    "            before_count = df_clean.loc[mask_year].shape[0]  \n",
    "              \n",
    "            # For the matching rows, update Last_Report_Date with the value from the structured JSON.  \n",
    "            # The merged dataframe has two date columns: original CSV (as e.g. Contain_Control_Date_x)  \n",
    "            # and the JSON data as Contain_Control_Date_y (or Contain_or_Last_Report_Date_y).  \n",
    "            json_date_col = date_column + '_y'  \n",
    "            for idx, row in df_merged.iterrows():  \n",
    "                if pd.isna(row['Last_Report_Date']) and pd.notna(row[json_date_col]):  \n",
    "                    df_clean.loc[(df_clean['Name'] == row['Name']) & (df_clean['Year'] == int(year)), 'Last_Report_Date'] = row[json_date_col]  \n",
    "              \n",
    "            # After update, count remaining missing values.  \n",
    "            mask_year = (df_clean['Year'] == int(year)) & (pd.isna(df_clean['Last_Report_Date']))  \n",
    "            after_count = df_clean.loc[mask_year].shape[0]  \n",
    "            updates_by_year[year] = before_count - after_count  \n",
    "            print('Year ' + year + ': Updated ' + str(updates_by_year[year]) + ' rows with Last_Report_Date')  \n",
    "    else:  \n",
    "        print('File not found: ' + filepath)  \n",
    "  \n",
    "# Optionally, if you want to replace all values of Last_Report_Date (with structured JSON if available)  \n",
    "# You could relax the condition and update whenever a JSON date exists, even overwriting non-null ones.  \n",
    "  \n",
    "# Save the updated CSV file.  \n",
    "output_filename = 'combined_incidents_clean_updated_all_years.csv'  \n",
    "df_clean.to_csv(output_filename, index=False)  \n",
    "print('\\\\nUpdated CSV saved as ' + output_filename)  \n",
    "  \n",
    "# Summary of updates  \n",
    "print('\\\\nSummary of updates:')  \n",
    "for year, count in updates_by_year.items():  \n",
    "    print('Year ' + year + ': ' + str(count) + ' rows updated.')  \n",
    "  \n",
    "# Check remaining missing Last_Report_Date values per year.  \n",
    "print('\\\\nRemaining rows with missing Last_Report_Date:')  \n",
    "for year in ['2009', '2010', '2011', '2021']:  \n",
    "    mask_year = (df_clean['Year'] == int(year)) & (pd.isna(df_clean['Last_Report_Date']))  \n",
    "    count = df_clean.loc[mask_year].shape[0]  \n",
    "    print('Year ' + year + ': ' + str(count) + ' rows still missing Last_Report_Date')  \n",
    "  \n",
    "# Show a sample of the updated rows for each year.  \n",
    "for year in ['2009', '2010', '2011', '2021']:  \n",
    "    mask_year = (df_clean['Year'] == int(year))  \n",
    "    print('\\\\nSample of updated ' + year + ' rows:')  \n",
    "    print(df_clean.loc[mask_year].head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b95e69-8df0-4bbe-b193-1fe4c0d77052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean.csv with 470 rows.\n",
      "\\nProcessing 2009 data from structured_incidents_data_2009.json\n",
      "Year 2009: Updated 27 rows in Last_Report_Date.\n",
      "\\nProcessing 2010 data from structured_incidents_data_2010.json\n",
      "Year 2010: Updated 9 rows in Last_Report_Date.\n",
      "\\nProcessing 2011 data from structured_incidents_data_2011.json\n",
      "Year 2011: Updated 41 rows in Last_Report_Date.\n",
      "\\nProcessing 2021 data from structured_incidents_data_2021.json\n",
      "Year 2021: Updated 0 rows in Last_Report_Date.\n",
      "\\nUpdated CSV saved as combined_incidents_clean_updated_all_years.csv\n",
      "\\nSummary of updates:\n",
      "Year 2009: 27 rows updated.\n",
      "Year 2010: 9 rows updated.\n",
      "Year 2011: 41 rows updated.\n",
      "Year 2021: 0 rows updated.\n",
      "\\nRemaining rows with missing Last_Report_Date:\n",
      "Year 2009: 0 rows still missing Last_Report_Date\n",
      "Year 2010: 0 rows still missing Last_Report_Date\n",
      "Year 2011: 0 rows still missing Last_Report_Date\n",
      "Year 2021: 24 rows still missing Last_Report_Date\n",
      "\\nSample of updated 2009 rows:\n",
      "            Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "443      Chester   SA    OK  07/10/2009        17-Jul-09     41,497     U   \n",
      "444  Bluff Creek   AK    AK  07/26/2009        10-Nov-09     41,756     L   \n",
      "445     Big Pole   EB    UT  08/06/2009         9-Nov-09     44,345     L   \n",
      "\n",
      "         Cost Inc_Type Contain_Control_Date    Year  \n",
      "443  $250,000       WF           07/17/2009  2009.0  \n",
      "444        NR       WF           11/10/2009  2009.0  \n",
      "445        NR       WF           11/09/2009  2009.0  \n",
      "\\nSample of updated 2010 rows:\n",
      "           Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "122  Long Butte   EB    ID  08/21/2010         3-Sep-10    306,113     L   \n",
      "123      Toklat   AK    AK  05/16/2010         4-Jun-10    171,727     L   \n",
      "124   Jefferson   EB    ID  07/13/2010        17-Jul-10    109,436     H   \n",
      "\n",
      "            Cost Inc_Type Contain_Control_Date    Year  \n",
      "122  $ 4,225,000       WF           09/03/2010  2010.0  \n",
      "123   $2,109,186       WF           06/04/2010  2010.0  \n",
      "124    $ 700,819       WF           07/17/2010  2010.0  \n",
      "\\nSample of updated 2011 rows:\n",
      "              Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "402         Wallow   SW    AZ  05/29/2011         8-Jul-11    538,049     U   \n",
      "403     Rock House   SA    TX  04/09/2011        12-May-11    314,444     H   \n",
      "404  Honey Prairie   SA    GA  04/30/2011        28-Dec-11    309,200     L   \n",
      "\n",
      "             Cost Inc_Type Contain_Control_Date    Year  \n",
      "402  $109,000,000      NaN           07/08/2011  2011.0  \n",
      "403    $8,399,072      NaN           05/12/2011  2011.0  \n",
      "404   $53,420,000      NaN           12/28/2011  2011.0  \n",
      "\\nSample of updated 2021 rows:\n",
      "         Name GACC State  Start_Date Last_Report_Date Size_Acres Cause Cost  \\\n",
      "315     Dixie   NO    CA  07/13/2021              NaN    963,309     U  NaN   \n",
      "316   Bootleg   NW    OR  07/06/2021              NaN    413,717     L  NaN   \n",
      "317  Monument   NO    CA  07/31/2021              NaN    223,124     L  NaN   \n",
      "\n",
      "    Inc_Type Contain_Control_Date    Year  \n",
      "315      NaN                  NaN  2021.0  \n",
      "316      NaN                  NaN  2021.0  \n",
      "317      NaN                  NaN  2021.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import os  \n",
    "  \n",
    "# Function to load and extract date information from a structured JSON file.  \n",
    "def load_and_extract_dates(filepath, year, date_column):  \n",
    "    try:  \n",
    "        df_raw = pd.read_json(filepath)  \n",
    "        if 'significant_incidents' in df_raw.columns:  \n",
    "            df = pd.json_normalize(df_raw['significant_incidents'])  \n",
    "        else:  \n",
    "            df = df_raw  \n",
    "        # Select only the Name and the specified date column, and add the Year column.  \n",
    "        if date_column in df.columns:  \n",
    "            df_dates = df[['Name', date_column]].copy()  \n",
    "            df_dates['Year'] = int(year)  \n",
    "            return df_dates  \n",
    "        else:  \n",
    "            print('Date column ' + date_column + ' not found in ' + filepath)  \n",
    "            return None  \n",
    "    except Exception as e:  \n",
    "        print('Error processing ' + filepath + ': ' + str(e))  \n",
    "        return None  \n",
    "  \n",
    "# Dictionary for defining which date column to use per year  \n",
    "date_columns = {  \n",
    "    '2009': 'Contain_Control_Date',  \n",
    "    '2010': 'Contain_Control_Date',  \n",
    "    '2011': 'Contain_Control_Date',  \n",
    "    '2021': 'Contain_or_Last_Report_Date'  \n",
    "}  \n",
    "  \n",
    "# Load the combined_incidents_clean.csv file.  \n",
    "df_clean = pd.read_csv('combined_incidents_clean.csv')  \n",
    "print('Loaded combined_incidents_clean.csv with ' + str(len(df_clean)) + ' rows.')  \n",
    "  \n",
    "# For each year, update Last_Report_Date with the date from the JSON file.  \n",
    "updates_by_year = {}  \n",
    "  \n",
    "for year, json_date_column in date_columns.items():  \n",
    "    filepath = f'structured_incidents_data_{year}.json'  \n",
    "    if os.path.exists(filepath):  \n",
    "        print('\\\\nProcessing ' + year + ' data from ' + filepath)  \n",
    "        df_dates = load_and_extract_dates(filepath, year, json_date_column)  \n",
    "        if df_dates is not None:  \n",
    "            # Merge df_clean (for the year) with the JSON dates based on Name and Year.  \n",
    "            mask_year = (df_clean['Year'] == int(year))  \n",
    "            df_clean_year = df_clean.loc[mask_year].copy()  \n",
    "              \n",
    "            df_merged = df_clean_year.merge(df_dates, on=['Name', 'Year'], how='left', suffixes=('', '_json'))  \n",
    "              \n",
    "            # Count how many rows we can update: update Last_Report_Date if JSON date exists.  \n",
    "            updated_count = 0  \n",
    "            for idx, row in df_merged.iterrows():  \n",
    "                json_date = row.get(json_date_column + '_json')  \n",
    "                # If the JSON date is not missing, update Last_Report_Date.  \n",
    "                if pd.notna(json_date):  \n",
    "                    # Update in the original df_clean  \n",
    "                    mask = (df_clean['Name'] == row['Name']) & (df_clean['Year'] == int(year))  \n",
    "                    df_clean.loc[mask, 'Last_Report_Date'] = json_date  \n",
    "                    updated_count += 1  \n",
    "            updates_by_year[year] = updated_count  \n",
    "            print('Year ' + year + ': Updated ' + str(updated_count) + ' rows in Last_Report_Date.')  \n",
    "    else:  \n",
    "        print('File not found: ' + filepath)  \n",
    "  \n",
    "# Save the updated combined file  \n",
    "output_filename = 'combined_incidents_clean_updated_all_years.csv'  \n",
    "df_clean.to_csv(output_filename, index=False)  \n",
    "print('\\\\nUpdated CSV saved as ' + output_filename)  \n",
    "  \n",
    "# Summary of updates  \n",
    "print('\\\\nSummary of updates:')  \n",
    "for year, count in updates_by_year.items():  \n",
    "    print('Year ' + year + ': ' + str(count) + ' rows updated.')  \n",
    "  \n",
    "# Check remaining missing Last_Report_Date values by year.  \n",
    "print('\\\\nRemaining rows with missing Last_Report_Date:')  \n",
    "for year in ['2009', '2010', '2011', '2021']:  \n",
    "    mask_year = (df_clean['Year'] == int(year)) & (pd.isna(df_clean['Last_Report_Date']))  \n",
    "    count = df_clean.loc[mask_year].shape[0]  \n",
    "    print('Year ' + year + ': ' + str(count) + ' rows still missing Last_Report_Date')  \n",
    "  \n",
    "# Show a sample of the updated rows for each year.  \n",
    "for year in ['2009', '2010', '2011', '2021']:  \n",
    "    mask_year = (df_clean['Year'] == int(year))  \n",
    "    print('\\\\nSample of updated ' + year + ' rows:')  \n",
    "    print(df_clean.loc[mask_year].head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "541139f9-9e8c-4363-baa7-43e7c24de4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean.csv with 470 rows.\n",
      "\\nProcessing year 2008 data from structured_incidents_data.json\n",
      "Year 2008: Updated 0 rows in Last_Report_Date.\n",
      "\\nProcessing year 2009 data from structured_incidents_data_2009.json\n",
      "Year 2009: Updated 27 rows in Last_Report_Date.\n",
      "\\nProcessing year 2010 data from structured_incidents_data_2010.json\n",
      "Year 2010: Updated 9 rows in Last_Report_Date.\n",
      "\\nProcessing year 2011 data from structured_incidents_data_2011.json\n",
      "Year 2011: Updated 41 rows in Last_Report_Date.\n",
      "\\nProcessing year 2021 data from structured_incidents_data_2021.json\n",
      "Merged date column not found for year 2021. Skipping update.\n",
      "\\nUpdated CSV saved as combined_incidents_clean_updated_all_years.csv\n",
      "\\nSummary of updates:\n",
      "Year 2008: 0 rows updated.\n",
      "Year 2009: 27 rows updated.\n",
      "Year 2010: 9 rows updated.\n",
      "Year 2011: 41 rows updated.\n",
      "\\nRemaining rows with missing Last_Report_Date:\n",
      "Year 2008: 0 rows still missing Last_Report_Date\n",
      "Year 2009: 0 rows still missing Last_Report_Date\n",
      "Year 2010: 0 rows still missing Last_Report_Date\n",
      "Year 2011: 0 rows still missing Last_Report_Date\n",
      "Year 2021: 24 rows still missing Last_Report_Date\n",
      "\\nSample of updated 2008 rows:\n",
      "Empty DataFrame\n",
      "Columns: [Name, GACC, State, Start_Date, Last_Report_Date, Size_Acres, Cause, Cost, Inc_Type, Contain_Control_Date, Year]\n",
      "Index: []\n",
      "\\nSample of updated 2009 rows:\n",
      "            Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "443      Chester   SA    OK  07/10/2009        17-Jul-09     41,497     U   \n",
      "444  Bluff Creek   AK    AK  07/26/2009        10-Nov-09     41,756     L   \n",
      "445     Big Pole   EB    UT  08/06/2009         9-Nov-09     44,345     L   \n",
      "\n",
      "         Cost Inc_Type Contain_Control_Date    Year  \n",
      "443  $250,000       WF           07/17/2009  2009.0  \n",
      "444        NR       WF           11/10/2009  2009.0  \n",
      "445        NR       WF           11/09/2009  2009.0  \n",
      "\\nSample of updated 2010 rows:\n",
      "           Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "122  Long Butte   EB    ID  08/21/2010         3-Sep-10    306,113     L   \n",
      "123      Toklat   AK    AK  05/16/2010         4-Jun-10    171,727     L   \n",
      "124   Jefferson   EB    ID  07/13/2010        17-Jul-10    109,436     H   \n",
      "\n",
      "            Cost Inc_Type Contain_Control_Date    Year  \n",
      "122  $ 4,225,000       WF           09/03/2010  2010.0  \n",
      "123   $2,109,186       WF           06/04/2010  2010.0  \n",
      "124    $ 700,819       WF           07/17/2010  2010.0  \n",
      "\\nSample of updated 2011 rows:\n",
      "              Name GACC State  Start_Date Last_Report_Date Size_Acres Cause  \\\n",
      "402         Wallow   SW    AZ  05/29/2011         8-Jul-11    538,049     U   \n",
      "403     Rock House   SA    TX  04/09/2011        12-May-11    314,444     H   \n",
      "404  Honey Prairie   SA    GA  04/30/2011        28-Dec-11    309,200     L   \n",
      "\n",
      "             Cost Inc_Type Contain_Control_Date    Year  \n",
      "402  $109,000,000      NaN           07/08/2011  2011.0  \n",
      "403    $8,399,072      NaN           05/12/2011  2011.0  \n",
      "404   $53,420,000      NaN           12/28/2011  2011.0  \n",
      "\\nSample of updated 2021 rows:\n",
      "         Name GACC State  Start_Date Last_Report_Date Size_Acres Cause Cost  \\\n",
      "315     Dixie   NO    CA  07/13/2021              NaN    963,309     U  NaN   \n",
      "316   Bootleg   NW    OR  07/06/2021              NaN    413,717     L  NaN   \n",
      "317  Monument   NO    CA  07/31/2021              NaN    223,124     L  NaN   \n",
      "\n",
      "    Inc_Type Contain_Control_Date    Year  \n",
      "315      NaN                  NaN  2021.0  \n",
      "316      NaN                  NaN  2021.0  \n",
      "317      NaN                  NaN  2021.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import os  \n",
    "  \n",
    "# Define configuration for each year with JSON file path and the corresponding date column to use  \n",
    "year_configs = {  \n",
    "    '2008': {  \n",
    "        'filepath': 'structured_incidents_data.json',  \n",
    "        'date_column': 'Contain_Control_Date'  \n",
    "    },  \n",
    "    '2009': {  \n",
    "        'filepath': 'structured_incidents_data_2009.json',  \n",
    "        'date_column': 'Contain_Control_Date'  \n",
    "    },  \n",
    "    '2010': {  \n",
    "        'filepath': 'structured_incidents_data_2010.json',  \n",
    "        'date_column': 'Contain_Control_Date'  \n",
    "    },  \n",
    "    '2011': {  \n",
    "        'filepath': 'structured_incidents_data_2011.json',  \n",
    "        'date_column': 'Contain_Control_Date'  \n",
    "    },  \n",
    "    '2021': {  \n",
    "        'filepath': 'structured_incidents_data_2021.json',  \n",
    "        'date_column': 'Contain_or_Last_Report_Date'  \n",
    "    }  \n",
    "}  \n",
    "  \n",
    "# Function to load and extract date information from a given JSON file for a specific year.  \n",
    "def load_and_extract_dates(config, year):  \n",
    "    filepath = config['filepath']  \n",
    "    date_column = config['date_column']  \n",
    "    try:  \n",
    "        df_raw = pd.read_json(filepath)  \n",
    "        # If the data is nested under \"significant_incidents\", flatten it  \n",
    "        if 'significant_incidents' in df_raw.columns:  \n",
    "            df = pd.json_normalize(df_raw['significant_incidents'])  \n",
    "        else:  \n",
    "            df = df_raw  \n",
    "        # Ensure the date column is available  \n",
    "        if date_column in df.columns:  \n",
    "            df_dates = df[['Name', date_column]].copy()  \n",
    "            df_dates['Year'] = int(year)  # add Year column for merging  \n",
    "            return df_dates  \n",
    "        else:  \n",
    "            print('Date column ' + date_column + ' not found in ' + filepath)  \n",
    "            return None  \n",
    "    except Exception as e:  \n",
    "        print('Error processing ' + filepath + ': ' + str(e))  \n",
    "        return None  \n",
    "  \n",
    "# Load the CSV file  \n",
    "df_clean = pd.read_csv('combined_incidents_clean.csv')  \n",
    "print('Loaded combined_incidents_clean.csv with ' + str(len(df_clean)) + ' rows.')  \n",
    "  \n",
    "# Dictionary to record how many rows are updated per year.  \n",
    "updates_by_year = {}  \n",
    "  \n",
    "# Process each year according to the mapping  \n",
    "for year, config in year_configs.items():  \n",
    "    filepath = config['filepath']  \n",
    "    if os.path.exists(filepath):  \n",
    "        print('\\\\nProcessing year ' + year + ' data from ' + filepath)  \n",
    "        df_dates = load_and_extract_dates(config, year)  \n",
    "        if df_dates is not None:  \n",
    "            # Filter CSV rows corresponding to the year  \n",
    "            mask_year = (df_clean['Year'] == int(year))  \n",
    "            df_clean_year = df_clean.loc[mask_year].copy()  \n",
    "          \n",
    "            # Merge CSV data with JSON dates on Name and Year.  \n",
    "            df_merged = df_clean_year.merge(df_dates, on=['Name', 'Year'], how='left')  \n",
    "            # Example: JSON date value appears in a column named like date_column+'_y'  \n",
    "            json_date_col = config['date_column']  \n",
    "            # After merging, the JSON column is renamed to e.g. 'Contain_Control_Date_y'  \n",
    "            merged_date_col = json_date_col + '_y'  \n",
    "              \n",
    "            if merged_date_col not in df_merged.columns:  \n",
    "                print('Merged date column not found for year ' + year + '. Skipping update.')  \n",
    "                continue  \n",
    "  \n",
    "            updated_count = 0  \n",
    "            # Loop over merged rows and update CSV if JSON date is available.  \n",
    "            for idx, row in df_merged.iterrows():  \n",
    "                json_date = row[merged_date_col]  \n",
    "                if pd.notna(json_date):  \n",
    "                    # Update in original df_clean using the Name and Year  \n",
    "                    mask = (df_clean['Name'] == row['Name']) & (df_clean['Year'] == int(year))  \n",
    "                    df_clean.loc[mask, 'Last_Report_Date'] = json_date  \n",
    "                    updated_count += 1  \n",
    "            updates_by_year[year] = updated_count  \n",
    "            print('Year ' + year + ': Updated ' + str(updated_count) + ' rows in Last_Report_Date.')  \n",
    "    else:  \n",
    "        print('File not found for year ' + year + ': ' + filepath)  \n",
    "  \n",
    "# Save the updated CSV file.  \n",
    "output_filename = 'combined_incidents_clean_updated_all_years.csv'  \n",
    "df_clean.to_csv(output_filename, index=False)  \n",
    "print('\\\\nUpdated CSV saved as ' + output_filename)  \n",
    "  \n",
    "# Summary of updates.  \n",
    "print('\\\\nSummary of updates:')  \n",
    "for year, count in updates_by_year.items():  \n",
    "    print('Year ' + year + ': ' + str(count) + ' rows updated.')  \n",
    "  \n",
    "# Check remaining missing Last_Report_Date values by year.  \n",
    "print('\\\\nRemaining rows with missing Last_Report_Date:')  \n",
    "for year in year_configs.keys():  \n",
    "    mask_year = (df_clean['Year'] == int(year)) & (pd.isna(df_clean['Last_Report_Date']))  \n",
    "    count = df_clean.loc[mask_year].shape[0]  \n",
    "    print('Year ' + year + ': ' + str(count) + ' rows still missing Last_Report_Date')  \n",
    "  \n",
    "# Display a sample of updated rows for each processed year.  \n",
    "for year in year_configs.keys():  \n",
    "    mask_year = (df_clean['Year'] == int(year))  \n",
    "    print('\\\\nSample of updated ' + year + ' rows:')  \n",
    "    print(df_clean.loc[mask_year].head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "401386ce-37c2-4103-a876-5d75b39ecf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean_final.csv with 470 rows\n",
      "Date columns in the dataset:\n",
      "Start_Date: 470 non-null values out of 470 rows\n",
      "Last_Report_Date: 372 non-null values out of 470 rows\n",
      "Contain_Control_Date: 151 non-null values out of 470 rows\n",
      "Rows where Last_Report_Date is null but Contain_Control_Date is not null: 74\n",
      "Sample rows where Last_Report_Date is null but Contain_Control_Date is not null:\n",
      "                       Name    Year Last_Report_Date Contain_Control_Date\n",
      "9                Glass Fire  2008.0              NaN           03/02/2008\n",
      "10          Klamath Theater  2008.0              NaN           09/26/2008\n",
      "11            Basin Complex  2008.0              NaN           07/29/2008\n",
      "12  Iron & Alps \\nComplexes  2008.0              NaN           09/04/2008\n",
      "13       Dunn Mtn. \\nAssist  2008.0              NaN           09/02/2008\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the date columns in our dataset to understand what we're working with\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file with the updated Year values\n",
    "csv_file = 'combined_incidents_clean_final.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Loaded {csv_file} with {len(df)} rows\")\n",
    "\n",
    "# Check the date columns\n",
    "print(\"\\\n",
    "Date columns in the dataset:\")\n",
    "date_columns = ['Start_Date', 'Last_Report_Date', 'Contain_Control_Date']\n",
    "for col in date_columns:\n",
    "    non_null_count = df[col].notna().sum()\n",
    "    print(f\"{col}: {non_null_count} non-null values out of {len(df)} rows\")\n",
    "\n",
    "# Check for rows where Last_Report_Date is null but Contain_Control_Date is not null\n",
    "condition = df['Last_Report_Date'].isna() & df['Contain_Control_Date'].notna()\n",
    "count = condition.sum()\n",
    "print(f\"\\\n",
    "Rows where Last_Report_Date is null but Contain_Control_Date is not null: {count}\")\n",
    "\n",
    "# Show a few examples of these rows\n",
    "if count > 0:\n",
    "    print(\"\\\n",
    "Sample rows where Last_Report_Date is null but Contain_Control_Date is not null:\")\n",
    "    sample = df[condition].head(5)\n",
    "    print(sample[['Name', 'Year', 'Last_Report_Date', 'Contain_Control_Date']])\n",
    "\n",
    "print(\"\\\n",
    "Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c925f7e4-6dba-4c7f-a84b-cd8e2ca35118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean_final.csv with 470 rows\n",
      "Null values in Last_Report_Date before update: 98\n",
      "Rows to update: 74\n",
      "Null values in Last_Report_Date after update: 24\n",
      "Rows updated: 74\n",
      "Saved updated CSV to combined_incidents_clean_final_updated.csv\n",
      "Sample rows after update:\n",
      "                       Name    Year Last_Report_Date Contain_Control_Date\n",
      "9                Glass Fire  2008.0       03/02/2008           03/02/2008\n",
      "10          Klamath Theater  2008.0       09/26/2008           09/26/2008\n",
      "11            Basin Complex  2008.0       07/29/2008           07/29/2008\n",
      "12  Iron & Alps \\nComplexes  2008.0       09/04/2008           09/04/2008\n",
      "13       Dunn Mtn. \\nAssist  2008.0       09/02/2008           09/02/2008\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Copy Contain_Control_Date to Last_Report_Date where Last_Report_Date is null\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'combined_incidents_clean_final.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"Loaded {csv_file} with {len(df)} rows\")\n",
    "\n",
    "# Count nulls in Last_Report_Date before update\n",
    "null_last_report_before = df['Last_Report_Date'].isna().sum()\n",
    "print(f\"Null values in Last_Report_Date before update: {null_last_report_before}\")\n",
    "\n",
    "# Identify rows where Last_Report_Date is null but Contain_Control_Date is not null\n",
    "condition = df['Last_Report_Date'].isna() & df['Contain_Control_Date'].notna()\n",
    "rows_to_update = condition.sum()\n",
    "print(f\"Rows to update: {rows_to_update}\")\n",
    "\n",
    "# Copy Contain_Control_Date to Last_Report_Date for these rows\n",
    "df.loc[condition, 'Last_Report_Date'] = df.loc[condition, 'Contain_Control_Date']\n",
    "\n",
    "# Count nulls in Last_Report_Date after update\n",
    "null_last_report_after = df['Last_Report_Date'].isna().sum()\n",
    "print(f\"Null values in Last_Report_Date after update: {null_last_report_after}\")\n",
    "print(f\"Rows updated: {null_last_report_before - null_last_report_after}\")\n",
    "\n",
    "# Save the updated CSV\n",
    "output_file = 'combined_incidents_clean_final_updated.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\\n",
    "Saved updated CSV to {output_file}\")\n",
    "\n",
    "# Show a few examples of the updated rows\n",
    "print(\"\\\n",
    "Sample rows after update:\")\n",
    "sample = df[condition].head(5)\n",
    "print(sample[['Name', 'Year', 'Last_Report_Date', 'Contain_Control_Date']])\n",
    "\n",
    "print(\"\\\n",
    "Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1f58951-e729-40b5-88a3-201ffd92d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON is a list with 24 elements. Sample:\n",
      "{'Name': 'Dixie', 'GACC': 'NO', 'State': 'CA', 'Start_Date': '13-Jul-21', 'Contain_or_Last_Report_Date': '10/23', 'Size_Acres': '963,309', 'Cause': 'U', 'Estimated_Cost': '$637,428,216'}\n",
      "{'Name': 'Bootleg', 'GACC': 'NW', 'State': 'OR', 'Start_Date': '06-Jul-21', 'Contain_or_Last_Report_Date': '8/13', 'Size_Acres': '413,717', 'Cause': 'L', 'Estimated_Cost': '$100,900,000'}\n",
      "{'Name': 'Monument', 'GACC': 'NO', 'State': 'CA', 'Start_Date': '31-Jul-21', 'Contain_or_Last_Report_Date': '10/25', 'Size_Acres': '223,124', 'Cause': 'L', 'Estimated_Cost': '$163,739,291'}\n",
      "{'Name': 'Caldor', 'GACC': 'NO', 'State': 'CA', 'Start_Date': '14-Aug-21', 'Contain_or_Last_Report_Date': '10/20', 'Size_Acres': '221,835', 'Cause': 'H', 'Estimated_Cost': '$271,147,512'}\n",
      "{'Name': 'River Complex', 'GACC': 'NO', 'State': 'CA', 'Start_Date': '30-Jul-21', 'Contain_or_Last_Report_Date': '10/24', 'Size_Acres': '199,359', 'Cause': 'L', 'Estimated_Cost': '$95,340,595'}\n",
      "Total incidents: 24\n",
      "Incidents missing Contain_or_Last_Report_Date: 0\n",
      "Done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c661ddc3-b5e3-4478-93ee-c82cdeb37f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined_incidents_clean_final_updated.csv with 470 rows\n",
      "Number of 2021 incidents in CSV: 24\n",
      "Extracted 0 date values from structured_incidents_data_2021.json\n",
      "Sample of extracted dates:\n",
      "Updated 0 rows with dates from structured_incidents_data_2021.json\n",
      "Remaining null values in Last_Report_Date: 24\n",
      "Saved updated CSV to combined_incidents_clean_final_updated_2021.csv\n",
      "Sample of updated 2021 incidents:\n",
      "              Name    Year  Start_Date Last_Report_Date\n",
      "315          Dixie  2021.0  07/13/2021              NaN\n",
      "316        Bootleg  2021.0  07/06/2021              NaN\n",
      "317       Monument  2021.0  07/31/2021              NaN\n",
      "318         Caldor  2021.0  08/14/2021              NaN\n",
      "319  River Complex  2021.0  07/30/2021              NaN\n",
      "Done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535d33a-b691-4017-8bf7-8de39672cf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92700f-d7b1-406d-b616-1def22496f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
